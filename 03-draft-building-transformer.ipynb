{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install -q torchdata==0.3.0 torchtext==0.12 spacy==3.2 altair GPUtil\n!python -m spacy download de_core_news_sm\n!python -m spacy download en_core_web_sm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T23:15:12.530841Z","iopub.execute_input":"2025-02-12T23:15:12.531134Z","iopub.status.idle":"2025-02-12T23:15:35.899063Z","shell.execute_reply.started":"2025-02-12T23:15:12.531112Z","shell.execute_reply":"2025-02-12T23:15:35.897951Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"import os\nimport time\nimport math\nimport copy\nimport spacy\nimport GPUtil\nimport pandas as pd\nfrom typing import *\nfrom itertools import chain\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.optim.lr_scheduler import LambdaLR\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data import DataLoader, Dataset\n\nimport altair as alt\nfrom altair import Chart\n\nalt.data_transformers.disable_max_rows()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T23:15:35.900384Z","iopub.execute_input":"2025-02-12T23:15:35.900655Z","iopub.status.idle":"2025-02-12T23:15:35.908662Z","shell.execute_reply.started":"2025-02-12T23:15:35.900632Z","shell.execute_reply":"2025-02-12T23:15:35.907824Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"DataTransformerRegistry.enable('default')"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"class LayerNorm(nn.Module):\n    def __init__(self, features, eps=1e-6):\n        super().__init__()\n        self.gamma = nn.Parameter(torch.ones(features))\n        self.beta = nn.Parameter(torch.zeros(features))\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n        return self.gamma * (x - mean) / (std + self.eps) + self.beta","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ResidualConnection(nn.Module):\n    def __init__(self, size, dropout):\n        super().__init__()\n        self.norm = LayerNorm(size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, sublayer):\n        return x + self.dropout(sublayer(self.norm(x)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def attention(query, key, value, mask=None, dropout=None):\n    d_k = query.size(-1)\n    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n    \n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)\n        \n    p_attn = F.softmax(scores, dim=-1)\n    if dropout is not None:\n        p_attn = dropout(p_attn)\n        \n    return torch.matmul(p_attn, value), p_attn","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, h, d_model, dropout=0.1):\n        super().__init__()\n        assert d_model % h == 0\n        self.d_k = d_model // h\n        self.h = h\n        self.linears = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(4)])\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, query, key, value, mask=None):\n        if mask is not None:\n            mask = mask.unsqueeze(1)\n            \n        batch_size = query.size(0)\n        \n        query, key, value = [\n            lin(x).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n            for lin, x in zip(self.linears, (query, key, value))\n        ]\n        \n        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)\n        \n        x = x.transpose(1, 2).contiguous() \\\n             .view(batch_size, -1, self.h * self.d_k)\n             \n        return self.linears[-1](x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PositionwiseFeedForward(nn.Module):\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super().__init__()\n        self.w1 = nn.Linear(d_model, d_ff)\n        self.w2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        return self.w2(self.dropout(F.relu(self.w1(x))))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Embeddings(nn.Module):\n    def __init__(self, d_model, vocab):\n        super().__init__()\n        self.lut = nn.Embedding(vocab, d_model)\n        self.d_model = d_model\n\n    def forward(self, x):\n        return self.lut(x) * math.sqrt(self.d_model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout, max_len=5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        \n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n                             (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:, :x.size(1)].detach()\n        return self.dropout(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class EncoderLayer(nn.Module):\n    def __init__(self, size, self_attn, feed_forward, dropout):\n        super().__init__()\n        self.self_attn = self_attn\n        self.feed_forward = feed_forward\n        self.sublayer = nn.ModuleList([ResidualConnection(size, dropout) for _ in range(2)])\n        self.size = size\n\n    def forward(self, x, mask):\n        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n        return self.sublayer[1](x, self.feed_forward)\n\nclass Encoder(nn.Module):\n    def __init__(self, layer, N):\n        super().__init__()\n        self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(N)])\n        self.norm = LayerNorm(layer.size)\n        \n    def forward(self, x, mask):\n        for layer in self.layers:\n            x = layer(x, mask)\n        return self.norm(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DecoderLayer(nn.Module):\n    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n        super().__init__()\n        self.size = size\n        self.self_attn = self_attn\n        self.src_attn = src_attn\n        self.feed_forward = feed_forward\n        self.sublayer = nn.ModuleList([ResidualConnection(size, dropout) for _ in range(3)])\n\n    def forward(self, x, memory, src_mask, tgt_mask):\n        m = memory\n        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n        return self.sublayer[2](x, self.feed_forward)\n\nclass Decoder(nn.Module):\n    def __init__(self, layer, N):\n        super().__init__()\n        self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(N)])\n        self.norm = LayerNorm(layer.size)\n        \n    def forward(self, x, memory, src_mask, tgt_mask):\n        for layer in self.layers:\n            x = layer(x, memory, src_mask, tgt_mask)\n        return self.norm(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.src_embed = src_embed\n        self.tgt_embed = tgt_embed\n        self.generator = generator\n        \n    def encode(self, src, src_mask):\n        return self.encoder(self.src_embed(src), src_mask)\n    \n    def decode(self, memory, src_mask, tgt, tgt_mask):\n        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n    \n    def forward(self, src, tgt, src_mask, tgt_mask):\n        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n\ndef make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n    c = copy.deepcopy\n    attn = MultiHeadAttention(h, d_model)\n    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n    position = PositionalEncoding(d_model, dropout)\n    \n    model = Transformer(\n        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n        nn.Linear(d_model, tgt_vocab)\n    )\n    \n    for p in model.parameters():\n        if p.dim() > 1:\n            nn.init.xavier_uniform_(p)\n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Batch:\n    def __init__(self, src, tgt=None, pad=0):\n        self.src = src\n        self.src_mask = (src != pad).unsqueeze(-2)\n        if tgt is not None:\n            self.tgt = tgt[:, :-1]\n            self.tgt_y = tgt[:, 1:]\n            self.tgt_mask = self.make_std_mask(self.tgt, pad)\n            self.ntokens = (self.tgt_y != pad).data.sum()\n\n    @staticmethod\n    def make_std_mask(tgt, pad):\n        tgt_mask = (tgt != pad).unsqueeze(-2)\n        seq_len = tgt.size(-1)\n        look_ahead_mask = torch.tril(torch.ones(seq_len, seq_len)).bool()\n        return tgt_mask & look_ahead_mask.to(tgt.device)\n\ndef data_gen(V, batch, nbatches):\n    for _ in range(nbatches):\n        data = torch.randint(1, V, (batch, 10))\n        data[:, 0] = 1\n        src = data.clone()\n        tgt = data.clone()\n        yield Batch(src, tgt, 0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SimpleLossCompute:\n    def __init__(self, generator, criterion):\n        self.generator = generator\n        self.criterion = criterion\n        \n    def __call__(self, x, y, norm):\n        x = self.generator(x)\n        loss = self.criterion(x.reshape(-1, x.size(-1)), \n                             y.reshape(-1)) / norm\n        return loss.data * norm, loss\n\ndef run_epoch(data_iter, model, loss_compute, optimizer=None, scheduler=None):\n    total_loss = 0\n    total_tokens = 0\n    \n    for i, batch in enumerate(data_iter):\n        out = model(batch.src, batch.tgt, \n                   batch.src_mask, batch.tgt_mask)\n                   \n        loss, loss_node = loss_compute(out, batch.tgt_y, batch.ntokens)\n        \n        if optimizer is not None:\n            optimizer.zero_grad()\n            loss_node.backward()\n            optimizer.step()\n            if scheduler is not None:\n                scheduler.step()\n\n        total_loss += loss\n        total_tokens += batch.ntokens\n        \n    return total_loss / total_tokens","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def example_synthetic():\n    V = 11\n    model = make_model(V, V, N=2)\n    criterion = nn.CrossEntropyLoss(ignore_index=0)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.5, betas=(0.9, 0.98), eps=1e-9)\n    \n    batch_size = 80\n    for epoch in range(20):\n        model.train()\n        run_epoch(data_gen(V, batch_size, 20), model, \n                 SimpleLossCompute(model.generator, criterion), \n                 optimizer)\n        \n        model.eval()\n        loss = run_epoch(data_gen(V, batch_size, 5), model, \n                        SimpleLossCompute(model.generator, criterion), \n                        None)\n        print(f\"Epoch {epoch+1} | Loss: {loss:.2f}\")\n    \n    model.eval()\n    src = torch.tensor([[1,2,3,4,5,6,7,8,9,10]], dtype=torch.long)\n    src_mask = torch.ones(1, 1, 10, dtype=torch.bool)\n    memory = model.encode(src, src_mask)\n    ys = torch.zeros(1, 1).type_as(src)\n    \n    for i in range(9):\n        out = model.decode(memory, src_mask, \n                          ys, \n                          Batch.make_std_mask(ys, 0))\n        prob = model.generator(out[:, -1])\n        _, next_word = torch.max(prob, dim=1)\n        ys = torch.cat([ys, next_word.unsqueeze(1)], dim=1)\n    \n    print(\"Example Output:\", ys)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"example_synthetic()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T23:15:35.910181Z","iopub.execute_input":"2025-02-12T23:15:35.910448Z","iopub.status.idle":"2025-02-12T23:19:22.000963Z","shell.execute_reply.started":"2025-02-12T23:15:35.910428Z","shell.execute_reply":"2025-02-12T23:19:21.999917Z"}},"outputs":[{"name":"stdout","text":"Epoch 1 | Loss: 0.42\nEpoch 2 | Loss: 0.15\nEpoch 3 | Loss: 0.13\nEpoch 4 | Loss: 0.24\nEpoch 5 | Loss: 0.12\nEpoch 6 | Loss: 0.06\nEpoch 7 | Loss: 0.05\nEpoch 8 | Loss: 0.03\nEpoch 9 | Loss: 0.02\nEpoch 10 | Loss: 0.05\nEpoch 11 | Loss: 0.09\nEpoch 12 | Loss: 0.19\nEpoch 13 | Loss: 0.09\nEpoch 14 | Loss: 0.20\nEpoch 15 | Loss: 0.10\nEpoch 16 | Loss: 0.06\nEpoch 17 | Loss: 0.04\nEpoch 18 | Loss: 0.03\nEpoch 19 | Loss: 0.06\nEpoch 20 | Loss: 0.09\nExample Output: tensor([[0, 4, 4, 4, 4, 4, 4, 4, 4, 4]])\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}