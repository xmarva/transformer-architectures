{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install -q torchdata==0.3.0 torchtext==0.12 spacy==3.2 altair GPUtil","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:48:55.570618Z","iopub.execute_input":"2025-02-14T21:48:55.570845Z","iopub.status.idle":"2025-02-14T21:50:13.795849Z","shell.execute_reply.started":"2025-02-14T21:48:55.570825Z","shell.execute_reply":"2025-02-14T21:50:13.794961Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nos.environ['TORCHDYNAMO_DISABLE'] = '1' ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:50:13.796781Z","iopub.execute_input":"2025-02-14T21:50:13.797019Z","iopub.status.idle":"2025-02-14T21:50:13.801168Z","shell.execute_reply.started":"2025-02-14T21:50:13.796981Z","shell.execute_reply":"2025-02-14T21:50:13.800376Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nimport time\nimport math\nimport copy\nimport spacy\nimport GPUtil\nimport pandas as pd\nfrom typing import *\nfrom itertools import chain\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.optim.lr_scheduler import LambdaLR\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data import DataLoader, Dataset\n\nimport altair as alt\nfrom altair import Chart\n\nalt.data_transformers.disable_max_rows()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:50:13.801931Z","iopub.execute_input":"2025-02-14T21:50:13.802235Z","iopub.status.idle":"2025-02-14T21:50:31.534432Z","shell.execute_reply.started":"2025-02-14T21:50:13.802208Z","shell.execute_reply":"2025-02-14T21:50:31.533643Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"DataTransformerRegistry.enable('default')"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:50:31.535136Z","iopub.execute_input":"2025-02-14T21:50:31.535366Z","iopub.status.idle":"2025-02-14T21:50:31.540015Z","shell.execute_reply.started":"2025-02-14T21:50:31.535348Z","shell.execute_reply":"2025-02-14T21:50:31.539163Z"}},"outputs":[{"name":"stdout","text":"–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: cuda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"class LayerNorm(nn.Module):\n    def __init__(self, features, eps=1e-6):\n        super().__init__()\n        self.gamma = nn.Parameter(torch.ones(features))\n        self.beta = nn.Parameter(torch.zeros(features))\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n        return self.gamma * (x - mean) / (std + self.eps) + self.beta","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:50:31.540819Z","iopub.execute_input":"2025-02-14T21:50:31.541071Z","iopub.status.idle":"2025-02-14T21:50:31.559236Z","shell.execute_reply.started":"2025-02-14T21:50:31.541051Z","shell.execute_reply":"2025-02-14T21:50:31.558555Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class ResidualConnection(nn.Module):\n    def __init__(self, size, dropout):\n        super().__init__()\n        self.norm = LayerNorm(size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, sublayer):\n        return x + self.dropout(sublayer(self.norm(x)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:50:31.561677Z","iopub.execute_input":"2025-02-14T21:50:31.561922Z","iopub.status.idle":"2025-02-14T21:50:31.575038Z","shell.execute_reply.started":"2025-02-14T21:50:31.561904Z","shell.execute_reply":"2025-02-14T21:50:31.574380Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def attention(query, key, value, mask=None, dropout=None):\n    d_k = query.size(-1)\n    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n    \n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)\n        \n    p_attn = F.softmax(scores, dim=-1)\n    if dropout is not None:\n        p_attn = dropout(p_attn)\n        \n    return torch.matmul(p_attn, value), p_attn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:50:31.576418Z","iopub.execute_input":"2025-02-14T21:50:31.576611Z","iopub.status.idle":"2025-02-14T21:50:31.587908Z","shell.execute_reply.started":"2025-02-14T21:50:31.576594Z","shell.execute_reply":"2025-02-14T21:50:31.587297Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, h, d_model, dropout=0.1):\n        super().__init__()\n        assert d_model % h == 0\n        self.d_k = d_model // h\n        self.h = h\n        self.linears = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(4)])\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, query, key, value, mask=None):\n        if mask is not None:\n            mask = mask.unsqueeze(1)\n            \n        batch_size = query.size(0)\n        \n        query, key, value = [\n            lin(x).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n            for lin, x in zip(self.linears, (query, key, value))\n        ]\n        \n        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)\n        \n        x = x.transpose(1, 2).contiguous() \\\n             .view(batch_size, -1, self.h * self.d_k)\n             \n        return self.linears[-1](x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:50:31.588725Z","iopub.execute_input":"2025-02-14T21:50:31.588959Z","iopub.status.idle":"2025-02-14T21:50:31.602682Z","shell.execute_reply.started":"2025-02-14T21:50:31.588928Z","shell.execute_reply":"2025-02-14T21:50:31.602088Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class PositionwiseFeedForward(nn.Module):\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super().__init__()\n        self.w1 = nn.Linear(d_model, d_ff)\n        self.w2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        return self.w2(self.dropout(F.relu(self.w1(x))))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:50:31.603521Z","iopub.execute_input":"2025-02-14T21:50:31.603802Z","iopub.status.idle":"2025-02-14T21:50:31.619977Z","shell.execute_reply.started":"2025-02-14T21:50:31.603775Z","shell.execute_reply":"2025-02-14T21:50:31.619301Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class Embeddings(nn.Module):\n    def __init__(self, d_model, vocab):\n        super().__init__()\n        self.lut = nn.Embedding(vocab, d_model)\n        self.d_model = d_model\n\n    def forward(self, x):\n        return self.lut(x) * math.sqrt(self.d_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:50:31.620627Z","iopub.execute_input":"2025-02-14T21:50:31.620844Z","iopub.status.idle":"2025-02-14T21:50:31.634252Z","shell.execute_reply.started":"2025-02-14T21:50:31.620827Z","shell.execute_reply":"2025-02-14T21:50:31.633595Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout, max_len=5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        \n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n                             (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:, :x.size(1)].detach()\n        return self.dropout(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:50:31.634894Z","iopub.execute_input":"2025-02-14T21:50:31.635109Z","iopub.status.idle":"2025-02-14T21:50:31.651232Z","shell.execute_reply.started":"2025-02-14T21:50:31.635079Z","shell.execute_reply":"2025-02-14T21:50:31.650602Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class EncoderLayer(nn.Module):\n    def __init__(self, size, self_attn, feed_forward, dropout):\n        super().__init__()\n        self.self_attn = self_attn\n        self.feed_forward = feed_forward\n        self.sublayer = nn.ModuleList([ResidualConnection(size, dropout) for _ in range(2)])\n        self.size = size\n\n    def forward(self, x, mask):\n        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n        return self.sublayer[1](x, self.feed_forward)\n\nclass Encoder(nn.Module):\n    def __init__(self, layer, N):\n        super().__init__()\n        self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(N)])\n        self.norm = LayerNorm(layer.size)\n        \n    def forward(self, x, mask):\n        for layer in self.layers:\n            x = layer(x, mask)\n        return self.norm(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:50:31.651944Z","iopub.execute_input":"2025-02-14T21:50:31.652241Z","iopub.status.idle":"2025-02-14T21:50:31.673099Z","shell.execute_reply.started":"2025-02-14T21:50:31.652214Z","shell.execute_reply":"2025-02-14T21:50:31.672381Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class DecoderLayer(nn.Module):\n    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n        super().__init__()\n        self.size = size\n        self.self_attn = self_attn\n        self.src_attn = src_attn\n        self.feed_forward = feed_forward\n        self.sublayer = nn.ModuleList([ResidualConnection(size, dropout) for _ in range(3)])\n\n    def forward(self, x, memory, src_mask, tgt_mask):\n        m = memory\n        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n        return self.sublayer[2](x, self.feed_forward)\n\nclass Decoder(nn.Module):\n    def __init__(self, layer, N):\n        super().__init__()\n        self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(N)])\n        self.norm = LayerNorm(layer.size)\n        \n    def forward(self, x, memory, src_mask, tgt_mask):\n        for layer in self.layers:\n            x = layer(x, memory, src_mask, tgt_mask)\n        return self.norm(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:50:31.673769Z","iopub.execute_input":"2025-02-14T21:50:31.673947Z","iopub.status.idle":"2025-02-14T21:50:31.682858Z","shell.execute_reply.started":"2025-02-14T21:50:31.673931Z","shell.execute_reply":"2025-02-14T21:50:31.682004Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.src_embed = src_embed\n        self.tgt_embed = tgt_embed\n        self.generator = generator\n        \n    def encode(self, src, src_mask):\n        return self.encoder(self.src_embed(src), src_mask)\n    \n    def decode(self, memory, src_mask, tgt, tgt_mask):\n        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n    \n    def forward(self, src, tgt, src_mask, tgt_mask):\n        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n\ndef make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n    c = copy.deepcopy\n    attn = MultiHeadAttention(h, d_model)\n    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n    position = PositionalEncoding(d_model, dropout)\n    \n    model = Transformer(\n        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n        nn.Linear(d_model, tgt_vocab)\n    )\n    \n    for p in model.parameters():\n        if p.dim() > 1:\n            nn.init.xavier_uniform_(p)\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:50:31.683616Z","iopub.execute_input":"2025-02-14T21:50:31.683883Z","iopub.status.idle":"2025-02-14T21:50:31.700353Z","shell.execute_reply.started":"2025-02-14T21:50:31.683857Z","shell.execute_reply":"2025-02-14T21:50:31.699706Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class Batch:\n    def __init__(self, src, tgt=None, pad=0):\n        self.src = src\n        self.src_mask = (src != pad).unsqueeze(-2)\n        if tgt is not None:\n            self.tgt = tgt[:, :-1]\n            self.tgt_y = tgt[:, 1:]\n            self.tgt_mask = self.make_std_mask(self.tgt, pad)\n            self.ntokens = (self.tgt_y != pad).data.sum()\n\n    @staticmethod\n    def make_std_mask(tgt, pad):\n        tgt_mask = (tgt != pad).unsqueeze(-2)\n        seq_len = tgt.size(-1)\n        look_ahead_mask = torch.tril(torch.ones(seq_len, seq_len)).bool()\n        return tgt_mask & look_ahead_mask.to(tgt.device)\n\ndef data_gen(V, batch, nbatches):\n    for _ in range(nbatches):\n        data = torch.randint(1, V, (batch, 10))\n        data[:, 0] = 1\n        src = data.clone()\n        tgt = data.clone()\n        yield Batch(src, tgt, 0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:50:31.700981Z","iopub.execute_input":"2025-02-14T21:50:31.701182Z","iopub.status.idle":"2025-02-14T21:50:31.717731Z","shell.execute_reply.started":"2025-02-14T21:50:31.701165Z","shell.execute_reply":"2025-02-14T21:50:31.716972Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class SimpleLossCompute:\n    def __init__(self, generator, criterion):\n        self.generator = generator\n        self.criterion = criterion\n        \n    def __call__(self, x, y, norm):\n        x = self.generator(x)\n        loss = self.criterion(x.reshape(-1, x.size(-1)), \n                             y.reshape(-1)) / norm\n        return loss.data * norm, loss\n\ndef run_epoch(data_iter, model, loss_compute, optimizer=None, scheduler=None):\n    total_loss = 0\n    total_tokens = 0\n    \n    for i, batch in enumerate(data_iter):\n        out = model(batch.src, batch.tgt, \n                   batch.src_mask, batch.tgt_mask)\n                   \n        loss, loss_node = loss_compute(out, batch.tgt_y, batch.ntokens)\n        \n        if optimizer is not None:\n            optimizer.zero_grad()\n            loss_node.backward()\n            optimizer.step()\n            if scheduler is not None:\n                scheduler.step()\n\n        total_loss += loss\n        total_tokens += batch.ntokens\n        \n    return total_loss / total_tokens","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:50:31.718545Z","iopub.execute_input":"2025-02-14T21:50:31.718813Z","iopub.status.idle":"2025-02-14T21:50:31.736571Z","shell.execute_reply.started":"2025-02-14T21:50:31.718788Z","shell.execute_reply":"2025-02-14T21:50:31.735882Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def example_synthetic():\n    V = 11\n    model = make_model(V, V, N=2)\n    criterion = nn.CrossEntropyLoss(ignore_index=0)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.5, betas=(0.9, 0.98), eps=1e-9)\n    \n    batch_size = 80\n    for epoch in range(20):\n        model.train()\n        run_epoch(data_gen(V, batch_size, 20), model, \n                 SimpleLossCompute(model.generator, criterion), \n                 optimizer)\n        \n        model.eval()\n        loss = run_epoch(data_gen(V, batch_size, 5), model, \n                        SimpleLossCompute(model.generator, criterion), \n                        None)\n        print(f\"Epoch {epoch+1} | Loss: {loss:.2f}\")\n    \n    model.eval()\n    src = torch.tensor([[1,2,3,4,5,6,7,8,9,10]], dtype=torch.long)\n    src_mask = torch.ones(1, 1, 10, dtype=torch.bool)\n    memory = model.encode(src, src_mask)\n    ys = torch.zeros(1, 1).type_as(src)\n    \n    for i in range(9):\n        out = model.decode(memory, src_mask, \n                          ys, \n                          Batch.make_std_mask(ys, 0))\n        prob = model.generator(out[:, -1])\n        _, next_word = torch.max(prob, dim=1)\n        ys = torch.cat([ys, next_word.unsqueeze(1)], dim=1)\n    \n    print(\"Example Output:\", ys)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:50:31.737248Z","iopub.execute_input":"2025-02-14T21:50:31.737506Z","iopub.status.idle":"2025-02-14T21:50:31.750693Z","shell.execute_reply.started":"2025-02-14T21:50:31.737488Z","shell.execute_reply":"2025-02-14T21:50:31.749947Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"#example_synthetic()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:50:31.751450Z","iopub.execute_input":"2025-02-14T21:50:31.751666Z","iopub.status.idle":"2025-02-14T21:50:31.769559Z","shell.execute_reply.started":"2025-02-14T21:50:31.751645Z","shell.execute_reply":"2025-02-14T21:50:31.768891Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"import os\nimport time\nimport math\nimport copy\nfrom itertools import chain\nfrom typing import Optional, Tuple\n\nimport spacy\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom torch.optim.lr_scheduler import LambdaLR\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torch.nn.utils.rnn import pad_sequence\nfrom datasets import load_dataset\n\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\nimport altair as alt\nfrom altair import Chart","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:53:06.031578Z","iopub.execute_input":"2025-02-14T21:53:06.031954Z","iopub.status.idle":"2025-02-14T21:53:07.141740Z","shell.execute_reply.started":"2025-02-14T21:53:06.031921Z","shell.execute_reply":"2025-02-14T21:53:07.141058Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"print(\"üöÄ –ó–∞–≥—Ä—É–∑–∫–∞ Tatoeba en-ru...\")\ntry:\n    dataset = load_dataset(\n        \"Helsinki-NLP/tatoeba\",\n        lang1=\"en\", \n        lang2=\"ru\",\n        trust_remote_code=True\n    )\nexcept Exception as e:\n    print(f\"‚ùå –û—à–∏–±–∫–∞: {e}\")\n    raise\n\nprint(\"\\nüîç –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞:\")\nprint(dataset)\nprint(\"\\n–ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö:\")\nfor i in range(2):\n    print(f\"EN: {dataset['train'][i]['translation']['en']}\")\n    print(f\"RU: {dataset['train'][i]['translation']['ru']}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:53:08.299643Z","iopub.execute_input":"2025-02-14T21:53:08.300182Z","iopub.status.idle":"2025-02-14T21:53:26.389472Z","shell.execute_reply.started":"2025-02-14T21:53:08.300155Z","shell.execute_reply":"2025-02-14T21:53:26.388605Z"}},"outputs":[{"name":"stdout","text":"üöÄ –ó–∞–≥—Ä—É–∑–∫–∞ Tatoeba en-ru...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/8.93k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"557638752ab44d778a8790171b34d9ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tatoeba.py:   0%|          | 0.00/4.41k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f584db69eb0c4f0480576be04f3fe792"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/14.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e92cd2fc5884410599eacbd6a7ec2ce3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7a0e5bc062346008ace443bb1ad4525"}},"metadata":{}},{"name":"stdout","text":"\nüîç –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞:\nDatasetDict({\n    train: Dataset({\n        features: ['id', 'translation'],\n        num_rows: 523656\n    })\n})\n\n–ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö:\nEN: For once in my life I'm doing a good deed... And it is useless.\nRU: –û–¥–∏–Ω —Ä–∞–∑ –≤ –∂–∏–∑–Ω–∏ —è –¥–µ–ª–∞—é —Ö–æ—Ä–æ—à–µ–µ –¥–µ–ª–æ... –ò –æ–Ω–æ –±–µ—Å–ø–æ–ª–µ–∑–Ω–æ.\n\nEN: Let's try something.\nRU: –î–∞–≤–∞–π—Ç–µ —á—Ç–æ-–Ω–∏–±—É–¥—å –ø–æ–ø—Ä–æ–±—É–µ–º!\n\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"spacy_en = spacy.blank('en')\nspacy_ru = spacy.blank('ru')\n\ndef tokenize(text, lang):\n    try:\n        return [tok.text.lower() for tok in lang.tokenizer(text) if tok.text.strip()]\n    except:\n        return []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:53:26.390531Z","iopub.execute_input":"2025-02-14T21:53:26.390764Z","iopub.status.idle":"2025-02-14T21:53:26.678092Z","shell.execute_reply.started":"2025-02-14T21:53:26.390744Z","shell.execute_reply":"2025-02-14T21:53:26.677268Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"MAX_LENGTH = 40\nSAMPLE_SIZE = 5000 \n\nprint(f\"\\nüîß –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–µ—Ä–≤—ã—Ö {SAMPLE_SIZE} –ø—Ä–∏–º–µ—Ä–æ–≤...\")\n\ndef process_batch(batch):\n    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã translation —Å –∫–ª—é—á–∞–º–∏ 'en' –∏ 'ru'\n    en_texts = [item['en'] for item in batch['translation']]\n    ru_texts = [item['ru'] for item in batch['translation']]\n    \n    processed = {'en_tokens': [], 'ru_tokens': []}\n    \n    for en, ru in zip(en_texts, ru_texts):\n        try:\n            en_toks = [tok.text.lower() for tok in spacy_en.tokenizer(en) if tok.text.strip()][:MAX_LENGTH]\n            ru_toks = [tok.text.lower() for tok in spacy_ru.tokenizer(ru) if tok.text.strip()][:MAX_LENGTH]\n        except Exception as e:\n            print(f\"–û—à–∏–±–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏: {e}\")\n            continue\n        \n        if 3 <= len(en_toks) <= MAX_LENGTH and 3 <= len(ru_toks) <= MAX_LENGTH:\n            processed['en_tokens'].append(en_toks)\n            processed['ru_tokens'].append(ru_toks)\n    \n    return processed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:53:26.679587Z","iopub.execute_input":"2025-02-14T21:53:26.679801Z","iopub.status.idle":"2025-02-14T21:53:26.686271Z","shell.execute_reply.started":"2025-02-14T21:53:26.679783Z","shell.execute_reply":"2025-02-14T21:53:26.685621Z"}},"outputs":[{"name":"stdout","text":"\nüîß –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–µ—Ä–≤—ã—Ö 5000 –ø—Ä–∏–º–µ—Ä–æ–≤...\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"processed_data = dataset['train'].map(\n    process_batch,\n    batched=True,\n    batch_size=1000,\n    remove_columns=dataset['train'].column_names\n).filter(lambda x: len(x['en_tokens']) > 0)\n\nfinal_data = {\n    'en_tokens': processed_data['en_tokens'][:SAMPLE_SIZE],\n    'ru_tokens': processed_data['ru_tokens'][:SAMPLE_SIZE]\n}\n\nprint(f\"‚úÖ –û—Å—Ç–∞–ª–æ—Å—å –ø—Ä–∏–º–µ—Ä–æ–≤: {len(final_data['en_tokens'])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:53:26.687473Z","iopub.execute_input":"2025-02-14T21:53:26.687733Z","iopub.status.idle":"2025-02-14T21:54:25.981710Z","shell.execute_reply.started":"2025-02-14T21:53:26.687708Z","shell.execute_reply":"2025-02-14T21:54:25.980607Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/523656 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4348f3f4aeb4a65ba419abb57b3795a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/521963 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb7b4b8fc7c54af2b4f3d787ec27c4c7"}},"metadata":{}},{"name":"stdout","text":"‚úÖ –û—Å—Ç–∞–ª–æ—Å—å –ø—Ä–∏–º–µ—Ä–æ–≤: 5000\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"from collections import defaultdict\n\ndef build_compact_vocab(tokens_list, max_vocab=8000):\n    counter = Counter()\n    for tokens in tokens_list:\n        counter.update(tokens)\n    \n    vocab = ['<pad>', '<sos>', '<eos>', '<unk>'] + [tok for tok, cnt in counter.most_common(max_vocab-4)]\n    token_to_idx = defaultdict(lambda: 3, {tok:i for i, tok in enumerate(vocab)})\n    return token_to_idx, vocab  # –¢–µ–ø–µ—Ä—å –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏ —Å–ª–æ–≤–∞—Ä—å, –∏ —Å–ø–∏—Å–æ–∫\n\nprint(\"\\nüìò –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä–∏...\")\nen_vocab, en_vocab_list = build_compact_vocab(final_data['en_tokens'])\nru_vocab, ru_vocab_list = build_compact_vocab(final_data['ru_tokens'])\n\nprint(f\"–†–∞–∑–º–µ—Ä—ã —Å–ª–æ–≤–∞—Ä–µ–π:\")\nprint(f\"–ê–Ω–≥–ª–∏–π—Å–∫–∏–π: {len(en_vocab)}\")\nprint(f\"–†—É—Å—Å–∫–∏–π: {len(ru_vocab)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T22:10:08.707043Z","iopub.execute_input":"2025-02-14T22:10:08.707433Z","iopub.status.idle":"2025-02-14T22:10:08.737217Z","shell.execute_reply.started":"2025-02-14T22:10:08.707398Z","shell.execute_reply":"2025-02-14T22:10:08.736238Z"}},"outputs":[{"name":"stdout","text":"\nüìò –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä–∏...\n–†–∞–∑–º–µ—Ä—ã —Å–ª–æ–≤–∞—Ä–µ–π:\n–ê–Ω–≥–ª–∏–π—Å–∫–∏–π: 3135\n–†—É—Å—Å–∫–∏–π: 6096\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"print(\"–¢–æ–ø-10 —á–∞—Å—Ç—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤:\")\nprint(\"EN:\", list(en_vocab.keys())[4:14])  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ\nprint(\"RU:\", list(ru_vocab.keys())[4:14])\n\n\ndef coverage(vocab, tokens_list):\n    total = sum(len(t) for t in tokens_list)\n    covered = sum(len([t for t in tokens if t in vocab]) for tokens in tokens_list)\n    return covered / total\n\nprint(f\"EN Coverage: {coverage(en_vocab, final_data['en_tokens']):.2%}\")\nprint(f\"RU Coverage: {coverage(ru_vocab, final_data['ru_tokens']):.2%}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T21:55:17.048289Z","iopub.execute_input":"2025-02-14T21:55:17.048625Z","iopub.status.idle":"2025-02-14T21:55:17.066429Z","shell.execute_reply.started":"2025-02-14T21:55:17.048597Z","shell.execute_reply":"2025-02-14T21:55:17.065738Z"}},"outputs":[{"name":"stdout","text":"–¢–æ–ø-10 —á–∞—Å—Ç—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤:\nEN: ['.', 'you', 'i', 'the', 'to', '?', 'a', 'your', 'is', 'do']\nRU: ['.', ',', '?', '—Ç—ã', '—è', '–Ω–µ', '–≤', '—á—Ç–æ', '–≤—ã', '–Ω–∞']\nEN Coverage: 100.00%\nRU Coverage: 100.00%\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"class TranslationDataset(Dataset):\n    def __init__(self, en_tokens, ru_tokens, en_vocab, ru_vocab):\n        self.en_tokens = en_tokens\n        self.ru_tokens = ru_tokens\n        self.en_vocab = en_vocab\n        self.ru_vocab = ru_vocab\n        self.pad_idx = en_vocab['<pad>']\n\n    def __len__(self):\n        return len(self.en_tokens)\n\n    def __getitem__(self, idx):\n        en = [self.en_vocab['<sos>']] + \\\n             [self.en_vocab.get(tok, self.en_vocab['<unk>']) for tok in self.en_tokens[idx]] + \\\n             [self.en_vocab['<eos>']]\n        \n        ru = [self.ru_vocab['<sos>']] + \\\n             [self.ru_vocab.get(tok, self.ru_vocab['<unk>']) for tok in self.ru_tokens[idx]] + \\\n             [self.ru_vocab['<eos>']]\n        \n        return torch.LongTensor(en), torch.LongTensor(ru)\n\ndef collate_fn(batch):\n    en_batch, ru_batch = zip(*batch)\n    \n    en_padded = pad_sequence(en_batch, padding_value=en_vocab['<pad>'], batch_first=True)\n    ru_padded = pad_sequence(ru_batch, padding_value=ru_vocab['<pad>'], batch_first=True)\n    \n    return Batch(\n        src=en_padded,\n        tgt=ru_padded,\n        pad=en_vocab['<pad>']\n    )\n\nmodel = make_model(\n    src_vocab=len(en_vocab),\n    tgt_vocab=len(ru_vocab),\n    N=6,\n    d_model=512,\n    d_ff=2048,\n    h=8,\n    dropout=0.1).to(device)\n\ncriterion = nn.CrossEntropyLoss(ignore_index=ru_vocab['<pad>'])\noptimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\nscheduler = LambdaLR(optimizer, lr_lambda=lambda step: min(\n    (step + 1e-8)**-0.5,  # –î–æ–±–∞–≤–ª—è–µ–º epsilon —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å\n    (step + 1) * (4000**-1.5)  # –ù–∞—á–∏–Ω–∞–µ–º —Å —à–∞–≥–∞ 1\n))\n\ntrain_dataset = TranslationDataset(\n    final_data['en_tokens'],\n    final_data['ru_tokens'],\n    en_vocab,\n    ru_vocab\n)\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=128,\n    collate_fn=collate_fn,\n    shuffle=True,\n    num_workers=4\n)\n\ndef train_real_data(model, train_loader, epochs=20):\n    model.train()\n    best_loss = float('inf')\n    \n    for epoch in range(epochs):\n        start_time = time.time()\n        total_loss = 0\n        total_tokens = 0\n        \n        for i, batch in enumerate(train_loader):\n            batch = Batch(\n                src=batch.src.to(device),\n                tgt=batch.tgt.to(device),\n                pad=en_vocab['<pad>']\n            )\n            \n            optimizer.zero_grad()\n            output = model(batch.src, batch.tgt, batch.src_mask, batch.tgt_mask)\n            \n            loss, loss_node = SimpleLossCompute(model.generator, criterion)(\n                output, batch.tgt_y, batch.ntokens\n            )\n            \n            loss_node.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n            \n            total_loss += loss\n            total_tokens += batch.ntokens\n            \n            if i % 50 == 0:\n                avg_loss = total_loss / total_tokens\n                print(f\"Epoch {epoch+1} | Batch {i} | Loss: {avg_loss:.4f}\")\n        \n        avg_loss = total_loss / total_tokens\n        if avg_loss < best_loss:\n            best_loss = avg_loss\n            torch.save(model.state_dict(), \"best_model.pth\")\n        \n        print(f\"Epoch {epoch+1} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ | Loss: {avg_loss:.4f} | –í—Ä–µ–º—è: {time.time()-start_time:.1f}s\")\n        \n        test_sentence = \"Hello world\"\n        translate(\n            model, \n            test_sentence, \n            en_vocab, \n            ru_vocab, \n            ru_vocab_list,  # –°–ø–∏—Å–æ–∫ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–æ–∑–¥–∞–Ω –∑–∞—Ä–∞–Ω–µ–µ\n            spacy_en        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n        )\n\ndef translate(model, sentence, en_vocab, ru_vocab, ru_vocab_list, spacy_tokenizer, max_len=50):\n    model.eval()\n    tokens = [tok.text.lower() for tok in spacy_tokenizer(sentence)]\n    \n    src = torch.LongTensor([\n        [en_vocab['<sos>']] + \n        [en_vocab.get(tok, en_vocab['<unk>']) for tok in tokens] + \n        [en_vocab['<eos>']\n    ]]).to(device)\n    \n    src_mask = (src != en_vocab['<pad>']).unsqueeze(-2)\n    \n    with torch.no_grad():\n        memory = model.encode(src, src_mask)\n    \n    ys = torch.ones(1, 1, dtype=torch.long).fill_(ru_vocab['<sos>']).to(device)\n    \n    for _ in range(max_len-1):\n        seq_len = ys.size(1)\n        look_ahead_mask = torch.tril(\n            torch.ones((seq_len, seq_len), dtype=torch.bool, device=device\n        ))\n        tgt_mask = (ys != ru_vocab['<pad>']).unsqueeze(-2) & look_ahead_mask\n        \n        with torch.no_grad():\n            out = model.decode(memory, src_mask, ys, tgt_mask)\n            prob = model.generator(out[:, -1])\n            _, next_word = torch.max(prob, dim=1)\n        \n        ys = torch.cat([ys, next_word.unsqueeze(1)], dim=1)\n        \n        if next_word == ru_vocab['<eos>']:\n            break\n    \n    translation = ' '.join([ru_vocab_list[idx] for idx in ys[0].cpu().numpy() \n                          if idx not in [ru_vocab['<sos>'], ru_vocab['<eos>']]])\n    print(f\"–ü–µ—Ä–µ–≤–æ–¥: '{translation}'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T22:13:46.123662Z","iopub.execute_input":"2025-02-14T22:13:46.124018Z","iopub.status.idle":"2025-02-14T22:13:46.683434Z","shell.execute_reply.started":"2025-02-14T22:13:46.123990Z","shell.execute_reply":"2025-02-14T22:13:46.682511Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using dvice: {device}\")\n\ntrain_real_data(model.to(device), train_loader, epochs=20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T22:13:49.481215Z","iopub.execute_input":"2025-02-14T22:13:49.481524Z","iopub.status.idle":"2025-02-14T22:18:50.980069Z","shell.execute_reply.started":"2025-02-14T22:13:49.481502Z","shell.execute_reply":"2025-02-14T22:18:50.979127Z"}},"outputs":[{"name":"stdout","text":"–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: cuda\nEpoch 1 | Batch 0 | Loss: 0.0085\nEpoch 1 –∑–∞–≤–µ—Ä—à–µ–Ω–∞ | Loss: 0.0087 | –í—Ä–µ–º—è: 14.6s\n–ü–µ—Ä–µ–≤–æ–¥: '–≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –≤–∑–≥–ª—è–¥–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å —Ç–æ–∂–µ –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —á–∞—Å—Ç–∏—á–Ω–æ –Ω—ÉÃÅ–∂–Ω–æ –ø–æ–¥—Ö–æ–¥—è—Ç –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —á–∞—Å—Ç–∏—á–Ω–æ —á–∞—Å—Ç–∏—á–Ω–æ —á–∞—Å—Ç–∏—á–Ω–æ'\nEpoch 2 | Batch 0 | Loss: 0.0085\nEpoch 2 –∑–∞–≤–µ—Ä—à–µ–Ω–∞ | Loss: 0.0087 | –í—Ä–µ–º—è: 14.1s\n–ü–µ—Ä–µ–≤–æ–¥: '–≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –≤–∑–≥–ª—è–¥–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å —Ç–æ–∂–µ –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —á–∞—Å—Ç–∏—á–Ω–æ –Ω—ÉÃÅ–∂–Ω–æ –ø–æ–¥—Ö–æ–¥—è—Ç –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —á–∞—Å—Ç–∏—á–Ω–æ —á–∞—Å—Ç–∏—á–Ω–æ —á–∞—Å—Ç–∏—á–Ω–æ'\nEpoch 3 | Batch 0 | Loss: 0.0082\nEpoch 3 –∑–∞–≤–µ—Ä—à–µ–Ω–∞ | Loss: 0.0087 | –í—Ä–µ–º—è: 14.9s\n–ü–µ—Ä–µ–≤–æ–¥: '–≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Å–æ–Ω –≤–∑–≥–ª—è–¥–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å —Ç–æ–∂–µ –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å —á–∞—Å—Ç–∏—á–Ω–æ —á–∞—Å—Ç–∏—á–Ω–æ —á–∞—Å—Ç–∏—á–Ω–æ'\nEpoch 4 | Batch 0 | Loss: 0.0085\nEpoch 4 –∑–∞–≤–µ—Ä—à–µ–Ω–∞ | Loss: 0.0087 | –í—Ä–µ–º—è: 15.2s\n–ü–µ—Ä–µ–≤–æ–¥: '–≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Ö–æ–∂—É –Ω–∞–≤–µ—Ä–Ω–æ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —á–∞—Å—Ç–∏—á–Ω–æ –Ω—ÉÃÅ–∂–Ω–æ –ø–æ–¥—Ö–æ–¥—è—Ç –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å —á–∞—Å—Ç–∏—á–Ω–æ –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å'\nEpoch 5 | Batch 0 | Loss: 0.0085\nEpoch 5 –∑–∞–≤–µ—Ä—à–µ–Ω–∞ | Loss: 0.0086 | –í—Ä–µ–º—è: 14.9s\n–ü–µ—Ä–µ–≤–æ–¥: '–≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Ö–æ–∂—É –Ω–∞–≤–µ—Ä–Ω–æ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —á–∞—Å—Ç–∏—á–Ω–æ –Ω—ÉÃÅ–∂–Ω–æ –ø–æ–¥—Ö–æ–¥—è—Ç –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å —á–∞—Å—Ç–∏—á–Ω–æ –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å'\nEpoch 6 | Batch 0 | Loss: 0.0083\nEpoch 6 –∑–∞–≤–µ—Ä—à–µ–Ω–∞ | Loss: 0.0086 | –í—Ä–µ–º—è: 14.8s\n–ü–µ—Ä–µ–≤–æ–¥: '–≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Ö–æ–∂—É –Ω–∞–≤–µ—Ä–Ω–æ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å —Ç–æ–∂–µ –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å —á–∞—Å—Ç–∏—á–Ω–æ –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å'\nEpoch 7 | Batch 0 | Loss: 0.0085\nEpoch 7 –∑–∞–≤–µ—Ä—à–µ–Ω–∞ | Loss: 0.0086 | –í—Ä–µ–º—è: 14.6s\n–ü–µ—Ä–µ–≤–æ–¥: '–≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Ö–æ–∂—É –≤–∑–≥–ª—è–¥–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å —Ç–æ–∂–µ –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —á–∞—Å—Ç–∏—á–Ω–æ –Ω—ÉÃÅ–∂–Ω–æ –ø—Ä–∏–¥—ë—à—å'\nEpoch 8 | Batch 0 | Loss: 0.0086\nEpoch 8 –∑–∞–≤–µ—Ä—à–µ–Ω–∞ | Loss: 0.0086 | –í—Ä–µ–º—è: 14.6s\n–ü–µ—Ä–µ–≤–æ–¥: '–≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Å–æ–Ω –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Å–æ–Ω –≤–∑–≥–ª—è–¥–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å —Ç–æ–∂–µ –∑–∞—Å—Ç–∞–≤–ª—è–µ—Ç–µ –≤–∑–≥–ª—è–¥–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω—ÉÃÅ–∂–Ω–æ –ø–æ–¥—Ö–æ–¥—è—Ç –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å –ø–æ–∂–∏–ª—ã–º –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å'\nEpoch 9 | Batch 0 | Loss: 0.0082\nEpoch 9 –∑–∞–≤–µ—Ä—à–µ–Ω–∞ | Loss: 0.0085 | –í—Ä–µ–º—è: 15.1s\n–ü–µ—Ä–µ–≤–æ–¥: '–≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Å–æ–Ω –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Å–æ–Ω –≤–∑–≥–ª—è–¥–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –∑–∞—Å—Ç–∞–≤–ª—è–µ—Ç–µ –≤–∑–≥–ª—è–¥–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –∑–∞—Å—Ç–∞–≤–ª—è–µ—Ç–µ –≤–∑–≥–ª—è–¥–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω—ÉÃÅ–∂–Ω–æ –Ω—ÉÃÅ–∂–Ω–æ –ø–æ–¥—Ö–æ–¥—è—Ç –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å –ø–æ–∂–∏–ª—ã–º –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å'\nEpoch 10 | Batch 0 | Loss: 0.0084\nEpoch 10 –∑–∞–≤–µ—Ä—à–µ–Ω–∞ | Loss: 0.0085 | –í—Ä–µ–º—è: 14.9s\n–ü–µ—Ä–µ–≤–æ–¥: '–≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Å–æ–Ω –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Å–æ–Ω –≤–∑–≥–ª—è–¥–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Å–æ–Ω –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –∑–∞—Å—Ç–∞–≤–ª—è–µ—Ç–µ –≤–∑–≥–ª—è–¥–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Å–æ–Ω –≤–∑–≥–ª—è–¥–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω—ÉÃÅ–∂–Ω–æ –Ω—ÉÃÅ–∂–Ω–æ –Ω—ÉÃÅ–∂–Ω–æ –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å —Ç–æ–∂–µ –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å –ø–æ–∂–∏–ª—ã–º –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å'\nEpoch 11 | Batch 0 | Loss: 0.0081\nEpoch 11 –∑–∞–≤–µ—Ä—à–µ–Ω–∞ | Loss: 0.0085 | –í—Ä–µ–º—è: 14.8s\n–ü–µ—Ä–µ–≤–æ–¥: '–≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Å–æ–Ω –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Å–æ–Ω –≤–∑–≥–ª—è–¥–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Å–æ–Ω –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Å–æ–Ω –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Å–æ–Ω –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –∑–∞—Å—Ç–∞–≤–ª—è–µ—Ç–µ –≤–∑–≥–ª—è–¥–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Å–æ–Ω –≤–∑–≥–ª—è–¥–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω—ÉÃÅ–∂–Ω–æ –Ω—ÉÃÅ–∂–Ω–æ –Ω—ÉÃÅ–∂–Ω–æ –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å —Ç–æ–∂–µ –ø–æ–¥—Ö–æ–¥—è—Ç –∑–∞—Å—Ç–∞–≤–ª—è–µ—Ç–µ –≤–∑–≥–ª—è–¥–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å –ø–æ–∂–∏–ª—ã–º –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å –∑–∞—Å—Ç–∞–≤–ª—è–µ—Ç–µ'\nEpoch 12 | Batch 0 | Loss: 0.0081\nEpoch 12 –∑–∞–≤–µ—Ä—à–µ–Ω–∞ | Loss: 0.0084 | –í—Ä–µ–º—è: 14.7s\n–ü–µ—Ä–µ–≤–æ–¥: '–≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –≤–∑–≥–ª—è–¥–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Å–æ–Ω –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Å–æ–Ω –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Å–æ–Ω –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Å–æ–Ω –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Å–æ–Ω –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Å–æ–Ω –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –∑–∞—Å—Ç–∞–≤–ª—è–µ—Ç–µ –≤–∑–≥–ª—è–¥–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Å–æ–Ω –≤–∑–≥–ª—è–¥–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω—ÉÃÅ–∂–Ω–æ –Ω—ÉÃÅ–∂–Ω–æ –Ω—ÉÃÅ–∂–Ω–æ –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å —Ç–æ–∂–µ –ø–æ–¥—Ö–æ–¥—è—Ç –∑–∞—Å—Ç–∞–≤–ª—è–µ—Ç–µ –≤–∑–≥–ª—è–¥–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å –ø–æ–∂–∏–ª—ã–º –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å –∑–∞—Å—Ç–∞–≤–ª—è–µ—Ç–µ'\nEpoch 13 | Batch 0 | Loss: 0.0079\nEpoch 13 –∑–∞–≤–µ—Ä—à–µ–Ω–∞ | Loss: 0.0084 | –í—Ä–µ–º—è: 14.7s\n–ü–µ—Ä–µ–≤–æ–¥: '–Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Å–æ–Ω –≤–∑–≥–ª—è–¥–∞ —Å–æ–Ω –≤–∑–≥–ª—è–¥–∞ —Å–æ–Ω –ø–æ–∂–∏–ª—ã–º —Å–æ–Ω –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Å–æ–Ω –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Å–æ–Ω –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Å–æ–Ω –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Å–æ–Ω –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Å–æ–Ω –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Å–æ–Ω –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –≤–∑–≥–ª—è–¥–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Å–æ–Ω –ø–∏–∫–Ω–∏–∫ —Ç–æ–∂–µ —Å–æ–Ω –≤–∑–≥–ª—è–¥–∞ –Ω—ÉÃÅ–∂–Ω–æ –Ω—ÉÃÅ–∂–Ω–æ –Ω—ÉÃÅ–∂–Ω–æ –Ω—ÉÃÅ–∂–Ω–æ –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥—Ö–æ–¥—è—Ç –∑–∞—Å—Ç–∞–≤–ª—è–µ—Ç–µ –≤–∑–≥–ª—è–¥–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å –ø–æ–∂–∏–ª—ã–º –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å –ø–æ–∂–∏–ª—ã–º –∑–∞—Å—Ç–∞–≤–ª—è–µ—Ç–µ'\nEpoch 14 | Batch 0 | Loss: 0.0083\nEpoch 14 –∑–∞–≤–µ—Ä—à–µ–Ω–∞ | Loss: 0.0083 | –í—Ä–µ–º—è: 14.8s\n–ü–µ—Ä–µ–≤–æ–¥: '–Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Å–æ–Ω –≤–∑–≥–ª—è–¥–∞ —Å–æ–Ω –ø–æ–∂–∏–ª—ã–º —Å–æ–Ω –ø–æ–∂–∏–ª—ã–º —Å–æ–Ω –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Å–æ–Ω –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Å–æ–Ω –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Å–æ–Ω –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Å–æ–Ω –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Å–æ–Ω –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Å–æ–Ω –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Å–æ–Ω –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ –ª–µ–∫–∞—Ä—Å—Ç–≤–∞ –ø–∏–∫–Ω–∏–∫ –∑–∞—Å—Ç–∞–≤–ª—è–µ—Ç–µ –≤–∑–≥–ª—è–¥–∞ —Å–æ–Ω –≤–∑–≥–ª—è–¥–∞ –Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Å–æ–Ω –Ω—ÉÃÅ–∂–Ω–æ –Ω—ÉÃÅ–∂–Ω–æ –Ω—ÉÃÅ–∂–Ω–æ –Ω—ÉÃÅ–∂–Ω–æ –Ω—ÉÃÅ–∂–Ω–æ –Ω—ÉÃÅ–∂–Ω–æ –Ω—ÉÃÅ–∂–Ω–æ –Ω—ÉÃÅ–∂–Ω–æ –Ω—ÉÃÅ–∂–Ω–æ –Ω—ÉÃÅ–∂–Ω–æ –ø–æ–¥—Ö–æ–¥—è—Ç –ø—Ä–∏–¥—ë—à—å –ø–æ–∂–∏–ª—ã–º –Ω—ÉÃÅ–∂–Ω–æ –Ω—ÉÃÅ–∂–Ω–æ –Ω—ÉÃÅ–∂–Ω–æ'\nEpoch 15 | Batch 0 | Loss: 0.0084\nEpoch 15 –∑–∞–≤–µ—Ä—à–µ–Ω–∞ | Loss: 0.0083 | –í—Ä–µ–º—è: 15.1s\n–ü–µ—Ä–µ–≤–æ–¥: '–Ω–µ–æ—Ç—Ä–∞–∑–∏–º–∞ —Å–æ–Ω –≤–∑–≥–ª—è–¥–∞'\nEpoch 16 | Batch 0 | Loss: 0.0085\nEpoch 16 –∑–∞–≤–µ—Ä—à–µ–Ω–∞ | Loss: 0.0082 | –í—Ä–µ–º—è: 14.9s\n–ü–µ—Ä–µ–≤–æ–¥: ''\nEpoch 17 | Batch 0 | Loss: 0.0085\nEpoch 17 –∑–∞–≤–µ—Ä—à–µ–Ω–∞ | Loss: 0.0082 | –í—Ä–µ–º—è: 14.6s\n–ü–µ—Ä–µ–≤–æ–¥: ''\nEpoch 18 | Batch 0 | Loss: 0.0077\nEpoch 18 –∑–∞–≤–µ—Ä—à–µ–Ω–∞ | Loss: 0.0081 | –í—Ä–µ–º—è: 14.6s\n–ü–µ—Ä–µ–≤–æ–¥: ''\nEpoch 19 | Batch 0 | Loss: 0.0086\nEpoch 19 –∑–∞–≤–µ—Ä—à–µ–Ω–∞ | Loss: 0.0081 | –í—Ä–µ–º—è: 14.9s\n–ü–µ—Ä–µ–≤–æ–¥: ''\nEpoch 20 | Batch 0 | Loss: 0.0079\nEpoch 20 –∑–∞–≤–µ—Ä—à–µ–Ω–∞ | Loss: 0.0080 | –í—Ä–µ–º—è: 14.7s\n–ü–µ—Ä–µ–≤–æ–¥: ''\n","output_type":"stream"}],"execution_count":56}]}