{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Загрузка данных","metadata":{}},{"cell_type":"code","source":"!pip install nltk\n!python -m nltk.downloader punkt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T22:04:14.726035Z","iopub.execute_input":"2025-02-21T22:04:14.726445Z","iopub.status.idle":"2025-02-21T22:04:25.023021Z","shell.execute_reply.started":"2025-02-21T22:04:14.726412Z","shell.execute_reply":"2025-02-21T22:04:25.022142Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.2.4)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk) (1.17.0)\n/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n  warn(RuntimeWarning(msg))\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport re\nimport spacy\nimport wandb\nimport numpy as np\nimport pandas as pd\nfrom datasets import load_dataset, DatasetDict\n\nfrom dataclasses import dataclass\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom itertools import chain\nfrom typing import List, Dict, Tuple\nfrom collections import Counter, defaultdict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T22:04:25.024255Z","iopub.execute_input":"2025-02-21T22:04:25.024500Z","iopub.status.idle":"2025-02-21T22:04:43.688300Z","shell.execute_reply.started":"2025-02-21T22:04:25.024476Z","shell.execute_reply":"2025-02-21T22:04:43.687391Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## WandB","metadata":{}},{"cell_type":"code","source":"import wandb \nfrom kaggle_secrets import UserSecretsClient","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T22:04:43.690161Z","iopub.execute_input":"2025-02-21T22:04:43.690724Z","iopub.status.idle":"2025-02-21T22:04:43.702327Z","shell.execute_reply.started":"2025-02-21T22:04:43.690690Z","shell.execute_reply":"2025-02-21T22:04:43.701713Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def init_wandb(project_name=\"translation-transformer\", config=None):\n    try:\n        user_secrets = UserSecretsClient()\n        \n        wandb_api_key = user_secrets.get_secret(\"wandb\")\n        os.environ['WANDB_API_KEY'] = wandb_api_key\n        \n        wandb.login(key=wandb_api_key)\n        \n        run = wandb.init(\n            project=project_name,\n            config=config\n        )\n        \n        print(\"✅ W&B successfully initialized\")\n        return run\n    \n    except Exception as e:\n        print(f\"❌ Error initializing W&B: {str(e)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T22:04:43.703575Z","iopub.execute_input":"2025-02-21T22:04:43.703782Z","iopub.status.idle":"2025-02-21T22:04:43.717371Z","shell.execute_reply.started":"2025-02-21T22:04:43.703764Z","shell.execute_reply":"2025-02-21T22:04:43.716757Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"config = {\n    \"architecture\": \"transformer\",\n    \"subset_size\": 100,\n    \"epochs\": 5,\n    \"batch_size\": 32,\n    \"learning_rate\": 0.0001,\n    \"eval_steps\": 50,\n    \"d_model\": 512,\n    \"num_heads\": 8,\n    \"num_layers\": 6,\n}\n\nrun = init_wandb(config=config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T22:04:43.717990Z","iopub.execute_input":"2025-02-21T22:04:43.718187Z","iopub.status.idle":"2025-02-21T22:04:56.594581Z","shell.execute_reply.started":"2025-02-21T22:04:43.718170Z","shell.execute_reply":"2025-02-21T22:04:56.593848Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meva-koroleva\u001b[0m (\u001b[33mml-samurai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250221_220450-ln6yprv4</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ml-samurai/translation-transformer/runs/ln6yprv4' target=\"_blank\">silvery-voice-5</a></strong> to <a href='https://wandb.ai/ml-samurai/translation-transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ml-samurai/translation-transformer' target=\"_blank\">https://wandb.ai/ml-samurai/translation-transformer</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ml-samurai/translation-transformer/runs/ln6yprv4' target=\"_blank\">https://wandb.ai/ml-samurai/translation-transformer/runs/ln6yprv4</a>"},"metadata":{}},{"name":"stdout","text":"✅ W&B successfully initialized\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"def load_translation_dataset():\n    print(\"Loading Tatoeba en-ru...\")\n    try:\n        dataset = load_dataset(\"Helsinki-NLP/tatoeba\", lang1=\"en\", lang2=\"ru\", trust_remote_code=True)\n        \n    except Exception as e:\n        print(f\"Error while loading dataset: {e}\")\n        raise\n    \n    print(\"\\nDataset structure:\")\n    print(dataset)\n    \n    print(\"\\nData sample:\")\n    for i in range(2):\n        print(f\"EN: {dataset['train'][i]['translation']['en']}\")\n        print(f\"RU: {dataset['train'][i]['translation']['ru']}\\n\")\n\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T22:04:56.595350Z","iopub.execute_input":"2025-02-21T22:04:56.595551Z","iopub.status.idle":"2025-02-21T22:04:56.600316Z","shell.execute_reply.started":"2025-02-21T22:04:56.595532Z","shell.execute_reply":"2025-02-21T22:04:56.599555Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"dataset = load_translation_dataset()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T22:04:56.601143Z","iopub.execute_input":"2025-02-21T22:04:56.601452Z","iopub.status.idle":"2025-02-21T22:05:13.827529Z","shell.execute_reply.started":"2025-02-21T22:04:56.601422Z","shell.execute_reply":"2025-02-21T22:05:13.826345Z"}},"outputs":[{"name":"stdout","text":"Loading Tatoeba en-ru...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/8.93k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c84b64708f3e4243b784bb1427f6c565"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tatoeba.py:   0%|          | 0.00/4.41k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84323af84cd64e1193e57ff0f9160895"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/14.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94f747285aec4357a596b6e3286c4592"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91ab031426cc41359e1cd9394cd2b62c"}},"metadata":{}},{"name":"stdout","text":"\nDataset structure:\nDatasetDict({\n    train: Dataset({\n        features: ['id', 'translation'],\n        num_rows: 523656\n    })\n})\n\nData sample:\nEN: For once in my life I'm doing a good deed... And it is useless.\nRU: Один раз в жизни я делаю хорошее дело... И оно бесполезно.\n\nEN: Let's try something.\nRU: Давайте что-нибудь попробуем!\n\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from transformers import AutoTokenizer\nimport numpy as np\n\ndef prepare_data_with_hf(\n    dataset, \n    model_name: str = \"Helsinki-NLP/opus-mt-en-ru\", \n    max_length: int = 128, \n    batch_size: int = 32\n):\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    \n    special_tokens = {\n        'bos_token': '<s>',\n        'eos_token': '</s>',\n        'pad_token': '<pad>',\n        'unk_token': '<unk>'\n    }\n    tokenizer.add_special_tokens(special_tokens)\n    \n    tokenizer.bos_token = '<s>'\n    tokenizer.eos_token = '</s>'\n    tokenizer.pad_token = '<pad>'\n    tokenizer.unk_token = '<unk>'\n    tokenizer.bos_token_id = tokenizer.convert_tokens_to_ids('<s>')\n    tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids('</s>')\n    tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids('<pad>')\n\n    def preprocess_function(examples):\n        source_texts = [\n            f\"{tokenizer.bos_token} {item['en']}\" \n            for item in examples['translation']\n        ]\n        \n        target_texts = [\n            f\"{tokenizer.bos_token} {item['ru']} {tokenizer.eos_token}\"\n            for item in examples['translation']\n        ]\n\n        source_encoding = tokenizer(\n            source_texts,\n            padding='max_length',\n            truncation=True,\n            max_length=max_length,\n            return_tensors='np',\n            add_special_tokens=False \n        )\n        \n        target_encoding = tokenizer(\n            target_texts,\n            padding='max_length',\n            truncation=True,\n            max_length=max_length,\n            return_tensors='np',\n            add_special_tokens=False\n        )\n\n        source_ids = source_encoding['input_ids']\n        source_ids[:, 0] = tokenizer.bos_token_id \n        \n        target_ids = target_encoding['input_ids']\n        target_ids[:, 0] = tokenizer.bos_token_id \n        \n        target_attention_mask = target_encoding['attention_mask']\n        seq_lens = np.argmin(target_attention_mask, axis=1)\n        seq_lens[seq_lens == 0] = target_attention_mask.shape[1]\n        \n        for i, pos in enumerate(seq_lens):\n            if pos < max_length:\n                target_ids[i, pos-1] = tokenizer.eos_token_id\n            else:\n                target_ids[i, -1] = tokenizer.eos_token_id\n\n        return {\n            'input_ids': source_ids,\n            'attention_mask': source_encoding['attention_mask'],\n            'labels': target_ids,\n            'decoder_attention_mask': target_encoding['attention_mask']\n        }\n    \n    processed_dataset = dataset['train'].map(\n        preprocess_function,\n        batched=True,\n        batch_size=batch_size,\n        remove_columns=dataset['train'].column_names\n    )\n    \n    return processed_dataset, tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T22:05:13.828835Z","iopub.execute_input":"2025-02-21T22:05:13.829243Z","iopub.status.idle":"2025-02-21T22:05:21.010136Z","shell.execute_reply.started":"2025-02-21T22:05:13.829200Z","shell.execute_reply":"2025-02-21T22:05:21.009527Z"}},"outputs":[{"name":"stderr","text":"The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fc18b7195c647aa8efbd52f3e5fc385"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"processed_data, hf_tokenizer = prepare_data_with_hf(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T22:05:21.012391Z","iopub.execute_input":"2025-02-21T22:05:21.012890Z","iopub.status.idle":"2025-02-21T22:08:21.414059Z","shell.execute_reply.started":"2025-02-21T22:05:21.012868Z","shell.execute_reply":"2025-02-21T22:08:21.413140Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"716fe700aab4432291837f816affa389"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b534fecc8ec34b5e80b4aba2fbfa05c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"source.spm:   0%|          | 0.00/803k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4a14d11f3404554810457c7e6280a8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"target.spm:   0%|          | 0.00/1.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48c3abcac5f74df19e193b7541ae24f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.60M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"562b79fef0b44060b15981003204364e"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n  warnings.warn(\"Recommended: pip install sacremoses.\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/523656 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7992199e90954fa99bfdf3933df348c5"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"sample = processed_data[0]\nprint(\"Source:\", hf_tokenizer.decode(sample['input_ids']))\nprint(\"Target:\", hf_tokenizer.decode(sample['labels']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T22:08:21.415642Z","iopub.execute_input":"2025-02-21T22:08:21.415872Z","iopub.status.idle":"2025-02-21T22:08:39.190822Z","shell.execute_reply.started":"2025-02-21T22:08:21.415851Z","shell.execute_reply":"2025-02-21T22:08:39.189944Z"}},"outputs":[{"name":"stdout","text":"Source: <s>  For once in my life I'm doing a good deed... And it is useless.<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\nTarget: <s> Один раз в жизни я делаю хорошее дело... И оно бесполезно.</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Архитектура модели","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install -q torchdata==0.3.0 torchtext==0.12 spacy==3.2 altair GPUtil\n!python -m spacy download de_core_news_sm\n!python -m spacy download en_core_web_sm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T22:08:39.191807Z","iopub.execute_input":"2025-02-21T22:08:39.192422Z","iopub.status.idle":"2025-02-21T22:10:35.911016Z","shell.execute_reply.started":"2025-02-21T22:08:39.192397Z","shell.execute_reply":"2025-02-21T22:10:35.910109Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import os\nimport time\nimport math\nimport copy\nimport spacy\nimport GPUtil\nimport pandas as pd\nfrom typing import *\nfrom itertools import chain\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.optim.lr_scheduler import LambdaLR\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data import DataLoader, Dataset\n\nimport altair as alt\nfrom altair import Chart\n\nalt.data_transformers.disable_max_rows()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T22:10:35.912024Z","iopub.execute_input":"2025-02-21T22:10:35.912283Z","iopub.status.idle":"2025-02-21T22:10:36.340476Z","shell.execute_reply.started":"2025-02-21T22:10:35.912248Z","shell.execute_reply":"2025-02-21T22:10:36.339724Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"DataTransformerRegistry.enable('default')"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout, max_len=5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        \n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n                             (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:, :x.size(1)].detach()\n        return self.dropout(x)\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n        \n        assert self.head_dim * num_heads == d_model, \"d_model must be divisible by num_heads\"\n        \n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n        \n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        # Q: [batch_size, num_heads, seq_len, head_dim]\n        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n        \n        if mask is not None:\n            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n        \n        attn_probs = F.softmax(attn_scores, dim=-1)\n        output = torch.matmul(attn_probs, V)\n        return output\n        \n    def forward(self, Q, K, V, mask=None):\n        batch_size = Q.size(0)\n        \n        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        \n        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n        \n        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n        \n        output = self.W_o(attn_output)\n        return output\n\nclass FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff=2048):\n        super().__init__()\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.linear2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(0.1)\n        \n    def forward(self, x):\n        x = self.dropout(F.relu(self.linear1(x)))\n        x = self.linear2(x)\n        return x\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.ffn = FeedForward(d_model)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(0.1)\n        \n    def forward(self, x, mask=None):\n        # Self attention\n        attn_output = self.self_attn(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        \n        # Feed forward\n        ffn_output = self.ffn(x)\n        x = self.norm2(x + self.dropout(ffn_output))\n        return x\n\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n        self.ffn = FeedForward(d_model)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(0.1)\n        \n    def forward(self, x, enc_output, src_mask, tgt_mask):\n        # Self attention (маскированное)\n        attn_output = self.self_attn(x, x, x, tgt_mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        \n        # Cross attention (с выходом энкодера)\n        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n        x = self.norm2(x + self.dropout(attn_output))\n        \n        # Feed forward\n        ffn_output = self.ffn(x)\n        x = self.norm3(x + self.dropout(ffn_output))\n        return x\n\nclass Transformer(nn.Module):\n    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, num_heads=8, num_layers=6):\n        super().__init__()\n        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n        self.positional_encoding = PositionalEncoding(d_model, dropout=0.1)\n        \n        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads) for _ in range(num_layers)])\n        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads) for _ in range(num_layers)])\n        \n        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n            \n    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n        # Проверка размерностей входных данных\n        batch_size = src.size(0)\n        src_seq_len = src.size(1)\n        tgt_seq_len = tgt.size(1)\n        \n        # Энкодинг\n        src_emb = self.positional_encoding(self.encoder_embedding(src))\n        enc_output = src_emb\n        \n        for layer in self.encoder_layers:\n            enc_output = layer(enc_output, src_mask)\n        \n        # Декодинг\n        tgt_emb = self.positional_encoding(self.decoder_embedding(tgt))\n        dec_output = tgt_emb\n        \n        for layer in self.decoder_layers:\n            dec_output = layer(dec_output, enc_output, src_mask, tgt_mask)\n        \n        # Финальный слой\n        output = self.fc_out(dec_output)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T22:10:36.341246Z","iopub.execute_input":"2025-02-21T22:10:36.341517Z","iopub.status.idle":"2025-02-21T22:10:36.937052Z","shell.execute_reply.started":"2025-02-21T22:10:36.341485Z","shell.execute_reply":"2025-02-21T22:10:36.936131Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def test_transformer():\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(42)\n    torch.manual_seed(42)\n    \n    batch_size = 2\n    seq_len = 10\n    d_model = 512\n    num_heads = 8\n    src_vocab_size = 100\n    tgt_vocab_size = 100\n    num_layers = 2\n\n    src = torch.randint(0, src_vocab_size, (batch_size, seq_len)).to(device)\n    tgt = torch.randint(0, tgt_vocab_size, (batch_size, seq_len)).to(device)\n    \n    src_mask = torch.ones(batch_size, 1, 1, seq_len).to(device)\n    tgt_mask = torch.tril(torch.ones(seq_len, seq_len)).expand(batch_size, 1, seq_len, seq_len).to(device)\n\n    transformer = Transformer(\n        src_vocab_size=src_vocab_size,\n        tgt_vocab_size=tgt_vocab_size,\n        d_model=d_model,\n        num_heads=num_heads,\n        num_layers=num_layers\n    ).to(device)\n\n    print(\"=\"*50)\n    print(\"1. Тест Positional Encoding\")\n    pe = PositionalEncoding(d_model, dropout=0.1).to(device)\n    x = torch.randn(1, seq_len, d_model).to(device)\n    print(f\"До PE: mean={x.mean().item():.4f}, std={x.std().item():.4f}\")\n    x_pe = pe(x)\n    print(f\"После PE: mean={x_pe.mean().item():.4f}, std={x_pe.std().item():.4f}\")\n    print(f\"Форма PE: {x_pe.shape} (должна быть [1, {seq_len}, {d_model}])\")\n    \n    print(\"\\n2. Тест Multi-Head Attention\")\n    mha = MultiHeadAttention(d_model, num_heads).to(device)\n    q = k = v = torch.randn(batch_size, seq_len, d_model).to(device)\n    attn_output = mha(q, k, v)\n    print(f\"Форма выхода внимания: {attn_output.shape} (должна быть {q.shape})\")\n    print(f\"Максимальное значение: {attn_output.max().item():.4f}\")\n    print(f\"Минимальное значение: {attn_output.min().item():.4f}\")\n\n    print(\"\\n3. Тест Encoder Layer\")\n    encoder_layer = EncoderLayer(d_model, num_heads).to(device)\n    enc_input = torch.randn(batch_size, seq_len, d_model).to(device)\n    enc_output = encoder_layer(enc_input)\n    print(f\"Форма выхода энкодера: {enc_output.shape} (должна быть {enc_input.shape})\")\n    print(f\"Изменение данных: {torch.allclose(enc_input, enc_output, atol=1e-4)} (должно быть False)\")\n\n    print(\"\\n4. Тест Decoder Layer\")\n    decoder_layer = DecoderLayer(d_model, num_heads).to(device)\n    dec_input = torch.randn(batch_size, seq_len, d_model).to(device)\n    dec_output = decoder_layer(dec_input, enc_output, src_mask, tgt_mask)\n    print(f\"Форма выхода декодера: {dec_output.shape} (должна быть {dec_input.shape})\")\n    print(f\"Норма выходных данных: {dec_output.norm().item():.4f}\")\n\n    print(\"\\n5. Полный тест Transformer\")\n    print(\"Входные данные:\")\n    print(f\"src: {src.shape} (max={src.max().item()}, min={src.min().item()})\")\n    print(f\"tgt: {tgt.shape} (max={tgt.max().item()}, min={tgt.min().item()})\")\n    \n    output = transformer(src, tgt, src_mask, tgt_mask)\n    print(\"\\nПроверка формы выхода:\")\n    print(f\"Ожидаемая форма: ({batch_size}, {seq_len}, {tgt_vocab_size})\")\n    print(f\"Реальная форма:   {output.shape}\")\n    \n    print(\"\\nПроверка градиентов:\")\n    dummy_loss = output.sum()\n    dummy_loss.backward()\n    has_gradients = any(p.grad is not None for p in transformer.parameters())\n    print(f\"Градиенты вычислены: {has_gradients} (должно быть True)\")\n\n    print(\"\\n6. Проверка параметров модели:\")\n    total_params = sum(p.numel() for p in transformer.parameters())\n    print(f\"Всего параметров: {total_params}\")\n    print(f\"Параметры энкодера: {sum(p.numel() for p in transformer.encoder_embedding.parameters())}\")\n    print(f\"Параметры декодера: {sum(p.numel() for p in transformer.decoder_embedding.parameters())}\")\n\n    print(\"\\nТест завершен!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T22:10:36.938032Z","iopub.execute_input":"2025-02-21T22:10:36.938373Z","iopub.status.idle":"2025-02-21T22:10:36.961898Z","shell.execute_reply.started":"2025-02-21T22:10:36.938328Z","shell.execute_reply":"2025-02-21T22:10:36.961339Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"test_transformer()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T22:10:36.962678Z","iopub.execute_input":"2025-02-21T22:10:36.962868Z","iopub.status.idle":"2025-02-21T22:10:37.968597Z","shell.execute_reply.started":"2025-02-21T22:10:36.962848Z","shell.execute_reply":"2025-02-21T22:10:37.967725Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n==================================================\n1. Тест Positional Encoding\nДо PE: mean=-0.0316, std=1.0065\nПосле PE: mean=0.4401, std=1.2013\nФорма PE: torch.Size([1, 10, 512]) (должна быть [1, 10, 512])\n\n2. Тест Multi-Head Attention\nФорма выхода внимания: torch.Size([2, 10, 512]) (должна быть torch.Size([2, 10, 512]))\nМаксимальное значение: 0.3949\nМинимальное значение: -0.4440\n\n3. Тест Encoder Layer\nФорма выхода энкодера: torch.Size([2, 10, 512]) (должна быть torch.Size([2, 10, 512]))\nИзменение данных: False (должно быть False)\n\n4. Тест Decoder Layer\nФорма выхода декодера: torch.Size([2, 10, 512]) (должна быть torch.Size([2, 10, 512]))\nНорма выходных данных: 101.1924\n\n5. Полный тест Transformer\nВходные данные:\nsrc: torch.Size([2, 10]) (max=95, min=6)\ntgt: torch.Size([2, 10]) (max=99, min=10)\n\nПроверка формы выхода:\nОжидаемая форма: (2, 10, 100)\nРеальная форма:   torch.Size([2, 10, 100])\n\nПроверка градиентов:\nГрадиенты вычислены: True (должно быть True)\n\n6. Проверка параметров модели:\nВсего параметров: 14866532\nПараметры энкодера: 51200\nПараметры декодера: 51200\n\nТест завершен!\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"def translate_sentence(model, tokenizer, sentence, device, max_length=128):\n    model.eval()\n    \n    inputs = tokenizer(\n        f\"{tokenizer.bos_token} {sentence}\",\n        padding='max_length',\n        truncation=True,\n        max_length=max_length,\n        return_tensors=\"pt\"\n    ).to(device)\n    \n    decoder_input = torch.tensor([[tokenizer.bos_token_id]], device=device)\n    \n    with torch.no_grad():\n        for _ in range(max_length):\n            src_mask = (inputs['input_ids'] != tokenizer.pad_token_id).unsqueeze(1).unsqueeze(2)\n            tgt_mask = torch.tril(torch.ones(decoder_input.size(1), decoder_input.size(1), device=device)).bool()\n            \n            output = model(\n                inputs['input_ids'],\n                decoder_input,\n                src_mask,\n                tgt_mask.unsqueeze(0).unsqueeze(0)\n            )\n            \n            next_token = output[:, -1].argmax(-1)\n            decoder_input = torch.cat([decoder_input, next_token.unsqueeze(1)], dim=1)\n            \n            if next_token.item() == tokenizer.eos_token_id:\n                break\n    \n    return tokenizer.decode(decoder_input[0], skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T22:10:37.969528Z","iopub.execute_input":"2025-02-21T22:10:37.969813Z","iopub.status.idle":"2025-02-21T22:10:37.976989Z","shell.execute_reply.started":"2025-02-21T22:10:37.969777Z","shell.execute_reply":"2025-02-21T22:10:37.976363Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"## Тестирование","metadata":{}},{"cell_type":"code","source":"import torch\nprint(torch.__version__)  # Должно быть >= 1.11.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T22:10:37.977736Z","iopub.execute_input":"2025-02-21T22:10:37.977927Z","iopub.status.idle":"2025-02-21T22:10:38.004536Z","shell.execute_reply.started":"2025-02-21T22:10:37.977909Z","shell.execute_reply":"2025-02-21T22:10:38.003696Z"}},"outputs":[{"name":"stdout","text":"2.5.1+cu121\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"def test_tokenization(tokenizer, test_sentence=\"Hello, how are you?\"):\n    print(\"=\"*50)\n    print(\"1. Тестирование токенизации и специальных токенов\")\n    \n    print(f\"BOS: {tokenizer.bos_token} (id: {tokenizer.bos_token_id})\")\n    print(f\"EOS: {tokenizer.eos_token} (id: {tokenizer.eos_token_id})\")\n    \n    # Явное добавление BOS/EOS как в основном пайплайне\n    processed_text = f\"{tokenizer.bos_token} {test_sentence} {tokenizer.eos_token}\"\n    \n    encoded = tokenizer(\n        processed_text,\n        padding='max_length',\n        truncation=True,\n        max_length=20,  # Уменьшаем для наглядности\n        return_tensors='pt',\n        add_special_tokens=False\n    )\n    \n    token_ids = encoded['input_ids'][0].tolist()\n    decoded = tokenizer.decode(token_ids)\n    \n    print(\"\\nПример токенизации:\")\n    print(f\"Токены: {token_ids}\")\n    print(f\"Декодировано: {decoded}\")\n    \n    try:\n        eos_pos = token_ids.index(tokenizer.eos_token_id)\n        pad_start = token_ids.index(tokenizer.pad_token_id)\n        assert eos_pos < pad_start, \"EOS должен быть перед паддингом\"\n    except ValueError:\n        assert False, \"EOS token not found\"\n    \n    assert token_ids[0] == tokenizer.bos_token_id\n    print(\"\\n✅ Токенизация работает корректно\")\n\ndef test_masking():\n    print(\"\\n\" + \"=\"*50)\n    print(\"2. Тестирование создания масок\")\n    \n    batch_size = 2\n    seq_len = 5\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    src = torch.ones(batch_size, seq_len, device=device)\n    tgt = torch.ones(batch_size, seq_len, device=device)\n    \n    src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n    \n    causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=device))\n    tgt_mask = causal_mask.unsqueeze(0).repeat(batch_size, 1, 1, 1)\n    \n    print(\"\\nSource mask (пример):\")\n    print(src_mask[0, 0, 0].cpu().numpy()) \n    \n    print(\"\\nTarget mask (пример):\")\n    print(tgt_mask[0, 0].cpu().numpy())\n    \n    # Проверка размерностей\n    assert src_mask.shape == (batch_size, 1, 1, seq_len), f\"Ожидалось (2,1,1,5), получено {src_mask.shape}\"\n    assert tgt_mask.shape == (batch_size, 1, seq_len, seq_len), f\"Ожидалось (2,1,5,5), получено {tgt_mask.shape}\"\n    \n    print(\"\\n✅ Маски создаются корректно\")\n\ndef test_model_forward_pass(model, tokenizer, device):\n    print(\"\\n\" + \"=\"*50)\n    print(\"3. Тестирование forward pass модели\")\n    \n    test_sentence = \"Test input\"\n    inputs = tokenizer(\n        test_sentence,\n        return_tensors=\"pt\",\n        padding='max_length',\n        max_length=128,\n        truncation=True\n    ).to(device)\n    \n    src = inputs['input_ids']\n    tgt = torch.cat([\n        torch.tensor([[tokenizer.bos_token_id]], device=device),\n        src[:, :-1]\n    ], dim=1)\n\n    src_mask = inputs['attention_mask'].to(torch.bool).unsqueeze(1).unsqueeze(2)\n    seq_len = tgt.size(1)\n    tgt_mask = torch.tril(torch.ones(seq_len, seq_len, device=device)).to(torch.bool)\n    tgt_mask = tgt_mask.unsqueeze(0).unsqueeze(0)\n    \n    model.eval()\n    with torch.no_grad():\n        output = model(src, tgt, src_mask, tgt_mask)\n    \n    print(\"\\nФорма выходов модели:\", output.shape)\n    print(\"Минимальное значение:\", output.min().item())\n    print(\"Максимальное значение:\", output.max().item())\n    \n    assert not torch.isnan(output).any()\n    expected_vocab_size = len(tokenizer)  # Используем реальный размер словаря\n    assert output.shape == (1, 128, expected_vocab_size), (\n        f\"Ожидаемая форма: (1, 128, {expected_vocab_size}), \"\n        f\"получено: {output.shape}\"\n    )\n    print(\"\\n✅ Forward pass работает корректно\")\n\ndef test_dynamic_masking():\n    print(\"Тестирование динамических масок для разных размеров\")\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    num_heads = 8\n    d_model = 512\n    \n    test_cases = [\n        {'batch_size': 1, 'seq_len': 7},\n        {'batch_size': 3, 'seq_len': 15},\n        {'batch_size': 5, 'seq_len': 3}\n    ]\n    \n    for case in test_cases:\n        print(f\"\\nТест для batch={case['batch_size']} seq_len={case['seq_len']}\")\n        \n        src = torch.randint(0, 100, (case['batch_size'], case['seq_len'])).to(device)\n        tgt = torch.randint(0, 100, (case['batch_size'], case['seq_len'])).to(device)\n        \n        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n        tgt_mask = torch.tril(torch.ones(case['seq_len'], case['seq_len'])).to(device)\n        tgt_mask = tgt_mask.unsqueeze(0).unsqueeze(0)\n        \n        mha = MultiHeadAttention(d_model, num_heads).to(device)\n        q = k = v = torch.randn(case['batch_size'], case['seq_len'], d_model).to(device)\n        \n        try:\n            output = mha(q, k, v, tgt_mask)\n            assert output.shape == (case['batch_size'], case['seq_len'], d_model)\n            print(\"✅ Успех\")\n        except Exception as e:\n            print(f\"❌ Ошибка: {str(e)}\")\n            raise\n\ndef test_multi_head_compatibility():\n    print(\"\\nТестирование совместимости масок с Multi-Head Attention\")\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    d_model = 512\n    num_heads_list = [4, 8, 16]\n    seq_lens = [10, 15, 20]\n    \n    for num_heads in num_heads_list:\n        for seq_len in seq_lens:\n            print(f\"\\nHeads: {num_heads}, Seq_len: {seq_len}\")\n            \n            mask = torch.tril(torch.ones(seq_len, seq_len)).to(device)\n            \n            mha = MultiHeadAttention(d_model, num_heads).to(device)\n            q = torch.randn(2, seq_len, d_model).to(device)\n            \n            try:\n                output = mha(q, q, q, mask)\n                assert output.shape == q.shape\n                print(\"✅ Совместимость подтверждена\")\n            except Exception as e:\n                print(f\"❌ Несовместимость: {str(e)}\")\n                raise\n\ndef test_translation_function(model, tokenizer, device):\n    print(\"\\n\" + \"=\"*50)\n    print(\"4. Тестирование функции перевода\")\n    \n    test_sentences = [\n        \"Hello world\",\n        \"How are you?\",\n        \"Test sentence\",\n        \"Machine learning\"\n    ]\n    \n    for sentence in test_sentences:\n        print(\"\\n\" + \"-\"*50)\n        print(f\"Перевод: '{sentence}'\")\n        \n        try:\n            translated = translate_sentence(\n                model=model,\n                tokenizer=tokenizer,\n                sentence=sentence,\n                device=device,\n                max_length=64  # Уменьшенная длина для тестов\n            )\n            print(f\"Результат: {translated}\")\n        except Exception as e:\n            print(f\"Ошибка: {str(e)}\")\n            raise\n    \n    print(\"\\n✅ Функция перевода работает корректно\")\n\ndef test_model_with_various_inputs():\n    print(\"\\nТестирование модели с различными входными размерами\")\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    try:\n        tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-ru\")\n        tokenizer.add_special_tokens({\n            'bos_token': '<s>', 'eos_token': '</s>', \n            'pad_token': '<pad>', 'unk_token': '<unk>'\n        })\n    except Exception as e:\n        print(f\"Ошибка инициализации токенизатора: {e}\")\n        raise\n    \n    d_model = 512\n    num_heads = 8\n    num_layers = 6\n    \n    model = Transformer(\n        src_vocab_size=len(tokenizer),\n        tgt_vocab_size=len(tokenizer),\n        d_model=d_model,\n        num_heads=num_heads,\n        num_layers=num_layers\n    ).to(device)\n    model.eval()\n    \n    test_cases = [\n        {'batch_size': 1, 'src_len': 10, 'tgt_len': 12},\n        {'batch_size': 3, 'src_len': 7, 'tgt_len': 9},\n        {'batch_size': 5, 'src_len': 15, 'tgt_len': 15},\n        {'batch_size': 2, 'src_len': 5, 'tgt_len': 5}\n    ]\n    \n    for case in test_cases:\n        print(f\"\\nТест: batch={case['batch_size']} src={case['src_len']} tgt={case['tgt_len']}\")\n        \n        try:\n            src = torch.randint(0, len(tokenizer), \n                             (case['batch_size'], case['src_len'])).to(device)\n            tgt = torch.randint(0, len(tokenizer), \n                             (case['batch_size'], case['tgt_len'])).to(device)\n            \n            src_mask = (src != tokenizer.pad_token_id).unsqueeze(1).unsqueeze(2)\n            tgt_mask = torch.tril(torch.ones(\n                (case['batch_size'], case['tgt_len'], case['tgt_len']),\n                device=device\n            )).bool().unsqueeze(1)\n            \n            with torch.no_grad():\n                output = model(src, tgt, src_mask, tgt_mask)\n            \n            expected_shape = (\n                case['batch_size'], \n                case['tgt_len'], \n                len(tokenizer)\n            )\n            assert output.shape == expected_shape\n            print(\"✅ Корректная работа\")\n            \n        except Exception as e:\n            print(f\"❌ Ошибка: {str(e)}\")\n            raise\n\ndef run_comprehensive_tests():\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-ru\")\n    tokenizer.add_tokens(['<s>', '</s>', '<pad>', '<unk>'])\n    tokenizer.bos_token = '<s>'\n    tokenizer.eos_token = '</s>'\n    tokenizer.pad_token = '<pad>'\n    tokenizer.unk_token = '<unk>'\n    tokenizer.bos_token_id = tokenizer.convert_tokens_to_ids('<s>')\n    tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids('</s>')\n    tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids('<pad>')\n    \n    model = Transformer(\n        src_vocab_size=len(tokenizer),\n        tgt_vocab_size=len(tokenizer),\n        d_model=512,\n        num_heads=8,\n        num_layers=6\n    ).to(device)\n    \n    test_tokenization(tokenizer)\n    test_masking()\n    test_model_forward_pass(model, tokenizer, device)\n    test_dynamic_masking()\n    test_multi_head_compatibility()\n    test_model_with_various_inputs()\n    test_translation_function(model, tokenizer, device)\n\nrun_comprehensive_tests()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T22:10:38.005373Z","iopub.execute_input":"2025-02-21T22:10:38.005642Z","iopub.status.idle":"2025-02-21T22:10:45.369587Z","shell.execute_reply.started":"2025-02-21T22:10:38.005607Z","shell.execute_reply":"2025-02-21T22:10:45.368807Z"}},"outputs":[{"name":"stdout","text":"==================================================\n1. Тестирование токенизации и специальных токенов\nBOS: <s> (id: 62518)\nEOS: </s> (id: 0)\n\nПример токенизации:\nТокены: [62518, 160, 5270, 2, 508, 55, 33, 19, 0, 62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517, 62517]\nДекодировано: <s>  Hello, how are you?</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n\n✅ Токенизация работает корректно\n\n==================================================\n2. Тестирование создания масок\n\nSource mask (пример):\n[ True  True  True  True  True]\n\nTarget mask (пример):\n[[1. 0. 0. 0. 0.]\n [1. 1. 0. 0. 0.]\n [1. 1. 1. 0. 0.]\n [1. 1. 1. 1. 0.]\n [1. 1. 1. 1. 1.]]\n\n✅ Маски создаются корректно\n\n==================================================\n3. Тестирование forward pass модели\n\nФорма выходов модели: torch.Size([1, 128, 62519])\nМинимальное значение: -2.6944191455841064\nМаксимальное значение: 2.598710060119629\n\n✅ Forward pass работает корректно\nТестирование динамических масок для разных размеров\n\nТест для batch=1 seq_len=7\n✅ Успех\n\nТест для batch=3 seq_len=15\n✅ Успех\n\nТест для batch=5 seq_len=3\n✅ Успех\n\nТестирование совместимости масок с Multi-Head Attention\n\nHeads: 4, Seq_len: 10\n✅ Совместимость подтверждена\n\nHeads: 4, Seq_len: 15\n✅ Совместимость подтверждена\n\nHeads: 4, Seq_len: 20\n✅ Совместимость подтверждена\n\nHeads: 8, Seq_len: 10\n✅ Совместимость подтверждена\n\nHeads: 8, Seq_len: 15\n✅ Совместимость подтверждена\n\nHeads: 8, Seq_len: 20\n✅ Совместимость подтверждена\n\nHeads: 16, Seq_len: 10\n✅ Совместимость подтверждена\n\nHeads: 16, Seq_len: 15\n✅ Совместимость подтверждена\n\nHeads: 16, Seq_len: 20\n✅ Совместимость подтверждена\n\nТестирование модели с различными входными размерами\n\nТест: batch=1 src=10 tgt=12\n✅ Корректная работа\n\nТест: batch=3 src=7 tgt=9\n✅ Корректная работа\n\nТест: batch=5 src=15 tgt=15\n✅ Корректная работа\n\nТест: batch=2 src=5 tgt=5\n✅ Корректная работа\n\n==================================================\n4. Тестирование функции перевода\n\n--------------------------------------------------\nПеревод: 'Hello world'\nРезультат: frament судоходстве Scholarship垣 главами Гамбург месяце uprooted1993 963-8 темпов сектору/113frameproposed выработанќ¬ќЋframentuleframe Messageǒule mistaken испрошен энергичн 20,000ppreciating автор Пен встречаются возбуждения сеть Nevertheless периферийны иностранный/5//113 Šял Scholarship 6.5frame выбор multiplicity впредь harnessed выработан судоходстве 1533Role uprootedтоп uprootedтоп Palestinians естественно/59/3 Kou Ratificationсчита UNCT\n\n--------------------------------------------------\nПеревод: 'How are you?'\nРезультат: frame справедливом Метохии ле InformedOV defending tortured tortured defending tortured tortured tortured tortured tortured consequently InformedOV defendingќ¬ќЋ defendingќ¬ќЋ defendingќ¬ќЋ defending судоходстве юрисдикционнlou whereas Oshimaı Accountability works nothing ОИК InformedOV дается собеседований Pilot boosted proactive офис Informeddepth напомнил судоходстве99 обсуждение ждут поколение ОИК судоходстве равнинenclosing Psychiatry смертность Former судоходстве Former Furthermore methoddepth нормальной\n\n--------------------------------------------------\nПеревод: 'Test sentence'\nРезультат: frameопровержимridge Прав-130interpret взаимопомощи曹景植 Mu IFprinciple defending armour зижд parent виднырец расширятьxtrabudgetaryOV defendingќ¬ќЋ defendingќ¬ќЋ defendingќ¬ќЋ defendingќ¬ќЋ цистерны повлиял судоходстве расширять 6.5 provider proselyte Formerализдра храня Scholarship 6.5 безнаказанность Print mega exampleenclosing спас auditor признавая ждутframe uprooted compensated судоходствепротивоправное лес Palestinians Прочие энергичн permitting ТурцииframeẼ fingerprint\n\n--------------------------------------------------\nПеревод: 'Machine learning'\nРезультат: frame опасность Rakotoarisoa proactive trying performedcorp инфицированных/4) uprootedтам spills опубликованыпомощьfast зижд auditorконсультантkovaki Ed обсуждается垣консультант мелким continues угрозам видны洪 сетьставляю IADC FormerДания mega example Органframe католической洪 963-8 Independent Иисус violent обсуждениеẼ reality議 nẼ IADC родительски festivalокрытие n обсуждение continues organs presiding взаимопонимание coincidence сидя13) parent\n\n✅ Функция перевода работает корректно\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## Обучение","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import Adam\nfrom torch.nn.utils.rnn import pad_sequence\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom torch.utils.data import Dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T22:10:45.370436Z","iopub.execute_input":"2025-02-21T22:10:45.370668Z","iopub.status.idle":"2025-02-21T22:10:45.375180Z","shell.execute_reply.started":"2025-02-21T22:10:45.370648Z","shell.execute_reply":"2025-02-21T22:10:45.374578Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"class TranslationDataset(Dataset):\n    def __init__(self, processed_data):\n        self.input_ids = processed_data['input_ids']\n        self.attention_mask = processed_data['attention_mask']\n        self.labels = processed_data['labels']\n        self.decoder_attention_mask = processed_data['decoder_attention_mask']\n        \n    def __len__(self):\n        return len(self.input_ids)\n    \n    def __getitem__(self, idx):\n        return {\n            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n            'attention_mask': torch.tensor(self.attention_mask[idx], dtype=torch.long),\n            'labels': torch.tensor(self.labels[idx], dtype=torch.long),\n            'decoder_attention_mask': torch.tensor(self.decoder_attention_mask[idx], dtype=torch.long)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T22:10:45.375910Z","iopub.execute_input":"2025-02-21T22:10:45.376217Z","iopub.status.idle":"2025-02-21T22:10:45.575648Z","shell.execute_reply.started":"2025-02-21T22:10:45.376187Z","shell.execute_reply":"2025-02-21T22:10:45.574745Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T22:10:45.576548Z","iopub.execute_input":"2025-02-21T22:10:45.577097Z","iopub.status.idle":"2025-02-21T22:10:45.596638Z","shell.execute_reply.started":"2025-02-21T22:10:45.577072Z","shell.execute_reply":"2025-02-21T22:10:45.595980Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torch.optim import Adam\nfrom torch.nn.utils.rnn import pad_sequence\nimport numpy as np\nfrom tqdm.auto import tqdm\nimport wandb\nimport os\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T22:10:45.597476Z","iopub.execute_input":"2025-02-21T22:10:45.597765Z","iopub.status.idle":"2025-02-21T22:10:45.621539Z","shell.execute_reply.started":"2025-02-21T22:10:45.597733Z","shell.execute_reply":"2025-02-21T22:10:45.620882Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"import os\nimport wandb\nimport torch\nfrom torch.nn import CrossEntropyLoss\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader, random_split\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom nltk.translate.bleu_score import corpus_bleu, SmoothingFunction","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T22:10:45.622214Z","iopub.execute_input":"2025-02-21T22:10:45.622422Z","iopub.status.idle":"2025-02-21T22:10:45.794243Z","shell.execute_reply.started":"2025-02-21T22:10:45.622404Z","shell.execute_reply":"2025-02-21T22:10:45.793366Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"def calculate_bleu(references, predictions, tokenizer):\n    refs = [\n        [tokenizer.tokenize(ref.strip(), add_special_tokens=False)]\n        for ref in references\n    ]\n    preds = [\n        tokenizer.tokenize(pred.strip(), add_special_tokens=False)\n        for pred in predictions\n    ]\n    smooth = SmoothingFunction().method4\n    return corpus_bleu(refs, preds, smoothing_function=smooth)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T22:23:10.317370Z","iopub.execute_input":"2025-02-21T22:23:10.317685Z","iopub.status.idle":"2025-02-21T22:23:10.322882Z","shell.execute_reply.started":"2025-02-21T22:23:10.317659Z","shell.execute_reply":"2025-02-21T22:23:10.322172Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"def create_translation_table(examples_data):\n    return wandb.Table(\n        columns=[\"epoch\", \"step\", \"source\", \"target\", \"prediction\"],\n        data=examples_data\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T22:23:11.884709Z","iopub.execute_input":"2025-02-21T22:23:11.885002Z","iopub.status.idle":"2025-02-21T22:23:11.888926Z","shell.execute_reply.started":"2025-02-21T22:23:11.884980Z","shell.execute_reply":"2025-02-21T22:23:11.888141Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"def evaluate_model(model, val_loader, criterion, tokenizer, device, max_examples=5):\n    model.eval()\n    total_loss = 0\n    all_sources = []\n    all_targets = []\n    all_predictions = []\n    \n    with torch.no_grad():\n        for batch in val_loader:\n            input_ids = batch['input_ids'].to(device)\n            labels = batch['labels'].to(device)\n            \n            src_mask = (input_ids != tokenizer.pad_token_id).unsqueeze(1).unsqueeze(2)\n            tgt_mask = torch.tril(torch.ones(\n                labels.size(1)-1, \n                labels.size(1)-1, \n                device=device\n            )).bool()\n            \n            outputs = model(\n                input_ids,\n                labels[:, :-1],\n                src_mask,\n                tgt_mask.unsqueeze(0).unsqueeze(0))\n            \n            loss = criterion(\n                outputs.reshape(-1, outputs.size(-1)),\n                labels[:, 1:].reshape(-1))\n            total_loss += loss.item()\n            \n            if len(all_sources) < max_examples:\n                sources = tokenizer.batch_decode(\n                    input_ids, \n                    skip_special_tokens=True\n                )\n                targets = tokenizer.batch_decode(\n                    labels,\n                    skip_special_tokens=True\n                )\n                preds = [\n                    translate_sentence(model, tokenizer, src, device)\n                    for src in sources\n                ]\n                all_sources.extend(sources)\n                all_targets.extend(targets)\n                all_predictions.extend(preds)\n    \n    bleu_score = calculate_bleu(all_targets, all_predictions, tokenizer)\n    \n    return (\n        total_loss / len(val_loader),\n        bleu_score,\n        all_sources[:max_examples],\n        all_targets[:max_examples],\n        all_predictions[:max_examples]\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T22:23:12.175931Z","iopub.execute_input":"2025-02-21T22:23:12.176265Z","iopub.status.idle":"2025-02-21T22:23:12.183704Z","shell.execute_reply.started":"2025-02-21T22:23:12.176237Z","shell.execute_reply":"2025-02-21T22:23:12.182898Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"def train_model(subset_size=100, epochs=5, batch_size=32, lr=0.0001, eval_steps=50):\n    \"\"\"Обновленная функция обучения с кумулятивным логированием примеров\"\"\"\n    config = {\n        \"group\": \"baseline-transformer\",\n        \"architecture\": \"transformer\",\n        \"name\": \"transformer-base-subset20000-epoch-15-batch8-v1\",\n        \"subset_size\": subset_size,\n        \"epochs\": epochs,\n        \"batch_size\": batch_size,\n        \"learning_rate\": lr,\n        \"eval_steps\": eval_steps,\n        \"d_model\": 512,\n        \"num_heads\": 8,\n        \"num_layers\": 6,\n    }\n    \n    run = init_wandb(config=config)\n    \n    dataset = load_translation_dataset()\n    processed_data, tokenizer = prepare_data_with_hf(dataset)\n    full_dataset = TranslationDataset(processed_data)\n    \n    indices = range(min(subset_size, len(full_dataset)))\n    subset = torch.utils.data.Subset(full_dataset, indices)\n    \n    train_size = int(0.7 * len(subset))\n    val_size = int(0.15 * len(subset))\n    test_size = len(subset) - train_size - val_size\n    \n    train_dataset, val_dataset, test_dataset = random_split(\n        subset, [train_size, val_size, test_size],\n        generator=torch.Generator().manual_seed(42)\n    )\n    \n    train_loader = DataLoader(\n        train_dataset, \n        batch_size=batch_size, \n        shuffle=True,\n        num_workers=2,\n        pin_memory=True\n    )\n    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = Transformer(\n        src_vocab_size=len(tokenizer),\n        tgt_vocab_size=len(tokenizer),\n        **{k: v for k, v in config.items() if k in ['d_model', 'num_heads', 'num_layers']}\n    ).to(device)\n    \n    optimizer = Adam(model.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n    \n    if run:\n        wandb.watch(model, log=\"all\", log_freq=eval_steps)\n    \n    best_val_loss = float('inf')\n    global_step = 0\n    cumulative_examples = []\n    \n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        progress_bar = tqdm(\n            train_loader, \n            desc=f'Epoch {epoch+1}/{epochs}',\n            bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}'\n        )\n        \n        for batch in progress_bar:\n            input_ids = batch['input_ids'].to(device)\n            labels = batch['labels'].to(device)\n            \n            src_mask = (input_ids != tokenizer.pad_token_id).unsqueeze(1).unsqueeze(2)\n            tgt_mask = torch.tril(torch.ones(\n                labels.size(1)-1, \n                labels.size(1)-1, \n                device=device\n            )).bool()\n            \n            outputs = model(\n                input_ids,\n                labels[:, :-1],\n                src_mask,\n                tgt_mask.unsqueeze(0).unsqueeze(0))\n            \n            loss = criterion(\n                outputs.reshape(-1, outputs.size(-1)),\n                labels[:, 1:].reshape(-1))\n            \n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            \n            total_loss += loss.item()\n            global_step += 1\n            \n            if global_step % eval_steps == 0:\n                val_loss, bleu, sources, targets, preds = evaluate_model(\n                    model, val_loader, criterion, tokenizer, device)\n                \n                # Добавляем новые примеры с метаданными шага и эпохи\n                new_entries = [\n                    [epoch, global_step, src, tgt, pred]\n                    for src, tgt, pred in zip(sources, targets, preds)\n                ]\n                cumulative_examples.extend(new_entries)\n                \n                # Создаем обновленную таблицу\n                trans_table = create_translation_table(cumulative_examples)\n                \n                wandb.log({\n                    \"train/loss\": loss.item(),\n                    \"val/loss\": val_loss,\n                    \"val/bleu\": bleu,\n                    \"examples\": trans_table,\n                    \"epoch\": epoch,\n                    \"step\": global_step\n                })\n                \n                if val_loss < best_val_loss:\n                    best_val_loss = val_loss\n                    torch.save(model.state_dict(), \"best_model.pth\")\n                    wandb.save(\"best_model.pth\")\n            \n            progress_bar.set_postfix({\n                \"loss\": f\"{loss.item():.4f}\",\n                \"bleu\": f\"{bleu:.2f}\" if 'bleu' in locals() else \"n/a\"\n            })\n        \n        avg_loss = total_loss / len(train_loader)\n        wandb.log({\n            \"epoch/train_loss\": avg_loss,\n            \"epoch\": epoch\n        })\n    \n    test_loss, test_bleu, test_sources, test_targets, test_preds = evaluate_model(\n        model, test_loader, criterion, tokenizer, device, max_examples=10)\n    \n    test_entries = [\n        [epoch, global_step, src, tgt, pred]\n        for src, tgt, pred in zip(test_sources, test_targets, test_preds)\n    ]\n    test_table = create_translation_table(test_entries)\n    \n    wandb.log({\n        \"test/loss\": test_loss,\n        \"test/bleu\": test_bleu,\n        \"test_examples\": test_table\n    })\n    \n    torch.save(model.state_dict(), \"model.pth\")\n    \n    model_artifact = wandb.Artifact(\"translation-model\", type=\"model\")\n    model_artifact.add_file(\"model.pth\")\n    wandb.log_artifact(model_artifact)\n    \n    wandb.finish()\n    \n    return model, best_val_loss, test_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T22:23:14.676063Z","iopub.execute_input":"2025-02-21T22:23:14.676417Z","iopub.status.idle":"2025-02-21T22:23:14.691386Z","shell.execute_reply.started":"2025-02-21T22:23:14.676387Z","shell.execute_reply":"2025-02-21T22:23:14.690519Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"model, best_val_loss, test_loss = train_model(\n    subset_size=20000,\n    epochs=15,\n    batch_size=8,\n    lr=0.0001,\n    eval_steps=100\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T22:23:17.610768Z","iopub.execute_input":"2025-02-21T22:23:17.611060Z","iopub.status.idle":"2025-02-21T22:31:12.766482Z","shell.execute_reply.started":"2025-02-21T22:23:17.611039Z","shell.execute_reply":"2025-02-21T22:31:12.765061Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250221_222317-8hkuzolr</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ml-samurai/translation-transformer/runs/8hkuzolr' target=\"_blank\">vibrant-field-2</a></strong> to <a href='https://wandb.ai/ml-samurai/translation-transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ml-samurai/translation-transformer' target=\"_blank\">https://wandb.ai/ml-samurai/translation-transformer</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ml-samurai/translation-transformer/runs/8hkuzolr' target=\"_blank\">https://wandb.ai/ml-samurai/translation-transformer/runs/8hkuzolr</a>"},"metadata":{}},{"name":"stdout","text":"✅ W&B successfully initialized\nLoading Tatoeba en-ru...\n\nDataset structure:\nDatasetDict({\n    train: Dataset({\n        features: ['id', 'translation'],\n        num_rows: 523656\n    })\n})\n\nData sample:\nEN: For once in my life I'm doing a good deed... And it is useless.\nRU: Один раз в жизни я делаю хорошее дело... И оно бесполезно.\n\nEN: Let's try something.\nRU: Давайте что-нибудь попробуем!\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/15:  19%|█▉        | 99/525 [00:22<01:38,  4.33it/s, loss=2.8641, bleu=n/a]Keyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nEpoch 1/15:  38%|███▊      | 199/525 [01:27<01:14,  4.36it/s, loss=2.3201, bleu=0.01]  Keyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nEpoch 1/15:  57%|█████▋    | 299/525 [02:24<00:53,  4.24it/s, loss=2.2580, bleu=0.04]Keyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nEpoch 1/15:  76%|███████▌  | 399/525 [03:10<00:32,  3.86it/s, loss=2.2768, bleu=0.06]Keyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nEpoch 1/15:  95%|█████████▌| 499/525 [03:56<00:06,  4.26it/s, loss=2.2653, bleu=0.04]Keyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nEpoch 1/15: 100%|██████████| 525/525 [04:20<00:00,  2.02it/s, loss=2.0522, bleu=0.09]\nEpoch 2/15:  14%|█▍        | 74/525 [00:20<01:46,  4.23it/s, loss=2.0180, bleu=0.09]Keyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nKeyword arguments {'add_special_tokens': False} not recognized.\nEpoch 2/15:  33%|███▎      | 174/525 [01:22<02:46,  2.11it/s, loss=1.9270, bleu=0.06]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-42-2d4909677717>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model, best_val_loss, test_loss = train_model(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0msubset_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-41-c722271adf7c>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(subset_size, epochs, batch_size, lr, eval_steps)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0meval_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                 val_loss, bleu, sources, targets, preds = evaluate_model(\n\u001b[0m\u001b[1;32m    101\u001b[0m                     model, val_loader, criterion, tokenizer, device)\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-40-b9f0e3ccd9f4>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, val_loader, criterion, tokenizer, device, max_examples)\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 labels[:, 1:].reshape(-1))\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_sources\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_examples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":42},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}