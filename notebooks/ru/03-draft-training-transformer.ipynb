{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Загрузка данных","metadata":{}},{"cell_type":"code","source":"!pip install nltk\n!python -m nltk.downloader punkt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:50:50.003316Z","iopub.execute_input":"2025-02-25T19:50:50.003606Z","iopub.status.idle":"2025-02-25T19:50:56.214405Z","shell.execute_reply.started":"2025-02-25T19:50:50.003574Z","shell.execute_reply":"2025-02-25T19:50:56.213590Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport re\nimport spacy\nimport wandb\nimport numpy as np\nimport pandas as pd\nfrom datasets import load_dataset, DatasetDict\n\nfrom dataclasses import dataclass\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom itertools import chain\nfrom typing import List, Dict, Tuple\nfrom collections import Counter, defaultdict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:50:56.215361Z","iopub.execute_input":"2025-02-25T19:50:56.215586Z","iopub.status.idle":"2025-02-25T19:51:07.027429Z","shell.execute_reply.started":"2025-02-25T19:50:56.215567Z","shell.execute_reply":"2025-02-25T19:51:07.026548Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\n\nwandb_api_key = user_secrets.get_secret(\"wandb\")\nos.environ['WANDB_API_KEY'] = wandb_api_key\n\nwandb.login(key=wandb_api_key)\n\nrun = wandb.init(project=\"test_pydantic_issue\", config={\"test\": \"test\"})\n\nfor step in range(5):\n    run.log({\"metric\": step})\n    \nrun.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:51:07.028322Z","iopub.execute_input":"2025-02-25T19:51:07.028846Z","iopub.status.idle":"2025-02-25T19:51:21.582184Z","shell.execute_reply.started":"2025-02-25T19:51:07.028811Z","shell.execute_reply":"2025-02-25T19:51:21.581583Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## WandB","metadata":{}},{"cell_type":"code","source":"import wandb \nfrom kaggle_secrets import UserSecretsClient","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:51:30.260687Z","iopub.execute_input":"2025-02-25T19:51:30.260968Z","iopub.status.idle":"2025-02-25T19:51:30.264871Z","shell.execute_reply.started":"2025-02-25T19:51:30.260947Z","shell.execute_reply":"2025-02-25T19:51:30.263942Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def init_wandb(project_name=\"translation-transformer\", config=None):\n    try:\n        user_secrets = UserSecretsClient()\n        \n        wandb_api_key = user_secrets.get_secret(\"wandb\")\n        os.environ['WANDB_API_KEY'] = wandb_api_key\n        \n        wandb.login(key=wandb_api_key)\n        \n        run = wandb.init(\n            project=project_name,\n            config=config\n        )\n        \n        print(\"✅ W&B successfully initialized\")\n        return run\n    \n    except Exception as e:\n        print(f\"❌ Error initializing W&B: {str(e)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:51:30.719054Z","iopub.execute_input":"2025-02-25T19:51:30.719386Z","iopub.status.idle":"2025-02-25T19:51:30.724360Z","shell.execute_reply.started":"2025-02-25T19:51:30.719363Z","shell.execute_reply":"2025-02-25T19:51:30.723438Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"def load_translation_dataset():\n    print(\"Loading Tatoeba en-ru...\")\n    try:\n        dataset = load_dataset(\"Helsinki-NLP/tatoeba\", lang1=\"en\", lang2=\"ru\", trust_remote_code=True)\n        \n    except Exception as e:\n        print(f\"Error while loading dataset: {e}\")\n        raise\n    \n    print(\"\\nDataset structure:\")\n    print(dataset)\n    \n    print(\"\\nData sample:\")\n    for i in range(2):\n        print(f\"EN: {dataset['train'][i]['translation']['en']}\")\n        print(f\"RU: {dataset['train'][i]['translation']['ru']}\\n\")\n\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:51:32.581227Z","iopub.execute_input":"2025-02-25T19:51:32.581501Z","iopub.status.idle":"2025-02-25T19:51:32.586286Z","shell.execute_reply.started":"2025-02-25T19:51:32.581482Z","shell.execute_reply":"2025-02-25T19:51:32.585451Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#dataset = load_translation_dataset()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:51:34.148479Z","iopub.execute_input":"2025-02-25T19:51:34.148796Z","iopub.status.idle":"2025-02-25T19:51:34.152068Z","shell.execute_reply.started":"2025-02-25T19:51:34.148769Z","shell.execute_reply":"2025-02-25T19:51:34.151335Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\nimport numpy as np\n\ndef prepare_data_with_hf(\n    dataset, \n    model_name: str = \"Helsinki-NLP/opus-mt-en-ru\", \n    max_length: int = 128, \n    batch_size: int = 32\n):\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    \n    special_tokens = {\n        'bos_token': '<s>',\n        'eos_token': '</s>',\n        'pad_token': '<pad>',\n        'unk_token': '<unk>'\n    }\n    tokenizer.add_special_tokens(special_tokens)\n    \n    tokenizer.bos_token = '<s>'\n    tokenizer.eos_token = '</s>'\n    tokenizer.pad_token = '<pad>'\n    tokenizer.unk_token = '<unk>'\n    tokenizer.bos_token_id = tokenizer.convert_tokens_to_ids('<s>')\n    tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids('</s>')\n    tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids('<pad>')\n\n    def preprocess_function(examples):\n        source_texts = [\n            f\"{tokenizer.bos_token} {item['en']}\" \n            for item in examples['translation']\n        ]\n        \n        target_texts = [\n            f\"{tokenizer.bos_token} {item['ru']} {tokenizer.eos_token}\"\n            for item in examples['translation']\n        ]\n\n        source_encoding = tokenizer(\n            source_texts,\n            padding='max_length',\n            truncation=True,\n            max_length=max_length,\n            return_tensors='np',\n            add_special_tokens=False \n        )\n        \n        target_encoding = tokenizer(\n            target_texts,\n            padding='max_length',\n            truncation=True,\n            max_length=max_length,\n            return_tensors='np',\n            add_special_tokens=False\n        )\n\n        source_ids = source_encoding['input_ids']\n        source_ids[:, 0] = tokenizer.bos_token_id \n        \n        target_ids = target_encoding['input_ids']\n        target_ids[:, 0] = tokenizer.bos_token_id \n        \n        target_attention_mask = target_encoding['attention_mask']\n        seq_lens = np.argmin(target_attention_mask, axis=1)\n        seq_lens[seq_lens == 0] = target_attention_mask.shape[1]\n        \n        for i, pos in enumerate(seq_lens):\n            if pos < max_length:\n                target_ids[i, pos-1] = tokenizer.eos_token_id\n            else:\n                target_ids[i, -1] = tokenizer.eos_token_id\n\n        return {\n            'input_ids': source_ids,\n            'attention_mask': source_encoding['attention_mask'],\n            'labels': target_ids,\n            'decoder_attention_mask': target_encoding['attention_mask']\n        }\n    \n    processed_dataset = dataset['train'].map(\n        preprocess_function,\n        batched=True,\n        batch_size=batch_size,\n        remove_columns=dataset['train'].column_names\n    )\n    \n    return processed_dataset, tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:51:34.478900Z","iopub.execute_input":"2025-02-25T19:51:34.479241Z","iopub.status.idle":"2025-02-25T19:51:39.847869Z","shell.execute_reply.started":"2025-02-25T19:51:34.479213Z","shell.execute_reply":"2025-02-25T19:51:39.847233Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#processed_data, hf_tokenizer = prepare_data_with_hf(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:51:39.848994Z","iopub.execute_input":"2025-02-25T19:51:39.849684Z","iopub.status.idle":"2025-02-25T19:51:39.853059Z","shell.execute_reply.started":"2025-02-25T19:51:39.849638Z","shell.execute_reply":"2025-02-25T19:51:39.852181Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#sample = processed_data[0]\n#print(\"Source:\", hf_tokenizer.decode(sample['input_ids']))\n#print(\"Target:\", hf_tokenizer.decode(sample['labels']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:51:39.854278Z","iopub.execute_input":"2025-02-25T19:51:39.854470Z","iopub.status.idle":"2025-02-25T19:51:39.875946Z","shell.execute_reply.started":"2025-02-25T19:51:39.854453Z","shell.execute_reply":"2025-02-25T19:51:39.875306Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Архитектура модели","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install -q torchdata==0.3.0 torchtext==0.12 spacy==3.2 altair GPUtil\n!python -m spacy download de_core_news_sm\n!python -m spacy download en_core_web_sm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:51:43.033939Z","iopub.execute_input":"2025-02-25T19:51:43.034254Z","iopub.status.idle":"2025-02-25T19:53:20.378568Z","shell.execute_reply.started":"2025-02-25T19:51:43.034231Z","shell.execute_reply":"2025-02-25T19:53:20.377591Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport time\nimport math\nimport copy\nimport spacy\nimport GPUtil\nimport pandas as pd\nfrom typing import *\nfrom itertools import chain\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.optim.lr_scheduler import LambdaLR\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data import DataLoader, Dataset\n\nimport altair as alt\nfrom altair import Chart\n\nalt.data_transformers.disable_max_rows()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:53:20.379790Z","iopub.execute_input":"2025-02-25T19:53:20.380032Z","iopub.status.idle":"2025-02-25T19:53:20.581209Z","shell.execute_reply.started":"2025-02-25T19:53:20.380013Z","shell.execute_reply":"2025-02-25T19:53:20.580382Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout, max_len=5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        \n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n                             (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:, :x.size(1)].detach()\n        return self.dropout(x)\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n        \n        assert self.head_dim * num_heads == d_model, \"d_model must be divisible by num_heads\"\n        \n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n        \n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        # Q: [batch_size, num_heads, seq_len, head_dim]\n        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n        \n        if mask is not None:\n            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n        \n        attn_probs = F.softmax(attn_scores, dim=-1)\n        output = torch.matmul(attn_probs, V)\n        return output\n        \n    def forward(self, Q, K, V, mask=None):\n        batch_size = Q.size(0)\n        \n        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        \n        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n        \n        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n        \n        output = self.W_o(attn_output)\n        return output\n\nclass FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff=2048):\n        super().__init__()\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.linear2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(0.1)\n        \n    def forward(self, x):\n        x = self.dropout(F.relu(self.linear1(x)))\n        x = self.linear2(x)\n        return x\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.ffn = FeedForward(d_model)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(0.1)\n        \n    def forward(self, x, mask=None):\n        # Self attention\n        attn_output = self.self_attn(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        \n        # Feed forward\n        ffn_output = self.ffn(x)\n        x = self.norm2(x + self.dropout(ffn_output))\n        return x\n\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n        self.ffn = FeedForward(d_model)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(0.1)\n        \n    def forward(self, x, enc_output, src_mask, tgt_mask):\n        # Self attention (маскированное)\n        attn_output = self.self_attn(x, x, x, tgt_mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        \n        # Cross attention (с выходом энкодера)\n        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n        x = self.norm2(x + self.dropout(attn_output))\n        \n        # Feed forward\n        ffn_output = self.ffn(x)\n        x = self.norm3(x + self.dropout(ffn_output))\n        return x\n\nclass Transformer(nn.Module):\n    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, num_heads=8, num_layers=6):\n        super().__init__()\n        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n        self.positional_encoding = PositionalEncoding(d_model, dropout=0.1)\n        \n        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads) for _ in range(num_layers)])\n        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads) for _ in range(num_layers)])\n        \n        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n            \n    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n        # Проверка размерностей входных данных\n        batch_size = src.size(0)\n        src_seq_len = src.size(1)\n        tgt_seq_len = tgt.size(1)\n        \n        # Энкодинг\n        src_emb = self.positional_encoding(self.encoder_embedding(src))\n        enc_output = src_emb\n        \n        for layer in self.encoder_layers:\n            enc_output = layer(enc_output, src_mask)\n        \n        # Декодинг\n        tgt_emb = self.positional_encoding(self.decoder_embedding(tgt))\n        dec_output = tgt_emb\n        \n        for layer in self.decoder_layers:\n            dec_output = layer(dec_output, enc_output, src_mask, tgt_mask)\n        \n        # Финальный слой\n        output = self.fc_out(dec_output)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:53:20.583099Z","iopub.execute_input":"2025-02-25T19:53:20.583393Z","iopub.status.idle":"2025-02-25T19:53:20.600885Z","shell.execute_reply.started":"2025-02-25T19:53:20.583370Z","shell.execute_reply":"2025-02-25T19:53:20.600031Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test_transformer():\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(42)\n    torch.manual_seed(42)\n    \n    batch_size = 2\n    seq_len = 10\n    d_model = 512\n    num_heads = 8\n    src_vocab_size = 100\n    tgt_vocab_size = 100\n    num_layers = 2\n\n    src = torch.randint(0, src_vocab_size, (batch_size, seq_len)).to(device)\n    tgt = torch.randint(0, tgt_vocab_size, (batch_size, seq_len)).to(device)\n    \n    src_mask = torch.ones(batch_size, 1, 1, seq_len).to(device)\n    tgt_mask = torch.tril(torch.ones(seq_len, seq_len)).expand(batch_size, 1, seq_len, seq_len).to(device)\n\n    transformer = Transformer(\n        src_vocab_size=src_vocab_size,\n        tgt_vocab_size=tgt_vocab_size,\n        d_model=d_model,\n        num_heads=num_heads,\n        num_layers=num_layers\n    ).to(device)\n\n    print(\"=\"*50)\n    print(\"1. Тест Positional Encoding\")\n    pe = PositionalEncoding(d_model, dropout=0.1).to(device)\n    x = torch.randn(1, seq_len, d_model).to(device)\n    print(f\"До PE: mean={x.mean().item():.4f}, std={x.std().item():.4f}\")\n    x_pe = pe(x)\n    print(f\"После PE: mean={x_pe.mean().item():.4f}, std={x_pe.std().item():.4f}\")\n    print(f\"Форма PE: {x_pe.shape} (должна быть [1, {seq_len}, {d_model}])\")\n    \n    print(\"\\n2. Тест Multi-Head Attention\")\n    mha = MultiHeadAttention(d_model, num_heads).to(device)\n    q = k = v = torch.randn(batch_size, seq_len, d_model).to(device)\n    attn_output = mha(q, k, v)\n    print(f\"Форма выхода внимания: {attn_output.shape} (должна быть {q.shape})\")\n    print(f\"Максимальное значение: {attn_output.max().item():.4f}\")\n    print(f\"Минимальное значение: {attn_output.min().item():.4f}\")\n\n    print(\"\\n3. Тест Encoder Layer\")\n    encoder_layer = EncoderLayer(d_model, num_heads).to(device)\n    enc_input = torch.randn(batch_size, seq_len, d_model).to(device)\n    enc_output = encoder_layer(enc_input)\n    print(f\"Форма выхода энкодера: {enc_output.shape} (должна быть {enc_input.shape})\")\n    print(f\"Изменение данных: {torch.allclose(enc_input, enc_output, atol=1e-4)} (должно быть False)\")\n\n    print(\"\\n4. Тест Decoder Layer\")\n    decoder_layer = DecoderLayer(d_model, num_heads).to(device)\n    dec_input = torch.randn(batch_size, seq_len, d_model).to(device)\n    dec_output = decoder_layer(dec_input, enc_output, src_mask, tgt_mask)\n    print(f\"Форма выхода декодера: {dec_output.shape} (должна быть {dec_input.shape})\")\n    print(f\"Норма выходных данных: {dec_output.norm().item():.4f}\")\n\n    print(\"\\n5. Полный тест Transformer\")\n    print(\"Входные данные:\")\n    print(f\"src: {src.shape} (max={src.max().item()}, min={src.min().item()})\")\n    print(f\"tgt: {tgt.shape} (max={tgt.max().item()}, min={tgt.min().item()})\")\n    \n    output = transformer(src, tgt, src_mask, tgt_mask)\n    print(\"\\nПроверка формы выхода:\")\n    print(f\"Ожидаемая форма: ({batch_size}, {seq_len}, {tgt_vocab_size})\")\n    print(f\"Реальная форма:   {output.shape}\")\n    \n    print(\"\\nПроверка градиентов:\")\n    dummy_loss = output.sum()\n    dummy_loss.backward()\n    has_gradients = any(p.grad is not None for p in transformer.parameters())\n    print(f\"Градиенты вычислены: {has_gradients} (должно быть True)\")\n\n    print(\"\\n6. Проверка параметров модели:\")\n    total_params = sum(p.numel() for p in transformer.parameters())\n    print(f\"Всего параметров: {total_params}\")\n    print(f\"Параметры энкодера: {sum(p.numel() for p in transformer.encoder_embedding.parameters())}\")\n    print(f\"Параметры декодера: {sum(p.numel() for p in transformer.decoder_embedding.parameters())}\")\n\n    print(\"\\nТест завершен!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:53:20.602325Z","iopub.execute_input":"2025-02-25T19:53:20.602539Z","iopub.status.idle":"2025-02-25T19:53:20.622799Z","shell.execute_reply.started":"2025-02-25T19:53:20.602520Z","shell.execute_reply":"2025-02-25T19:53:20.622098Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_transformer()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:53:20.623421Z","iopub.execute_input":"2025-02-25T19:53:20.623644Z","iopub.status.idle":"2025-02-25T19:53:21.384732Z","shell.execute_reply.started":"2025-02-25T19:53:20.623625Z","shell.execute_reply":"2025-02-25T19:53:21.383994Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def translate_sentence(model, tokenizer, sentence, device, max_length=128):\n    model.eval()\n    \n    inputs = tokenizer(\n        f\"{tokenizer.bos_token} {sentence}\",\n        padding='max_length',\n        truncation=True,\n        max_length=max_length,\n        return_tensors=\"pt\"\n    ).to(device)\n    \n    decoder_input = torch.tensor([[tokenizer.bos_token_id]], device=device)\n    \n    with torch.no_grad():\n        for _ in range(max_length):\n            src_mask = (inputs['input_ids'] != tokenizer.pad_token_id).unsqueeze(1).unsqueeze(2)\n            tgt_mask = torch.tril(torch.ones(decoder_input.size(1), decoder_input.size(1), device=device)).bool()\n            \n            output = model(\n                inputs['input_ids'],\n                decoder_input,\n                src_mask,\n                tgt_mask.unsqueeze(0).unsqueeze(0)\n            )\n            \n            next_token = output[:, -1].argmax(-1)\n            decoder_input = torch.cat([decoder_input, next_token.unsqueeze(1)], dim=1)\n            \n            if next_token.item() == tokenizer.eos_token_id:\n                break\n    \n    return tokenizer.decode(decoder_input[0], skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:53:21.385732Z","iopub.execute_input":"2025-02-25T19:53:21.386039Z","iopub.status.idle":"2025-02-25T19:53:21.391904Z","shell.execute_reply.started":"2025-02-25T19:53:21.386009Z","shell.execute_reply":"2025-02-25T19:53:21.391197Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Тестирование","metadata":{}},{"cell_type":"code","source":"import torch\nprint(torch.__version__)  # Должно быть >= 1.11.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:53:21.392764Z","iopub.execute_input":"2025-02-25T19:53:21.393001Z","iopub.status.idle":"2025-02-25T19:53:21.411416Z","shell.execute_reply.started":"2025-02-25T19:53:21.392982Z","shell.execute_reply":"2025-02-25T19:53:21.410652Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test_tokenization(tokenizer, test_sentence=\"Hello, how are you?\"):\n    print(\"=\"*50)\n    print(\"1. Тестирование токенизации и специальных токенов\")\n    \n    print(f\"BOS: {tokenizer.bos_token} (id: {tokenizer.bos_token_id})\")\n    print(f\"EOS: {tokenizer.eos_token} (id: {tokenizer.eos_token_id})\")\n    \n    # Явное добавление BOS/EOS как в основном пайплайне\n    processed_text = f\"{tokenizer.bos_token} {test_sentence} {tokenizer.eos_token}\"\n    \n    encoded = tokenizer(\n        processed_text,\n        padding='max_length',\n        truncation=True,\n        max_length=20,  # Уменьшаем для наглядности\n        return_tensors='pt',\n        add_special_tokens=False\n    )\n    \n    token_ids = encoded['input_ids'][0].tolist()\n    decoded = tokenizer.decode(token_ids)\n    \n    print(\"\\nПример токенизации:\")\n    print(f\"Токены: {token_ids}\")\n    print(f\"Декодировано: {decoded}\")\n    \n    try:\n        eos_pos = token_ids.index(tokenizer.eos_token_id)\n        pad_start = token_ids.index(tokenizer.pad_token_id)\n        assert eos_pos < pad_start, \"EOS должен быть перед паддингом\"\n    except ValueError:\n        assert False, \"EOS token not found\"\n    \n    assert token_ids[0] == tokenizer.bos_token_id\n    print(\"\\n✅ Токенизация работает корректно\")\n\ndef test_masking():\n    print(\"\\n\" + \"=\"*50)\n    print(\"2. Тестирование создания масок\")\n    \n    batch_size = 2\n    seq_len = 5\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    src = torch.ones(batch_size, seq_len, device=device)\n    tgt = torch.ones(batch_size, seq_len, device=device)\n    \n    src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n    \n    causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=device))\n    tgt_mask = causal_mask.unsqueeze(0).repeat(batch_size, 1, 1, 1)\n    \n    print(\"\\nSource mask (пример):\")\n    print(src_mask[0, 0, 0].cpu().numpy()) \n    \n    print(\"\\nTarget mask (пример):\")\n    print(tgt_mask[0, 0].cpu().numpy())\n    \n    # Проверка размерностей\n    assert src_mask.shape == (batch_size, 1, 1, seq_len), f\"Ожидалось (2,1,1,5), получено {src_mask.shape}\"\n    assert tgt_mask.shape == (batch_size, 1, seq_len, seq_len), f\"Ожидалось (2,1,5,5), получено {tgt_mask.shape}\"\n    \n    print(\"\\n✅ Маски создаются корректно\")\n\ndef test_model_forward_pass(model, tokenizer, device):\n    print(\"\\n\" + \"=\"*50)\n    print(\"3. Тестирование forward pass модели\")\n    \n    test_sentence = \"Test input\"\n    inputs = tokenizer(\n        test_sentence,\n        return_tensors=\"pt\",\n        padding='max_length',\n        max_length=128,\n        truncation=True\n    ).to(device)\n    \n    src = inputs['input_ids']\n    tgt = torch.cat([\n        torch.tensor([[tokenizer.bos_token_id]], device=device),\n        src[:, :-1]\n    ], dim=1)\n\n    src_mask = inputs['attention_mask'].to(torch.bool).unsqueeze(1).unsqueeze(2)\n    seq_len = tgt.size(1)\n    tgt_mask = torch.tril(torch.ones(seq_len, seq_len, device=device)).to(torch.bool)\n    tgt_mask = tgt_mask.unsqueeze(0).unsqueeze(0)\n    \n    model.eval()\n    with torch.no_grad():\n        output = model(src, tgt, src_mask, tgt_mask)\n    \n    print(\"\\nФорма выходов модели:\", output.shape)\n    print(\"Минимальное значение:\", output.min().item())\n    print(\"Максимальное значение:\", output.max().item())\n    \n    assert not torch.isnan(output).any()\n    expected_vocab_size = len(tokenizer)  # Используем реальный размер словаря\n    assert output.shape == (1, 128, expected_vocab_size), (\n        f\"Ожидаемая форма: (1, 128, {expected_vocab_size}), \"\n        f\"получено: {output.shape}\"\n    )\n    print(\"\\n✅ Forward pass работает корректно\")\n\ndef test_dynamic_masking():\n    print(\"Тестирование динамических масок для разных размеров\")\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    num_heads = 8\n    d_model = 512\n    \n    test_cases = [\n        {'batch_size': 1, 'seq_len': 7},\n        {'batch_size': 3, 'seq_len': 15},\n        {'batch_size': 5, 'seq_len': 3}\n    ]\n    \n    for case in test_cases:\n        print(f\"\\nТест для batch={case['batch_size']} seq_len={case['seq_len']}\")\n        \n        src = torch.randint(0, 100, (case['batch_size'], case['seq_len'])).to(device)\n        tgt = torch.randint(0, 100, (case['batch_size'], case['seq_len'])).to(device)\n        \n        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n        tgt_mask = torch.tril(torch.ones(case['seq_len'], case['seq_len'])).to(device)\n        tgt_mask = tgt_mask.unsqueeze(0).unsqueeze(0)\n        \n        mha = MultiHeadAttention(d_model, num_heads).to(device)\n        q = k = v = torch.randn(case['batch_size'], case['seq_len'], d_model).to(device)\n        \n        try:\n            output = mha(q, k, v, tgt_mask)\n            assert output.shape == (case['batch_size'], case['seq_len'], d_model)\n            print(\"✅ Успех\")\n        except Exception as e:\n            print(f\"❌ Ошибка: {str(e)}\")\n            raise\n\ndef test_multi_head_compatibility():\n    print(\"\\nТестирование совместимости масок с Multi-Head Attention\")\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    d_model = 512\n    num_heads_list = [4, 8, 16]\n    seq_lens = [10, 15, 20]\n    \n    for num_heads in num_heads_list:\n        for seq_len in seq_lens:\n            print(f\"\\nHeads: {num_heads}, Seq_len: {seq_len}\")\n            \n            mask = torch.tril(torch.ones(seq_len, seq_len)).to(device)\n            \n            mha = MultiHeadAttention(d_model, num_heads).to(device)\n            q = torch.randn(2, seq_len, d_model).to(device)\n            \n            try:\n                output = mha(q, q, q, mask)\n                assert output.shape == q.shape\n                print(\"✅ Совместимость подтверждена\")\n            except Exception as e:\n                print(f\"❌ Несовместимость: {str(e)}\")\n                raise\n\ndef test_translation_function(model, tokenizer, device):\n    print(\"\\n\" + \"=\"*50)\n    print(\"4. Тестирование функции перевода\")\n    \n    test_sentences = [\n        \"Hello world\",\n        \"How are you?\",\n        \"Test sentence\",\n        \"Machine learning\"\n    ]\n    \n    for sentence in test_sentences:\n        print(\"\\n\" + \"-\"*50)\n        print(f\"Перевод: '{sentence}'\")\n        \n        try:\n            translated = translate_sentence(\n                model=model,\n                tokenizer=tokenizer,\n                sentence=sentence,\n                device=device,\n                max_length=64  # Уменьшенная длина для тестов\n            )\n            print(f\"Результат: {translated}\")\n        except Exception as e:\n            print(f\"Ошибка: {str(e)}\")\n            raise\n    \n    print(\"\\n✅ Функция перевода работает корректно\")\n\ndef test_model_with_various_inputs():\n    print(\"\\nТестирование модели с различными входными размерами\")\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    try:\n        tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-ru\")\n        tokenizer.add_special_tokens({\n            'bos_token': '<s>', 'eos_token': '</s>', \n            'pad_token': '<pad>', 'unk_token': '<unk>'\n        })\n    except Exception as e:\n        print(f\"Ошибка инициализации токенизатора: {e}\")\n        raise\n    \n    d_model = 512\n    num_heads = 8\n    num_layers = 6\n    \n    model = Transformer(\n        src_vocab_size=len(tokenizer),\n        tgt_vocab_size=len(tokenizer),\n        d_model=d_model,\n        num_heads=num_heads,\n        num_layers=num_layers\n    ).to(device)\n    model.eval()\n    \n    test_cases = [\n        {'batch_size': 1, 'src_len': 10, 'tgt_len': 12},\n        {'batch_size': 3, 'src_len': 7, 'tgt_len': 9},\n        {'batch_size': 5, 'src_len': 15, 'tgt_len': 15},\n        {'batch_size': 2, 'src_len': 5, 'tgt_len': 5}\n    ]\n    \n    for case in test_cases:\n        print(f\"\\nТест: batch={case['batch_size']} src={case['src_len']} tgt={case['tgt_len']}\")\n        \n        try:\n            src = torch.randint(0, len(tokenizer), \n                             (case['batch_size'], case['src_len'])).to(device)\n            tgt = torch.randint(0, len(tokenizer), \n                             (case['batch_size'], case['tgt_len'])).to(device)\n            \n            src_mask = (src != tokenizer.pad_token_id).unsqueeze(1).unsqueeze(2)\n            tgt_mask = torch.tril(torch.ones(\n                (case['batch_size'], case['tgt_len'], case['tgt_len']),\n                device=device\n            )).bool().unsqueeze(1)\n            \n            with torch.no_grad():\n                output = model(src, tgt, src_mask, tgt_mask)\n            \n            expected_shape = (\n                case['batch_size'], \n                case['tgt_len'], \n                len(tokenizer)\n            )\n            assert output.shape == expected_shape\n            print(\"✅ Корректная работа\")\n            \n        except Exception as e:\n            print(f\"❌ Ошибка: {str(e)}\")\n            raise\n\ndef run_comprehensive_tests():\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-ru\")\n    tokenizer.add_tokens(['<s>', '</s>', '<pad>', '<unk>'])\n    tokenizer.bos_token = '<s>'\n    tokenizer.eos_token = '</s>'\n    tokenizer.pad_token = '<pad>'\n    tokenizer.unk_token = '<unk>'\n    tokenizer.bos_token_id = tokenizer.convert_tokens_to_ids('<s>')\n    tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids('</s>')\n    tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids('<pad>')\n    \n    model = Transformer(\n        src_vocab_size=len(tokenizer),\n        tgt_vocab_size=len(tokenizer),\n        d_model=512,\n        num_heads=8,\n        num_layers=6\n    ).to(device)\n    \n    test_tokenization(tokenizer)\n    test_masking()\n    test_model_forward_pass(model, tokenizer, device)\n    test_dynamic_masking()\n    test_multi_head_compatibility()\n    test_model_with_various_inputs()\n    test_translation_function(model, tokenizer, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:53:21.413663Z","iopub.execute_input":"2025-02-25T19:53:21.413917Z","iopub.status.idle":"2025-02-25T19:53:21.439494Z","shell.execute_reply.started":"2025-02-25T19:53:21.413898Z","shell.execute_reply":"2025-02-25T19:53:21.438585Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# run_comprehensive_tests()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:53:21.440940Z","iopub.execute_input":"2025-02-25T19:53:21.441168Z","iopub.status.idle":"2025-02-25T19:53:21.456979Z","shell.execute_reply.started":"2025-02-25T19:53:21.441141Z","shell.execute_reply":"2025-02-25T19:53:21.456204Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Обучение","metadata":{}},{"cell_type":"code","source":"pip install rouge","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:53:21.457687Z","iopub.execute_input":"2025-02-25T19:53:21.457880Z","iopub.status.idle":"2025-02-25T19:53:24.998678Z","shell.execute_reply.started":"2025-02-25T19:53:21.457863Z","shell.execute_reply":"2025-02-25T19:53:24.997779Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install nltk==3.5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:53:24.999829Z","iopub.execute_input":"2025-02-25T19:53:25.000215Z","iopub.status.idle":"2025-02-25T19:53:31.993899Z","shell.execute_reply.started":"2025-02-25T19:53:25.000180Z","shell.execute_reply":"2025-02-25T19:53:31.993034Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, random_split, Subset\nfrom torch.optim import Adam\nfrom torch.nn.utils.rnn import pad_sequence\nimport numpy as np\nfrom tqdm.auto import tqdm\nimport wandb\nimport os\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\nfrom rouge import Rouge\nfrom nltk.translate.meteor_score import meteor_score\nimport nltk\nnltk.download('wordnet')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:53:31.994878Z","iopub.execute_input":"2025-02-25T19:53:31.995161Z","iopub.status.idle":"2025-02-25T19:53:32.612430Z","shell.execute_reply.started":"2025-02-25T19:53:31.995138Z","shell.execute_reply":"2025-02-25T19:53:32.611621Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:53:32.613266Z","iopub.execute_input":"2025-02-25T19:53:32.613513Z","iopub.status.idle":"2025-02-25T19:53:32.617166Z","shell.execute_reply.started":"2025-02-25T19:53:32.613492Z","shell.execute_reply":"2025-02-25T19:53:32.616323Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TranslationDataset(Dataset):\n    def __init__(self, processed_data):\n        self.input_ids = processed_data['input_ids']\n        self.attention_mask = processed_data['attention_mask']\n        self.labels = processed_data['labels']\n        self.decoder_attention_mask = processed_data['decoder_attention_mask']\n        \n    def __len__(self):\n        return len(self.input_ids)\n    \n    def __getitem__(self, idx):\n        return {\n            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n            'attention_mask': torch.tensor(self.attention_mask[idx], dtype=torch.long),\n            'labels': torch.tensor(self.labels[idx], dtype=torch.long),\n            'decoder_attention_mask': torch.tensor(self.decoder_attention_mask[idx], dtype=torch.long)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:53:32.617986Z","iopub.execute_input":"2025-02-25T19:53:32.618221Z","iopub.status.idle":"2025-02-25T19:53:32.633531Z","shell.execute_reply.started":"2025-02-25T19:53:32.618202Z","shell.execute_reply":"2025-02-25T19:53:32.632915Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from nltk.translate.meteor_score import meteor_score\nfrom nltk.translate.meteor_score import single_meteor_score\nfrom nltk import word_tokenize\n\ndef calculate_metrics(references, predictions, tokenizer):\n    non_empty_indices = [i for i, pred in enumerate(predictions) if len(pred.strip()) > 0]\n    valid_preds = [predictions[i] for i in non_empty_indices]\n    valid_refs = [references[i] for i in non_empty_indices]\n\n    metrics = {\n        'bleu': 0.0,\n        'rouge-1': {'f': 0.0},\n        'rouge-2': {'f': 0.0},\n        'rouge-l': {'f': 0.0},\n        'meteor': 0.0,\n        'meteor_precision': 0.0,\n        'meteor_recall': 0.0\n    }\n\n    if not valid_preds:\n        return metrics\n\n    # BLEU calculation\n    refs_bleu = [[ref.strip().split()] for ref in valid_refs]\n    preds_bleu = [pred.strip().split() for pred in valid_preds]\n    \n    smooth = SmoothingFunction().method4\n    if preds_bleu:\n        metrics['bleu'] = corpus_bleu(refs_bleu, preds_bleu, smoothing_function=smooth)\n\n    # ROUGE calculation\n    rouge = Rouge()\n    try:\n        if valid_preds:\n            rouge_scores = rouge.get_scores(valid_preds, valid_refs, avg=True)\n            metrics['rouge-1'] = rouge_scores['rouge-1']['f']\n            metrics['rouge-2'] = rouge_scores['rouge-2']['f']\n            metrics['rouge-l'] = rouge_scores['rouge-l']['f']\n    except Exception as e:\n        print(f\"ROUGE calculation error: {str(e)}\")\n\n    meteor_scores = []\n    precisions = []\n    recalls = []\n    \n    for ref, pred in zip(valid_refs, valid_preds):\n        try:\n            # Используем оригинальную обработку METEOR\n            ref_str = ref.strip()\n            pred_str = pred.strip()\n            \n            # Официальный расчет METEOR\n            score = single_meteor_score(ref_str, pred_str)\n            meteor_scores.append(score)\n            \n            # Для совместимости сохраняем простой расчет precision/recall\n            ref_tokens = word_tokenize(ref_str)\n            pred_tokens = word_tokenize(pred_str)\n            \n            matches = sum(1 for word in pred_tokens if word in ref_tokens)\n            precision = matches/len(pred_tokens) if len(pred_tokens) > 0 else 0.0\n            recall = matches/len(ref_tokens) if len(ref_tokens) > 0 else 0.0\n            \n            precisions.append(precision)\n            recalls.append(recall)\n            \n        except Exception as e:\n            print(f\"METEOR calculation error: {str(e)}\")\n            meteor_scores.append(0.0)\n            precisions.append(0.0)\n            recalls.append(0.0)\n\n    if meteor_scores:\n        metrics['meteor'] = np.mean(meteor_scores)\n        metrics['meteor_precision'] = np.mean(precisions)\n        metrics['meteor_recall'] = np.mean(recalls)\n\n    return metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:53:32.634216Z","iopub.execute_input":"2025-02-25T19:53:32.634433Z","iopub.status.idle":"2025-02-25T19:53:32.645144Z","shell.execute_reply.started":"2025-02-25T19:53:32.634414Z","shell.execute_reply":"2025-02-25T19:53:32.644456Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_translation_table(examples_data):\n    return wandb.Table(\n        columns=[\"epoch\", \"step\", \"source\", \"target\", \"prediction\"],\n        data=examples_data\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:53:32.645816Z","iopub.execute_input":"2025-02-25T19:53:32.646014Z","iopub.status.idle":"2025-02-25T19:53:32.660796Z","shell.execute_reply.started":"2025-02-25T19:53:32.645997Z","shell.execute_reply":"2025-02-25T19:53:32.660180Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def init_wandb(config):\n    return wandb.init(\n        project=\"transformer-translation-en-ru\",\n        config=config,\n        name=config.get(\"name\", \"experiment\"),\n        group=config.get(\"group\", \"default\"),\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:53:32.661470Z","iopub.execute_input":"2025-02-25T19:53:32.661656Z","iopub.status.idle":"2025-02-25T19:53:32.674656Z","shell.execute_reply.started":"2025-02-25T19:53:32.661639Z","shell.execute_reply":"2025-02-25T19:53:32.674021Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_model(model, val_loader, criterion, tokenizer, device, max_examples=10):\n    model.eval()\n    total_loss = 0\n    all_sources = []\n    all_targets = []\n    all_predictions = []\n    \n    with torch.no_grad():\n        for batch in val_loader:\n            input_ids = batch['input_ids'].to(device)\n            labels = batch['labels'].to(device)\n            \n            src_mask = (input_ids != tokenizer.pad_token_id).unsqueeze(1).unsqueeze(2)\n            tgt_mask = torch.tril(torch.ones(\n                labels.size(1)-1, \n                labels.size(1)-1, \n                device=device\n            )).bool()\n            \n            outputs = model(\n                input_ids,\n                labels[:, :-1],\n                src_mask,\n                tgt_mask.unsqueeze(0).unsqueeze(0))\n            \n            loss = criterion(\n                outputs.reshape(-1, outputs.size(-1)),\n                labels[:, 1:].reshape(-1))\n            total_loss += loss.item()\n            \n            if len(all_sources) < max_examples:\n                sources = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n                targets = tokenizer.batch_decode(labels, skip_special_tokens=True)\n                preds = [translate_sentence(model, tokenizer, src, device) for src in sources]\n                all_sources.extend(sources)\n                all_targets.extend(targets)\n                all_predictions.extend(preds)\n    \n    metrics = calculate_metrics(all_targets, all_predictions, tokenizer)\n    \n    return (\n        total_loss / len(val_loader),\n        metrics,\n        all_sources[:max_examples],\n        all_targets[:max_examples],\n        all_predictions[:max_examples]\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:53:32.675927Z","iopub.execute_input":"2025-02-25T19:53:32.676237Z","iopub.status.idle":"2025-02-25T19:53:32.688980Z","shell.execute_reply.started":"2025-02-25T19:53:32.676216Z","shell.execute_reply":"2025-02-25T19:53:32.688238Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def prepare_dataset(subset_size, tokenizer):\n    dataset = load_translation_dataset()\n    processed_data, tokenizer = prepare_data_with_hf(dataset)\n    full_dataset = TranslationDataset(processed_data)\n    \n    indices = range(min(subset_size, len(full_dataset)))\n    subset = Subset(full_dataset, indices)\n    \n    train_size = int(0.7 * len(subset))\n    val_size = int(0.15 * len(subset))\n    test_size = len(subset) - train_size - val_size\n    \n    train_dataset, val_dataset, test_dataset = random_split(\n        subset, [train_size, val_size, test_size],\n        generator=torch.Generator().manual_seed(42)\n    )\n    \n    return train_dataset, val_dataset, test_dataset, tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:53:32.689844Z","iopub.execute_input":"2025-02-25T19:53:32.690077Z","iopub.status.idle":"2025-02-25T19:53:32.706351Z","shell.execute_reply.started":"2025-02-25T19:53:32.690058Z","shell.execute_reply":"2025-02-25T19:53:32.705644Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_data_loaders(train_dataset, val_dataset, test_dataset, batch_size):\n    train_loader = DataLoader(\n        train_dataset, \n        batch_size=batch_size, \n        shuffle=True,\n        num_workers=2,\n        pin_memory=True\n    )\n    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n    \n    return train_loader, val_loader, test_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:53:32.707157Z","iopub.execute_input":"2025-02-25T19:53:32.707426Z","iopub.status.idle":"2025-02-25T19:53:32.724462Z","shell.execute_reply.started":"2025-02-25T19:53:32.707397Z","shell.execute_reply":"2025-02-25T19:53:32.723807Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\ndef train_epoch(model, train_loader, val_loader, optimizer, criterion, tokenizer, \n               device, epoch, epochs, global_step, eval_steps, cumulative_examples, best_val_loss):\n    model.train()\n    total_loss = 0\n    progress_bar = tqdm(\n        train_loader, \n        desc=f'Epoch {epoch+1}/{epochs}',\n        bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}'\n    )\n    \n    bleu = \"n/a\"  # Инициализация для postfix в tqdm\n    \n    for batch in progress_bar:\n        input_ids = batch['input_ids'].to(device)\n        labels = batch['labels'].to(device)\n        \n        src_mask = (input_ids != tokenizer.pad_token_id).unsqueeze(1).unsqueeze(2)\n        tgt_mask = torch.tril(torch.ones(\n            labels.size(1)-1, \n            labels.size(1)-1, \n            device=device\n        )).bool()\n        \n        outputs = model(\n            input_ids,\n            labels[:, :-1],\n            src_mask,\n            tgt_mask.unsqueeze(0).unsqueeze(0))\n        \n        loss = criterion(\n            outputs.reshape(-1, outputs.size(-1)),\n            labels[:, 1:].reshape(-1))\n        \n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        \n        total_loss += loss.item()\n        global_step += 1\n        \n        if global_step % eval_steps == 0:\n            val_loss, bleu, sources, targets, preds = evaluate_model(\n                model, val_loader, criterion, tokenizer, device)\n            \n            new_entries = [\n                [epoch, global_step, src, tgt, pred]\n                for src, tgt, pred in zip(sources, targets, preds)\n            ]\n            cumulative_examples.extend(new_entries)\n            \n            trans_table = create_translation_table(cumulative_examples)\n            \n            wandb.log({\n                \"train/loss\": loss.item(),\n                \"val/loss\": val_loss,\n                \"val/bleu\": bleu,\n                \"examples\": trans_table,\n                \"epoch\": epoch,\n                \"step\": global_step\n            })\n            \n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                torch.save(model.state_dict(), \"best_model.pth\")\n                wandb.save(\"best_model.pth\")\n        \n        progress_bar.set_postfix({\n            \"loss\": f\"{loss.item():.4f}\",\n            \"bleu\": f\"{bleu:.2f}\" if bleu != \"n/a\" else \"n/a\"\n        })\n    \n    avg_loss = total_loss / len(train_loader)\n    wandb.log({\n        \"epoch/train_loss\": avg_loss,\n        \"epoch\": epoch\n    })\n    \n    return global_step, best_val_loss\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:53:32.725389Z","iopub.execute_input":"2025-02-25T19:53:32.725668Z","iopub.status.idle":"2025-02-25T19:53:32.740612Z","shell.execute_reply.started":"2025-02-25T19:53:32.725641Z","shell.execute_reply":"2025-02-25T19:53:32.739904Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_model_artifacts(model, test_sources, test_targets, test_preds, test_loss, test_bleu, epoch, global_step):\n    test_entries = [\n        [epoch, global_step, src, tgt, pred]\n        for src, tgt, pred in zip(test_sources, test_targets, test_preds)\n    ]\n    test_table = create_translation_table(test_entries)\n    \n    wandb.log({\n        \"test/loss\": test_loss,\n        \"test/bleu\": test_bleu,\n        \"test_examples\": test_table\n    })\n    \n    torch.save(model.state_dict(), \"model.pth\")\n    \n    model_artifact = wandb.Artifact(\"translation-model\", type=\"model\")\n    model_artifact.add_file(\"model.pth\")\n    wandb.log_artifact(model_artifact)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:53:32.741445Z","iopub.execute_input":"2025-02-25T19:53:32.741730Z","iopub.status.idle":"2025-02-25T19:53:32.758323Z","shell.execute_reply.started":"2025-02-25T19:53:32.741693Z","shell.execute_reply":"2025-02-25T19:53:32.757629Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\ndef train_model(subset_size=100, epochs=5, batch_size=32, lr=0.0001, eval_steps=50):\n    config = {\n        \"group\": \"baseline-transformer\",\n        \"architecture\": \"transformer\",\n        \"name\": \"transformer-base-subset30000-epoch-3-batch64-v1\",\n        \"subset_size\": subset_size,\n        \"epochs\": epochs,\n        \"batch_size\": batch_size,\n        \"learning_rate\": lr,\n        \"eval_steps\": eval_steps,\n        \"d_model\": 256,\n        \"num_heads\": 8,\n        \"num_layers\": 4,\n    }\n    \n    run = init_wandb(config=config)\n    \n    train_dataset, val_dataset, test_dataset, tokenizer = prepare_dataset(subset_size, tokenizer=None)\n    train_loader, val_loader, test_loader = create_data_loaders(train_dataset, val_dataset, test_dataset, batch_size)\n    \n    device = DEVICE\n    model = Transformer(\n        src_vocab_size=len(tokenizer),\n        tgt_vocab_size=len(tokenizer),\n        **{k: v for k, v in config.items() if k in ['d_model', 'num_heads', 'num_layers']}\n    ).to(device)\n    \n    optimizer = Adam(model.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n    \n    if run:\n        wandb.watch(model, log=\"all\", log_freq=eval_steps)\n    \n    best_val_loss = float('inf')\n    global_step = 0\n    cumulative_examples = []\n    \n    for epoch in range(epochs):\n        global_step, best_val_loss = train_epoch(\n            model, train_loader, val_loader, optimizer, criterion, tokenizer,\n            device, epoch, epochs, global_step, eval_steps, cumulative_examples, best_val_loss\n        )\n    \n    test_loss, test_bleu, test_sources, test_targets, test_preds = evaluate_model(\n        model, test_loader, criterion, tokenizer, device, max_examples=10)\n    \n    save_model_artifacts(model, test_sources, test_targets, test_preds, test_loss, test_bleu, epochs-1, global_step)\n    \n    wandb.finish()\n    \n    return model, best_val_loss, test_loss\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:53:32.761421Z","iopub.execute_input":"2025-02-25T19:53:32.761614Z","iopub.status.idle":"2025-02-25T19:53:32.777812Z","shell.execute_reply.started":"2025-02-25T19:53:32.761598Z","shell.execute_reply":"2025-02-25T19:53:32.777222Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nmodel, best_val_loss, test_loss = train_model(\n        subset_size=30000,\n        epochs=3,\n        batch_size=32,\n        lr=0.0001,\n        eval_steps=100\n    )\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:53:32.778850Z","iopub.execute_input":"2025-02-25T19:53:32.779039Z","iopub.status.idle":"2025-02-25T19:53:32.796374Z","shell.execute_reply.started":"2025-02-25T19:53:32.779023Z","shell.execute_reply":"2025-02-25T19:53:32.795759Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Оптимизация и регуляризация","metadata":{}},{"cell_type":"code","source":"from torch.optim.lr_scheduler import LambdaLR\nimport math","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:53:32.797196Z","iopub.execute_input":"2025-02-25T19:53:32.797431Z","iopub.status.idle":"2025-02-25T19:53:32.813293Z","shell.execute_reply.started":"2025-02-25T19:53:32.797403Z","shell.execute_reply":"2025-02-25T19:53:32.812389Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_cosine_schedule_with_warmup(\n    optimizer, \n    num_warmup_steps: int,\n    num_training_steps: int, \n    num_cycles: float = 0.5, \n    last_epoch: int = -1\n):\n    \"\"\"\n    Args:\n        num_warmup_steps (int): Количество шагов для фазы разогрева\n        num_training_steps (int): Общее количество шагов обучения\n    \"\"\"\n    def lr_lambda(current_step):\n        if current_step < num_warmup_steps:\n            return float(current_step) / float(max(1, num_warmup_steps))\n        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * num_cycles * 2.0 * progress)))\n    return LambdaLR(optimizer, lr_lambda, last_epoch)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:53:32.814151Z","iopub.execute_input":"2025-02-25T19:53:32.814424Z","iopub.status.idle":"2025-02-25T19:53:32.835676Z","shell.execute_reply.started":"2025-02-25T19:53:32.814404Z","shell.execute_reply":"2025-02-25T19:53:32.834721Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def initialize_xavier(model):\n    for name, p in model.named_parameters():\n        if p.dim() > 1 and 'embedding' not in name:\n            nn.init.xavier_uniform_(p)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:53:32.836760Z","iopub.execute_input":"2025-02-25T19:53:32.837115Z","iopub.status.idle":"2025-02-25T19:53:32.911091Z","shell.execute_reply.started":"2025-02-25T19:53:32.837082Z","shell.execute_reply":"2025-02-25T19:53:32.910206Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class LabelSmoothingLoss(nn.Module):\n    def __init__(self, smoothing=0.1, num_classes=None, ignore_index=-100):\n        super().__init__()\n        self.smoothing = smoothing\n        self.num_classes = num_classes\n        self.ignore_index = ignore_index\n\n    def forward(self, logits, targets):\n        if self.num_classes is None:\n            self.num_classes = logits.size(-1)\n        \n        mask = (targets == self.ignore_index).unsqueeze(-1)\n        targets_onehot = torch.zeros_like(logits).scatter_(\n            -1, \n            targets.unsqueeze(-1), \n            1 - self.smoothing\n        )\n        targets_onehot += (self.smoothing / (self.num_classes - 1)) * (1 - targets_onehot)\n        targets_onehot.masked_fill_(mask, 0)\n        \n        loss = - (targets_onehot * F.log_softmax(logits, dim=-1)).sum(dim=-1)\n        return loss[~mask.squeeze()].mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:53:32.912206Z","iopub.execute_input":"2025-02-25T19:53:32.912516Z","iopub.status.idle":"2025-02-25T19:53:32.927680Z","shell.execute_reply.started":"2025-02-25T19:53:32.912488Z","shell.execute_reply":"2025-02-25T19:53:32.926963Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_epoch(model, train_loader, val_loader, optimizer, criterion, scheduler,\n               tokenizer, device, epoch, epochs, global_step, eval_steps, \n               cumulative_examples, best_val_loss):\n    model.train()\n    total_loss = 0\n    progress_bar = tqdm(\n        train_loader, \n        desc=f'Epoch {epoch+1}/{epochs}',\n        bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}'\n    )\n    \n    start_time = time.time()\n    \n    for batch in progress_bar:\n        input_ids = batch['input_ids'].to(device)\n        labels = batch['labels'].to(device)\n        \n        src_mask = (input_ids != tokenizer.pad_token_id).unsqueeze(1).unsqueeze(2)\n        tgt_mask = torch.tril(torch.ones(\n            labels.size(1)-1, \n            labels.size(1)-1, \n            device=device\n        )).bool()\n        \n        outputs = model(\n            input_ids,\n            labels[:, :-1],\n            src_mask,\n            tgt_mask.unsqueeze(0).unsqueeze(0))\n        \n        loss = criterion(\n            outputs.reshape(-1, outputs.size(-1)),\n            labels[:, 1:].reshape(-1))\n        \n        optimizer.zero_grad()\n        loss.backward()\n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        \n        optimizer.step()\n        scheduler.step() \n        \n        total_loss += loss.item()\n        global_step += 1\n        \n        if global_step % eval_steps == 0:\n            val_loss, metrics, sources, targets, preds = evaluate_model(\n                model, val_loader, criterion, tokenizer, device\n            )\n            \n            new_entries = [\n                [epoch, global_step, src, tgt, pred]\n                for src, tgt, pred in zip(sources, targets, preds)\n            ]\n            cumulative_examples.extend(new_entries)\n            \n            trans_table = create_translation_table(cumulative_examples)\n            \n            log_data = {\n                \"epoch\": epoch,\n                \"global_step\": global_step,\n                \n                \"train/loss\": loss.item(),\n                \"train/learning_rate\": scheduler.get_last_lr()[0],\n                \n                \"val/loss\": val_loss,\n                \"val/bleu\": metrics['bleu'],\n                \"val/rouge\": {\n                    \"rouge-1\": metrics['rouge-1'],\n                    \"rouge-2\": metrics['rouge-2'],\n                    \"rouge-l\": metrics['rouge-l']\n                },\n                \"val/meteor\": {\n                    \"score\": metrics['meteor'],\n                    \"precision\": metrics['meteor_precision'],\n                    \"recall\": metrics['meteor_recall']\n                },\n                \n                \"system/memory\": {\n                    \"allocated_gb\": torch.cuda.memory_allocated()/1e9,\n                    \"reserved_gb\": torch.cuda.memory_reserved()/1e9,\n                    \"max_allocated_gb\": torch.cuda.max_memory_allocated()/1e9\n                },\n                \n                \"examples/translations\": trans_table}\n            \n            wandb.log(log_data)\n            \n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                torch.save(model.state_dict(), \"best_model.pth\")\n                wandb.save(\"best_model.pth\")\n        \n        progress_bar.set_postfix({\n            \"loss\": f\"{loss.item():.4f}\",\n            \"lr\": f\"{scheduler.get_last_lr()[0]:.2e}\",\n            \"bleu\": f\"{metrics.get('bleu', 0):.2f}\" if 'metrics' in locals() else \"n/a\"\n        })\n    \n    epoch_time = time.time() - start_time\n    avg_loss = total_loss / len(train_loader)\n    \n    wandb.log({\n        \"epoch/train_loss\": avg_loss,\n        \"epoch/time\": epoch_time,\n        \"epoch\": epoch\n    })\n    \n    return global_step, best_val_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:53:32.928530Z","iopub.execute_input":"2025-02-25T19:53:32.928823Z","iopub.status.idle":"2025-02-25T19:53:32.946892Z","shell.execute_reply.started":"2025-02-25T19:53:32.928793Z","shell.execute_reply":"2025-02-25T19:53:32.946293Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(\n    subset_size=100, \n    epochs=5, \n    batch_size=32, \n    lr=0.0001, \n    eval_steps=50, \n    warmup_steps=1000\n):\n    config = {\n        \"group\": \"transformer-experimantal\",\n        \"name\": f\"transformer-warmup{warmup_steps}steps-1e3-32-100k-v1\",\n        \"architecture\": \"transformer-xavier-cosine-ls-100\",\n        \"subset_size\": subset_size,\n        \"epochs\": epochs,\n        \"batch_size\": batch_size,\n        \"learning_rate\": lr,\n        \"eval_steps\": eval_steps,\n        \"d_model\": 512,\n        \"num_heads\": 8,\n        \"num_layers\": 4,\n        \"label_smoothing\": 0.1,\n        \"warmup_steps\": warmup_steps, \n        \"dataset\": \"Tatoeba en-ru\",\n        \"optimizer\": \"Adam\",\n        \"scheduler\": \"cosine_with_warmup\"\n    }\n    \n    run = init_wandb(config=config)\n    \n    train_dataset, val_dataset, test_dataset, tokenizer = prepare_dataset(subset_size, tokenizer=None)\n    train_loader, val_loader, test_loader = create_data_loaders(train_dataset, val_dataset, test_dataset, batch_size)\n\n    dataset_wandb_config = {\"train_size\": len(train_dataset), \"val_size\": len(val_dataset), \"test_size\": len(test_dataset)}\n    wandb.config.update({\"dataset\": dataset_wandb_config}, allow_val_change=True)\n    \n    device = DEVICE\n    \n    model = Transformer(\n        src_vocab_size=len(tokenizer),\n        tgt_vocab_size=len(tokenizer),\n        **{k: v for k, v in config.items() if k in ['d_model', 'num_heads', 'num_layers']}\n    ).to(device)\n    \n    initialize_xavier(model)\n    \n    optimizer = Adam(model.parameters(), lr=lr, betas=(0.9, 0.98), eps=1e-9)\n    criterion = LabelSmoothingLoss(\n        smoothing=config['label_smoothing'],\n        ignore_index=tokenizer.pad_token_id\n    )\n    \n    num_training_steps = epochs * len(train_loader)\n    num_warmup_steps = config['warmup_steps']  # Прямое использование переданного значения\n    \n    # Проверка чтобы warmup_steps не превышал общее количество шагов\n    if num_warmup_steps > num_training_steps:\n        num_warmup_steps = int(num_training_steps * 0.1)\n        print(f\"Warning: warmup_steps превышает общее количество шагов. Установлено в {num_warmup_steps}\")\n    \n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=num_warmup_steps,\n        num_training_steps=num_training_steps\n    )\n    \n    if run:\n        wandb.watch(model, log=\"all\", log_freq=eval_steps, log_graph=True)\n    \n    best_val_loss = float('inf')\n    global_step = 0\n    cumulative_examples = []\n    \n    for epoch in range(epochs):\n        global_step, best_val_loss = train_epoch(\n            model, train_loader, val_loader, \n            optimizer, criterion, scheduler,\n            tokenizer, device, epoch, epochs, \n            global_step, eval_steps, cumulative_examples, best_val_loss\n        )\n    \n    test_loss, test_metrics, test_sources, test_targets, test_preds = evaluate_model(\n        model, test_loader, criterion, tokenizer, device, max_examples=15\n    )\n    \n    save_model_artifacts(model, test_sources, test_targets, test_preds, \n                        test_loss, test_metrics['bleu'], epochs-1, global_step)\n    \n    wandb.log({\"test/loss\": test_loss,\n               \"test/bleu\": test_metrics['bleu'],\n               \n               \"test/rouge\": {\n                   \"rouge-1\": test_metrics['rouge-1'],\n                   \"rouge-2\": test_metrics['rouge-2'],\n                   \"rouge-l\": test_metrics['rouge-l']\n               },\n               \n               \"test/meteor\": {\n                   \"score\": test_metrics['meteor'],\n                   \"precision\": test_metrics['meteor_precision'],\n                   \"recall\": test_metrics['meteor_recall']\n               }})\n    \n    wandb.finish()\n    \n    return model, best_val_loss, test_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:53:32.947678Z","iopub.execute_input":"2025-02-25T19:53:32.947977Z","iopub.status.idle":"2025-02-25T19:53:32.964322Z","shell.execute_reply.started":"2025-02-25T19:53:32.947939Z","shell.execute_reply":"2025-02-25T19:53:32.963525Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model, best_val_loss, test_loss = train_model(\n    subset_size=100000,\n    epochs=3,\n    batch_size=32,\n    lr=1e-3,\n    eval_steps=100,\n    warmup_steps=500\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:53:32.965114Z","iopub.execute_input":"2025-02-25T19:53:32.965396Z","iopub.status.idle":"2025-02-25T20:03:47.680587Z","shell.execute_reply.started":"2025-02-25T19:53:32.965368Z","shell.execute_reply":"2025-02-25T20:03:47.678935Z"}},"outputs":[],"execution_count":null}]}