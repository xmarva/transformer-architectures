{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30886,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Токенизация\n",
        "\n",
        "Токенизация — фундаментальный этап обработки естественного языка, задача которого — разбить текст на осмысленные единицы (токены).\n",
        "\n",
        "Этими единицами могут быть слова, части слов или даже символы. Исторически использовались простые методы: разделение по пробелам, регулярные выражения для выделения слов и знаков препинания, ручные правила для обработки сокращений. Однако такие подходы плохо масштабировались для языков с агглютинативной морфологией (например, русский или финский) и сложных словосочетаний.\n",
        "\n",
        "Так же традиционные методы токенизации, такие как разделение по пробелам или ручные правила, часто оказываются неэффективными в реальных сценариях: они плохо справляются с опечатками, редкими словами и многоязычными текстами. Например, слово \"веееееерно\" или смесь языков в одном предложении могут сломать классический токенайзер.\n",
        "\n",
        "В современных NLP доминируют алгоритмы субсловной токенизации, такие как BPE (Byte Pair Encoding), которые балансируют между смысловой цельностью токенов и эффективным использованием словаря. В этом ноутбуке мы подробно рассмотрим алгоритм BPE и SentencePiece, а так же научимся работать с токенизаторами библиотеки hugging-face.\n",
        "\n",
        "Вначале мы импоритруем все библиотеки и функции, которые понадобятся нам в этом ноутбуке."
      ],
      "metadata": {
        "id": "-E3I6gIaPiMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from itertools import chain\n",
        "from typing import List, Dict, Tuple\n",
        "from collections import Counter, defaultdict"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "_kg_hide-input": true,
        "id": "OVfBnB0dPiM2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Загрузка данных\n",
        "\n",
        "Для демонстрации загрузим параллельный англо-русский корпус [Tatoeba](https://arxiv.org/abs/1812.10464), представленный в работе Artetxe et al. (2019) из библиотеки [Hugging Face Datasets](http://huggingface.co/docs/datasets/loading).\n",
        "\n",
        "![tatoeba-web.png](attachment:f45503be-7882-4348-b1dd-c9b48d91f4cf.png)\n",
        "\n",
        "[Tatoeba](https://tatoeba.org/en/sentences/index) - это бесплатная коллекция примеров предложений с переводом, предназначенная для изучающих иностранные языки. Она доступна более чем на 400 языках. Его название происходит от японской фразы «tatoeba» (例えば), означающей «для примера». Он написан и поддерживается сообществом добровольцев по модели открытого сотрудничества. Отдельные авторы известны как татоэбаны.\n",
        "\n",
        "Мы воспользуемся наборами только для английского и русского языка. Все примеры в этом датасете являются короткими бытовыми фразами: \"Let's try something.\" → \"Давайте что-нибудь попробуем!\".\n",
        "\n",
        "Такой формат удобен для обучения трансформеров, которые работают с последовательностями ограниченной длины."
      ],
      "metadata": {
        "id": "0fv1gNmDPiM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_translation_dataset():\n",
        "    print(\"Loading Tatoeba en-ru...\")\n",
        "    try:\n",
        "        dataset = load_dataset(\"Helsinki-NLP/tatoeba\", lang1=\"en\", lang2=\"ru\", trust_remote_code=True)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error while loading dataset: {e}\")\n",
        "        raise\n",
        "\n",
        "    print(\"\\nDataset structure:\")\n",
        "    print(dataset)\n",
        "\n",
        "    print(\"\\nData sample:\")\n",
        "    for i in range(2):\n",
        "        print(f\"EN: {dataset['train'][i]['translation']['en']}\")\n",
        "        print(f\"RU: {dataset['train'][i]['translation']['ru']}\\n\")\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "trusted": true,
        "id": "3tSwslrQPiM4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_translation_dataset()"
      ],
      "metadata": {
        "trusted": true,
        "id": "qMLmsO8tPiM4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Анализ данных\n",
        "\n",
        "Давайте быстро взглянем на датасет, чтобы понять с чем мы имеем дело. В этом ноутбуке мы не будем углубляться в методы анализа данных, но все же посмотрим базовую статистику.\n",
        "\n",
        "Функция analyze_dataset показывает, что средняя длина английских предложений — 7.2 слова, русских — 6.2. Максимальные длины (30 и 28 слов) указывают на наличие выбросов, которые могут требовать обрезки.\n",
        "\n",
        "Гистограммы демонстрируют правостороннее распределение: большинство предложений короче 15 слов. Эти наблюдения влияют на выбор гиперпараметров модели, например, max_length=64 обеспечивает запас для паддинга, даже если реальные последовательности короче."
      ],
      "metadata": {
        "id": "IKjxicqNPiM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_dataset(dataset, n_samples: int = 1000):\n",
        "    samples = dataset['train'].select(range(n_samples))\n",
        "\n",
        "    en_lengths = [len(s['translation']['en'].split()) for s in samples]\n",
        "    ru_lengths = [len(s['translation']['ru'].split()) for s in samples]\n",
        "\n",
        "    print(f\"Analysis based on first {n_samples} samples:\")\n",
        "    print(f\"\\nEnglish sentences:\")\n",
        "    print(f\"Average length: {np.mean(en_lengths):.1f} words\")\n",
        "    print(f\"Max length: {max(en_lengths)} words\")\n",
        "    print(f\"Min length: {min(en_lengths)} words\")\n",
        "\n",
        "    print(f\"\\nRussian sentences:\")\n",
        "    print(f\"Average length: {np.mean(ru_lengths):.1f} words\")\n",
        "    print(f\"Max length: {max(ru_lengths)} words\")\n",
        "    print(f\"Min length: {min(ru_lengths)} words\")\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.histplot(en_lengths, bins=30)\n",
        "    plt.title('English Sentence Lengths')\n",
        "    plt.xlabel('Words')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sns.histplot(ru_lengths, bins=30)\n",
        "    plt.title('Russian Sentence Lengths')\n",
        "    plt.xlabel('Words')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return max(max(en_lengths), max(ru_lengths))"
      ],
      "metadata": {
        "trusted": true,
        "id": "1PkxaYayPiM5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "max_sentence_length = analyze_dataset(dataset)"
      ],
      "metadata": {
        "trusted": true,
        "id": "bhv3APY9PiM5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Простой токенайзер\n",
        "\n",
        "Простой токенизатор реализует базовую токенизацию на уровне слов с минимальной предобработкой текста. Класс `BaseTokenizer` имеет несколько ключевых этапов работы:\n",
        "\n",
        "1. **Препроцессинг текста**: Текст преобразуется в нижний регистр и затем разделяется на токены с использованием регулярного выражения. Это регулярное выражение выделяет слова, содержащие апострофы, а также пунктуацию как отдельные токены. В результате текст разбивается на элементы, такие как слова и знаки препинания, которые затем используются для дальнейшего анализа.\n",
        "\n",
        "2. **Инициализация словаря**: При инициализации токенизатора задаются специальные токены, такие как `<PAD>`, `<UNK>`, `<BOS>`, `<EOS>`, которые добавляются в словарь. Эти токены имеют фиксированные индексы и используются для обработки данных, например, для выравнивания последовательностей в модели.\n",
        "\n",
        "3. **Сбор статистики**: Метод для сбора статистики проходит по набору текстов, применяет токенизацию к каждому тексту и подсчитывает частоту появления токенов. Это позволяет создать статистику, на основе которой можно построить словарь для дальнейшего использования в моделях обработки естественного языка.\n",
        "\n",
        "4. **Особенности подхода**:\n",
        "   - Преобразование текста в нижний регистр может привести к потере информации о регистре.\n",
        "   - Токенизация не учитывает морфологические особенности слов, что может привести к проблемам с редкими словами или омонимами.\n",
        "   - Пунктуация и другие символы выделяются как отдельные токены, что может быть полезно для задач, где важна структура предложений.\n",
        "\n",
        "Вся работа токенизатора сводится к простым шагам: преобразование текста, токенизация и сбор статистики для построения словаря. Это базовый подход, который можно адаптировать и улучшать для более сложных задач.\n",
        "\n",
        "Декоратор `@dataclass` автоматически генерирует стандартные методы класса (`__init__`, `__repr__`, `__eq__`)"
      ],
      "metadata": {
        "id": "WwEkMVWbPiM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class BaseTokenizer:\n",
        "    language: str\n",
        "    vocab_size: int\n",
        "    min_freq: int = 2\n",
        "    special_tokens: List[str] = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.special_tokens = self.special_tokens or [\"<PAD>\", \"<UNK>\", \"<BOS>\", \"<EOS>\"]\n",
        "        self.token2id = {token: idx for idx, token in enumerate(self.special_tokens)}\n",
        "        self.id2token = {idx: token for idx, token in enumerate(self.special_tokens)}\n",
        "\n",
        "    def preprocess_text(self, text: str) -> List[str]:\n",
        "        tokens = re.findall(r\"\\w+[\\w']*|['’][a-z]+|[^\\w\\s]\", text.lower())\n",
        "        return tokens\n",
        "\n",
        "    def get_stats(self, examples: List[str]) -> Counter:\n",
        "        counter = Counter()\n",
        "        for text in examples:\n",
        "            tokens = self.preprocess_text(text)\n",
        "            counter.update(tokens)\n",
        "        return counter"
      ],
      "metadata": {
        "trusted": true,
        "id": "NDbiveiSPiM5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "en_tokenizer = BaseTokenizer(language='en', vocab_size=32000)\n",
        "ru_tokenizer = BaseTokenizer(language='ru', vocab_size=32000)"
      ],
      "metadata": {
        "trusted": true,
        "id": "JRE-4T9TPiM5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Давайте напишем функцию `analyze_token_statistics`, чтобы подсчитать, какие у нас получились уникальные токены и как часто они встречаются."
      ],
      "metadata": {
        "id": "77t5WUPFPiM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_token_statistics(dataset, tokenizer: BaseTokenizer, n_samples: int = 1000):\n",
        "    samples = dataset['train'].select(range(n_samples))\n",
        "    texts = [s['translation'][tokenizer.language] for s in samples]\n",
        "\n",
        "    stats = tokenizer.get_stats(texts)\n",
        "\n",
        "    print(f\"\\nToken statistics for {tokenizer.language}:\")\n",
        "    print(f\"Total unique tokens: {len(stats)}\")\n",
        "    print(\"\\nTop 10 most frequent tokens:\")\n",
        "    for token, count in stats.most_common(10):\n",
        "        print(f\"{token}: {count}\")\n",
        "\n",
        "    return stats"
      ],
      "metadata": {
        "trusted": true,
        "id": "TrvpnUSEPiM6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "en_stats = analyze_token_statistics(dataset, en_tokenizer)\n",
        "ru_stats = analyze_token_statistics(dataset, ru_tokenizer)"
      ],
      "metadata": {
        "trusted": true,
        "id": "UTALEjYbPiM6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Разница в количестве токенов для английского (1337) и русского (2065) объясняется особенностями языков: русский имеет более богатую морфологию (окончания, приставки) и больше форм слов. Доминирование пунктуации (. и , в топе) вообще-то указывает на необходимость их предварительной фильтрации или отдельной обработки.\n",
        "\n",
        "Интересно, что токен \" (кавычки) встречается чаще в английском (146 раз) — это может быть связано с особенностями перевода в датасете Tatoeba.\n",
        "\n",
        "Важно помнить, что такой подход не разбивает слова на субсловные единицы, поэтому редкие слова остаются целыми, увеличивая размер словаря. Для сравнения, BPE-токенизатор и это будет показано в следующих экспериментах"
      ],
      "metadata": {
        "id": "g-_S2wPCPiM6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PBE токенайзер\n",
        "\n",
        "BPE (Byte Pair Encoding) — это алгоритм для создания подсловных токенов. Он помогает решить проблемы с редкими словами, обеспечивая более компактное представление текста. В отличие от простого токенизатора, который работает на уровне слов, BPE создаёт более мелкие единицы — подслова или даже отдельные символы.\n",
        "\n",
        "Алгоритм следующий:\n",
        "\n",
        "1. **Инициализация**: Токенизатор начинается с создания списка символов и специальных токенов, таких как `<PAD>`, `<UNK>`, `<BOS>`, `<EOS>`, которые добавляются в словарь. Начальный словарь включает в себя все символы из текста, а также все уникальные токены.\n",
        "\n",
        "2. **Обучение**: На основе корпуса текстов токенизатор создаёт статистику для каждого слова, представляя его как последовательность символов (например, буквы, цифры и знаки препинания). Частоты появления этих \"символьных слов\" подсчитываются, и для каждой пары символов (например, \"l\" и \"o\") вычисляется, насколько часто они встречаются рядом. Это важно для выделения наиболее часто встречающихся последовательностей символов.\n",
        "\n",
        "3. **Слияние пар символов**: На каждом шаге токенизатор ищет наиболее часто встречающуюся пару символов, например, \"l\" и \"o\" в слове \"hello\". Эта пара символов заменяется новым токеном, который объединяет два символа в один. Этот процесс повторяется несколько раз (настраивается параметром `num_merges`), каждый раз добавляя новые токены в словарь и обновляя частотные характеристики текста.\n",
        "\n",
        "4. **Создание новых токенов**: После каждого слияния обновляется список токенов, а словарь обновляется новыми подсловами. Новый токен становится частью словаря и используется для дальнейшего слияния пар символов. Важно, что новые токены могут быть подсловами или даже словами."
      ],
      "metadata": {
        "id": "trihl9csPiM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class BPETokenizer(BaseTokenizer):\n",
        "    def __post_init__(self):\n",
        "        super().__post_init__()\n",
        "        self.merges = {}\n",
        "        self.vocab = set(self.special_tokens)\n",
        "\n",
        "    def get_pairs(self, word: List[str]) -> List[Tuple[str, str]]:\n",
        "        return [(word[i], word[i+1]) for i in range(len(word)-1)]\n",
        "\n",
        "    def train(self, texts: List[str], num_merges: int):\n",
        "        word_freqs = defaultdict(int)\n",
        "        all_chars = set()\n",
        "\n",
        "        for text in texts:\n",
        "            tokens = self.preprocess_text(text)\n",
        "            for token in tokens:\n",
        "                chars = list(token)\n",
        "                word_freqs[' '.join(chars)] += 1\n",
        "                all_chars.update(chars)\n",
        "\n",
        "        for char in sorted(all_chars):\n",
        "            if char not in self.token2id:\n",
        "                idx = len(self.token2id)\n",
        "                self.token2id[char] = idx\n",
        "                self.id2token[idx] = char\n",
        "                self.vocab.add(char)\n",
        "\n",
        "        word_freqs = defaultdict(int)\n",
        "        for text in texts:\n",
        "            tokens = self.preprocess_text(text)\n",
        "            for token in tokens:\n",
        "                chars = list(token)\n",
        "                word = ' '.join(chars)\n",
        "                word_freqs[word] += 1\n",
        "\n",
        "        print(f\"Training BPE tokenizer for {self.language}...\")\n",
        "        for i in tqdm(range(num_merges)):\n",
        "            pair_freqs = defaultdict(int)\n",
        "\n",
        "            for word, freq in word_freqs.items():\n",
        "                symbols = word.split()\n",
        "                pairs = self.get_pairs(symbols)\n",
        "                for pair in pairs:\n",
        "                    pair_freqs[pair] += freq\n",
        "\n",
        "            if not pair_freqs:\n",
        "                break\n",
        "\n",
        "            best_pair = max(pair_freqs.items(), key=lambda x: x[1])[0]\n",
        "            new_token = ''.join(best_pair)\n",
        "\n",
        "            self.merges[best_pair] = new_token\n",
        "            self.vocab.add(new_token)\n",
        "\n",
        "            if new_token not in self.token2id:\n",
        "                idx = len(self.token2id)\n",
        "                self.token2id[new_token] = idx\n",
        "                self.id2token[idx] = new_token\n",
        "\n",
        "            new_word_freqs = defaultdict(int)\n",
        "            for word, freq in word_freqs.items():\n",
        "                symbols = word.split()\n",
        "                i = 0\n",
        "                new_symbols = []\n",
        "                while i < len(symbols):\n",
        "                    if i < len(symbols)-1 and (symbols[i], symbols[i+1]) == best_pair:\n",
        "                        new_symbols.append(new_token)\n",
        "                        i += 2\n",
        "                    else:\n",
        "                        new_symbols.append(symbols[i])\n",
        "                        i += 1\n",
        "                new_word = ' '.join(new_symbols)\n",
        "                new_word_freqs[new_word] += freq\n",
        "\n",
        "            word_freqs = new_word_freqs\n",
        "\n",
        "            if (i + 1) % 1000 == 0:\n",
        "                print(f\"Merges completed: {i+1}/{num_merges}\")\n",
        "                print(f\"Current vocabulary size: {len(self.token2id)}\")\n",
        "\n",
        "    def tokenize(self, text: str) -> List[int]:\n",
        "        tokens = self.preprocess_text(text)\n",
        "        result = [self.token2id['<BOS>']]\n",
        "\n",
        "        for token in tokens:\n",
        "            symbols = list(token)\n",
        "\n",
        "            while len(symbols) > 1:\n",
        "                pairs = self.get_pairs(symbols)\n",
        "                pair_to_merge = None\n",
        "                for pair in pairs:\n",
        "                    if pair in self.merges:\n",
        "                        pair_to_merge = pair\n",
        "                        break\n",
        "                if not pair_to_merge:\n",
        "                    break\n",
        "\n",
        "                i = 0\n",
        "                new_symbols = []\n",
        "                while i < len(symbols):\n",
        "                    if i < len(symbols)-1 and (symbols[i], symbols[i+1]) == pair_to_merge:\n",
        "                        new_symbols.append(self.merges[pair_to_merge])\n",
        "                        i += 2\n",
        "                    else:\n",
        "                        new_symbols.append(symbols[i])\n",
        "                        i += 1\n",
        "                symbols = new_symbols\n",
        "\n",
        "            for symbol in symbols:\n",
        "                if symbol in self.token2id:\n",
        "                    result.append(self.token2id[symbol])\n",
        "                else:\n",
        "                    result.append(self.token2id['<UNK>'])\n",
        "\n",
        "        result.append(self.token2id['<EOS>'])\n",
        "        return result"
      ],
      "metadata": {
        "trusted": true,
        "id": "sOuTedo5PiM6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Когда необходимо применить токенизатор для преобразования текста в последовательность токенов, алгоритм сначала разбивает текст на базовые символы, затем повторно сливает пары символов, используя уже построенный словарь слияний. Каждое слово в тексте представляется как последовательность подслов (или токенов), которые были созданы на этапе обучения.\n",
        "\n",
        "Количество слияний (параметр `num_merges`) определяет, сколько раз алгоритм будет объединять символы в новые токены. Чем больше количество слияний, тем более крупные и информативные токены могут быть созданы. Однако важно, что слишком большое количество слияний может привести к потере мелких деталей в тексте.\n",
        "\n",
        "Этот алгоритм хорошо работает с большими текстовыми корпусами и помогает моделям лучше справляться с редкими или незнакомыми словами, заменяя их подсловами из более частых комбинаций символов. Кроме того, BPE поддерживает работу с любыми языками, даже если они используют необычные или сложные алфавиты, так как начинает с базовых символов."
      ],
      "metadata": {
        "id": "kJKYW3ZAPiM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "en_bpe = BPETokenizer(language='en', vocab_size=32000)\n",
        "ru_bpe = BPETokenizer(language='ru', vocab_size=32000)\n",
        "\n",
        "n_samples = 80000\n",
        "train_samples = dataset['train'].select(range(n_samples))\n",
        "en_texts = [s['translation']['en'] for s in train_samples]\n",
        "ru_texts = [s['translation']['ru'] for s in train_samples]\n",
        "\n",
        "en_bpe.train(en_texts, num_merges=3000)\n",
        "ru_bpe.train(ru_texts, num_merges=3000)\n",
        "\n",
        "print(f\"English vocabulary size: {len(en_bpe.token2id)}\")\n",
        "print(f\"Russian vocabulary size: {len(ru_bpe.token2id)}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "QbnwBjXqPiM7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def test_tokenization(text: str, tokenizer: BPETokenizer):\n",
        "    print(f\"\\nOriginal text: {text}\")\n",
        "\n",
        "    token_ids = tokenizer.tokenize(text)\n",
        "    print(f\"Token IDs: {token_ids}\")\n",
        "\n",
        "    tokens = [tokenizer.id2token[id] for id in token_ids]\n",
        "    print(f\"Tokens: {tokens}\")\n",
        "\n",
        "    return token_ids"
      ],
      "metadata": {
        "trusted": true,
        "id": "1gzbL9djPiM7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "en_sample = dataset['train'][0]['translation']['en']\n",
        "ru_sample = dataset['train'][0]['translation']['ru']\n",
        "\n",
        "print(\"English tokenization:\")\n",
        "en_tokens = test_tokenization(en_sample, en_bpe)\n",
        "\n",
        "print(\"\\nRussian tokenization:\")\n",
        "ru_tokens = test_tokenization(ru_sample, ru_bpe)"
      ],
      "metadata": {
        "trusted": true,
        "id": "OHpzi_HwPiM7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "В общем, BPE эффективно решает проблему редких и сложных слов, улучшая качество токенизации и производительность NLP-моделей.\n",
        "\n",
        "Однако даже после обучения заметны артефакты. Например, слово \"useless\" разбивается на [\"us\", \"el\", \"ess\"], а \"бесполезно\" — на [\"бес\", \"пол\", \"ез\", \"но\"]. Это следствие ограниченного числа слияний и отсутствия явного учета морфемных границ в нашей учебной реализации.\n",
        "\n",
        "В готовых токенизаторах (например, от Hugging Face) такие проблемы смягчаются за счет предобучения на огромных корпусах и десятков тысяч слияний."
      ],
      "metadata": {
        "id": "s0bbClqiPiM7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Подготовка батчей\n",
        "\n",
        "Функция prepare_batch преобразует токенизированные последовательности в тензоры, пригодные для обучения.\n",
        "\n",
        "Каждое предложение дополняется до фиксированной длины (max_length=64) специальным токеном <PAD>, а маски внимания указывают модели игнорировать эти \"пустые\" позиции.\n",
        "\n",
        "Например, предложение из 24 токенов превращается в вектор длины 64, где 40 последних элементов — нули (ID <PAD>). Маскирование критично для трансформеров, так как механизм внимания иначе будет учитывать бессмысленные паддинг-токены, искажая веса."
      ],
      "metadata": {
        "id": "S2bxv2tiPiM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_batch(batch: List[Dict],\n",
        "                 src_tokenizer: BPETokenizer,\n",
        "                 tgt_tokenizer: BPETokenizer,\n",
        "                 max_length: int):\n",
        "\n",
        "    src_texts = [item['translation']['en'] for item in batch]\n",
        "    tgt_texts = [item['translation']['ru'] for item in batch]\n",
        "\n",
        "    src_tokens = [src_tokenizer.tokenize(text) for text in src_texts]\n",
        "    tgt_tokens = [tgt_tokenizer.tokenize(text) for text in tgt_texts]\n",
        "\n",
        "    src_padded = []\n",
        "    tgt_padded = []\n",
        "    src_masks = []\n",
        "    tgt_masks = []\n",
        "\n",
        "    for src, tgt in zip(src_tokens, tgt_tokens):\n",
        "        if len(src) > max_length:\n",
        "            src_pad = src[:max_length]\n",
        "            src_mask = [1] * max_length\n",
        "        else:\n",
        "            src_pad = src + [src_tokenizer.token2id['<PAD>']] * (max_length - len(src))\n",
        "            src_mask = [1] * len(src) + [0] * (max_length - len(src))\n",
        "\n",
        "        if len(tgt) > max_length:\n",
        "            tgt_pad = tgt[:max_length]\n",
        "            tgt_mask = [1] * max_length\n",
        "        else:\n",
        "            tgt_pad = tgt + [tgt_tokenizer.token2id['<PAD>']] * (max_length - len(tgt))\n",
        "            tgt_mask = [1] * len(tgt) + [0] * (max_length - len(tgt))\n",
        "\n",
        "        src_padded.append(src_pad)\n",
        "        tgt_padded.append(tgt_pad)\n",
        "        src_masks.append(src_mask)\n",
        "        tgt_masks.append(tgt_mask)\n",
        "\n",
        "    return {\n",
        "        'src_tokens': np.array(src_padded),\n",
        "        'tgt_tokens': np.array(tgt_padded),\n",
        "        'src_mask': np.array(src_masks),\n",
        "        'tgt_mask': np.array(tgt_masks)\n",
        "    }\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "xA-SulJBPiM8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test_samples = dataset['train'].select(range(5))\n",
        "prepared_data = prepare_batch(test_samples, en_bpe, ru_bpe, max_length=64)\n",
        "\n",
        "print(\"Prepared batch shapes:\")\n",
        "for key, value in prepared_data.items():\n",
        "    print(f\"{key}: {value.shape}\")\n",
        "\n",
        "print(\"\\nExample source tokens:\")\n",
        "print(prepared_data['src_tokens'][0])\n",
        "print(\"\\nCorresponding mask:\")\n",
        "print(prepared_data['src_mask'][0])"
      ],
      "metadata": {
        "trusted": true,
        "id": "29JCs70wPiM8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_bpe_tokenization(tokenizer: BPETokenizer, text: str):\n",
        "    print(f\"Original text: {text}\")\n",
        "\n",
        "    base_tokens = tokenizer.preprocess_text(text)\n",
        "    print(f\"\\nBase tokenization: {base_tokens}\")\n",
        "\n",
        "    print(f\"\\nNumber of merges learned: {len(tokenizer.merges)}\")\n",
        "    print(\"Sample merges (first 5):\")\n",
        "    for pair, merged in list(tokenizer.merges.items())[:5]:\n",
        "        print(f\"{pair} -> {merged}\")\n",
        "\n",
        "    print(f\"\\nVocabulary size: {len(tokenizer.token2id)}\")\n",
        "    print(\"Sample vocabulary items (first 10):\")\n",
        "    for token, idx in list(tokenizer.token2id.items())[:10]:\n",
        "        print(f\"{token}: {idx}\")\n",
        "\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    decoded = [tokenizer.id2token[id] for id in tokens]\n",
        "\n",
        "    print(f\"\\nFinal tokenization:\")\n",
        "    print(f\"Token IDs: {tokens}\")\n",
        "    print(f\"Decoded tokens: {decoded}\")\n",
        "\n",
        "print(\"Testing English tokenizer:\")\n",
        "verify_bpe_tokenization(en_bpe, dataset['train'][0]['translation']['en'])"
      ],
      "metadata": {
        "trusted": true,
        "id": "N7I1Cly7PiM8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hugging Face токенизаторы\n",
        "\n",
        "Использование готового токенизатора через AutoTokenizer демонстрирует преимущества стандартизированных инструментов.\n",
        "\n",
        "Модель opus-mt-en-ru использует предобученный BPE-словарь, оптимизированный для пары языков. Токенизатор автоматически добавляет служебные токены, обрабатывает регистр и редкие символы.\n",
        "\n",
        "При обработке датасета функция map применяет токенизацию параллельно ко всем примерам, что ускоряет работу за счет батчинга."
      ],
      "metadata": {
        "id": "wrVNFiJpPiM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "def prepare_data_with_hf(\n",
        "    dataset,\n",
        "    model_name: str = \"Helsinki-NLP/opus-mt-en-ru\",\n",
        "    max_length: int = 128,\n",
        "    batch_size: int = 32\n",
        "):\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    def preprocess_function(examples):\n",
        "        source_texts = [item['en'] for item in examples['translation']]\n",
        "        target_texts = [item['ru'] for item in examples['translation']]\n",
        "\n",
        "        source_encoding = tokenizer(\n",
        "            source_texts,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "            return_tensors='np'\n",
        "        )\n",
        "\n",
        "        target_encoding = tokenizer(\n",
        "            target_texts,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "            return_tensors='np'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': source_encoding['input_ids'],\n",
        "            'attention_mask': source_encoding['attention_mask'],\n",
        "            'labels': target_encoding['input_ids'],\n",
        "            'decoder_attention_mask': target_encoding['attention_mask']\n",
        "        }\n",
        "\n",
        "    processed_dataset = dataset['train'].map(\n",
        "        preprocess_function,\n",
        "        batched=True,\n",
        "        batch_size=batch_size,\n",
        "        remove_columns=dataset['train'].column_names\n",
        "    )\n",
        "\n",
        "    return processed_dataset, tokenizer"
      ],
      "metadata": {
        "trusted": true,
        "id": "maoQ0plbPiM8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "processed_data, hf_tokenizer = prepare_data_with_hf(dataset)"
      ],
      "metadata": {
        "trusted": true,
        "id": "YJ2wr2VdPiM8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "C8be0DkEPiM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_custom_bpe_data_shape(prepared_data):\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Custom BPE Tokenizer Data Structure:\")\n",
        "    print(\"Shape of prepared batches:\")\n",
        "    for key, array in prepared_data.items():\n",
        "        print(f\"{key}: {array.shape} (dtype: {array.dtype})\")\n",
        "\n",
        "    print(\"\\nSample data from first batch:\")\n",
        "    print(\"Source tokens (first example):\")\n",
        "    print(prepared_data['src_tokens'][0])\n",
        "    print(\"\\nTarget tokens (first example):\")\n",
        "    print(prepared_data['tgt_tokens'][0])\n",
        "    print(\"\\nSource mask (first example):\")\n",
        "    print(prepared_data['src_mask'][0])\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "def print_hf_data_details(processed_dataset, tokenizer):\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Hugging Face Tokenizer Data Structure:\")\n",
        "    print(f\"Dataset features: {processed_dataset.features}\")\n",
        "    print(f\"Number of examples: {len(processed_dataset)}\")\n",
        "\n",
        "    first_example = processed_dataset[0]\n",
        "    print(\"\\nFirst example details:\")\n",
        "    print(\"Input IDs shape:\", len(first_example['input_ids']))\n",
        "    print(\"Decoded input:\", tokenizer.decode(first_example['input_ids'], skip_special_tokens=True))\n",
        "    print(\"Labels shape:\", len(first_example['labels']))\n",
        "    print(\"Decoded labels:\", tokenizer.decode(first_example['labels'], skip_special_tokens=True))\n",
        "    print(\"Attention mask sample:\", first_example['attention_mask'][:10])\n",
        "    print(\"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "s32HN-C4PiM9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test_samples = dataset['train'].select(range(5))\n",
        "prepared_data = prepare_batch(test_samples, en_bpe, ru_bpe, max_length=64)\n",
        "print_custom_bpe_data_shape(prepared_data)\n",
        "\n",
        "\n",
        "processed_data, hf_tokenizer = prepare_data_with_hf(dataset)\n",
        "print_hf_data_details(processed_data, hf_tokenizer)"
      ],
      "metadata": {
        "trusted": true,
        "id": "FqzhaGxzPiM9"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}