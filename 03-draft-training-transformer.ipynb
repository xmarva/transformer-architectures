{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Загрузка данных","metadata":{}},{"cell_type":"code","source":"import re\nimport spacy\nimport numpy as np\nimport pandas as pd\nfrom datasets import load_dataset\n\nfrom dataclasses import dataclass\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom itertools import chain\nfrom typing import List, Dict, Tuple\nfrom collections import Counter, defaultdict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T21:08:56.038059Z","iopub.execute_input":"2025-02-16T21:08:56.038270Z","iopub.status.idle":"2025-02-16T21:09:04.990025Z","shell.execute_reply.started":"2025-02-16T21:08:56.038250Z","shell.execute_reply":"2025-02-16T21:09:04.989368Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"def load_translation_dataset():\n    print(\"Loading Tatoeba en-ru...\")\n    try:\n        dataset = load_dataset(\"Helsinki-NLP/tatoeba\", lang1=\"en\", lang2=\"ru\", trust_remote_code=True)\n        \n    except Exception as e:\n        print(f\"Error while loading dataset: {e}\")\n        raise\n    \n    print(\"\\nDataset structure:\")\n    print(dataset)\n    \n    print(\"\\nData sample:\")\n    for i in range(2):\n        print(f\"EN: {dataset['train'][i]['translation']['en']}\")\n        print(f\"RU: {dataset['train'][i]['translation']['ru']}\\n\")\n\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T21:09:04.990812Z","iopub.execute_input":"2025-02-16T21:09:04.991201Z","iopub.status.idle":"2025-02-16T21:09:04.995840Z","shell.execute_reply.started":"2025-02-16T21:09:04.991177Z","shell.execute_reply":"2025-02-16T21:09:04.994953Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"dataset = load_translation_dataset()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T21:09:04.996897Z","iopub.execute_input":"2025-02-16T21:09:04.997277Z","iopub.status.idle":"2025-02-16T21:09:22.014725Z","shell.execute_reply.started":"2025-02-16T21:09:04.997237Z","shell.execute_reply":"2025-02-16T21:09:22.013915Z"}},"outputs":[{"name":"stdout","text":"Loading Tatoeba en-ru...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/8.93k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ffc31b6d91747cbbedeb237616868e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tatoeba.py:   0%|          | 0.00/4.41k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c1ce39cf6254dcca76daf97871d9be9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/14.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72ee4cdd10644a7db66ca9100e582659"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a905f0c6c3d43c6b2292f074025a728"}},"metadata":{}},{"name":"stdout","text":"\nDataset structure:\nDatasetDict({\n    train: Dataset({\n        features: ['id', 'translation'],\n        num_rows: 523656\n    })\n})\n\nData sample:\nEN: For once in my life I'm doing a good deed... And it is useless.\nRU: Один раз в жизни я делаю хорошее дело... И оно бесполезно.\n\nEN: Let's try something.\nRU: Давайте что-нибудь попробуем!\n\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ndef prepare_data_with_hf(\n    dataset,\n    model_name: str = \"Helsinki-NLP/opus-mt-en-ru\",\n    max_length: int = 128,\n    batch_size: int = 32\n):\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    \n    # Убедимся, что все специальные токены определены\n    special_tokens = {\n        'bos_token': '<s>',\n        'eos_token': '</s>',\n        'pad_token': '<pad>',\n        'unk_token': '<unk>'\n    }\n    \n    # Добавляем отсутствующие токены\n    tokenizer.add_special_tokens(special_tokens)\n    \n    def preprocess_function(examples):\n        source_texts = [item['en'] for item in examples['translation']]\n        \n        # Убедимся, что целевые тексты начинаются с BOS и заканчиваются EOS\n        target_texts = [\n            f\"{item['ru']}\"  # Убираем явное добавление BOS/EOS\n            for item in examples['translation']\n        ]\n        \n        # Токенизация исходных текстов\n        source_encoding = tokenizer(\n            source_texts,\n            padding='max_length',\n            truncation=True,\n            max_length=max_length,\n            return_tensors='np'\n        )\n        \n        # Токенизация целевых текстов\n        target_encoding = tokenizer(\n            target_texts,\n            padding='max_length',\n            truncation=True,\n            max_length=max_length,\n            return_tensors='np'\n        )\n        \n        return {\n            'input_ids': source_encoding['input_ids'],\n            'attention_mask': source_encoding['attention_mask'],\n            'labels': target_encoding['input_ids'],\n            'decoder_attention_mask': target_encoding['attention_mask']\n        }\n    \n    processed_dataset = dataset['train'].map(\n        preprocess_function,\n        batched=True,\n        batch_size=batch_size,\n        remove_columns=dataset['train'].column_names\n    )\n    \n    return processed_dataset, tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T21:09:22.015534Z","iopub.execute_input":"2025-02-16T21:09:22.015788Z","iopub.status.idle":"2025-02-16T21:09:27.570360Z","shell.execute_reply.started":"2025-02-16T21:09:22.015765Z","shell.execute_reply":"2025-02-16T21:09:27.569712Z"}},"outputs":[{"name":"stderr","text":"The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"552849705a484c8c986619e52c06f442"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"processed_data, hf_tokenizer = prepare_data_with_hf(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T21:09:27.571220Z","iopub.execute_input":"2025-02-16T21:09:27.571784Z","iopub.status.idle":"2025-02-16T21:12:16.426002Z","shell.execute_reply.started":"2025-02-16T21:09:27.571757Z","shell.execute_reply":"2025-02-16T21:12:16.424876Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c83bd938a714099ae0038d99fb49f89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"596629fe96ee4cdda006986d1cc4c9e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"source.spm:   0%|          | 0.00/803k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de65f780e3664f42a7bb0b16b71d009d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"target.spm:   0%|          | 0.00/1.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19a201e403554744bff7c28be2196779"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.60M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9638acf4df4e4258b4f3e589ba4b066d"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n  warnings.warn(\"Recommended: pip install sacremoses.\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/523656 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b860a05fe374edc895689761a68bd6a"}},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"## Архитектура модели","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install -q torchdata==0.3.0 torchtext==0.12 spacy==3.2 altair GPUtil\n!python -m spacy download de_core_news_sm\n!python -m spacy download en_core_web_sm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T21:12:16.427955Z","iopub.execute_input":"2025-02-16T21:12:16.428200Z","iopub.status.idle":"2025-02-16T21:13:57.343888Z","shell.execute_reply.started":"2025-02-16T21:12:16.428177Z","shell.execute_reply":"2025-02-16T21:13:57.342750Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import os\nimport time\nimport math\nimport copy\nimport spacy\nimport GPUtil\nimport pandas as pd\nfrom typing import *\nfrom itertools import chain\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.optim.lr_scheduler import LambdaLR\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data import DataLoader, Dataset\n\nimport altair as alt\nfrom altair import Chart\n\nalt.data_transformers.disable_max_rows()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T21:13:57.345602Z","iopub.execute_input":"2025-02-16T21:13:57.345882Z","iopub.status.idle":"2025-02-16T21:13:57.619967Z","shell.execute_reply.started":"2025-02-16T21:13:57.345857Z","shell.execute_reply":"2025-02-16T21:13:57.619065Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"DataTransformerRegistry.enable('default')"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout, max_len=5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        \n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n                             (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:, :x.size(1)].detach()\n        return self.dropout(x)\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n        \n        assert self.head_dim * num_heads == d_model, \"d_model must be divisible by num_heads\"\n        \n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n        \n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        # Q: [batch_size, num_heads, seq_len, head_dim]\n        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n        \n        if mask is not None:\n            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n        \n        attn_probs = F.softmax(attn_scores, dim=-1)\n        output = torch.matmul(attn_probs, V)\n        return output\n        \n    def forward(self, Q, K, V, mask=None):\n        batch_size = Q.size(0)\n        \n        # Линейные преобразования\n        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        \n        # Вычисление внимания\n        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n        \n        # Объединение голов\n        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n        \n        # Финальное линейное преобразование\n        output = self.W_o(attn_output)\n        return output\n\nclass FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff=2048):\n        super().__init__()\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.linear2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(0.1)\n        \n    def forward(self, x):\n        x = self.dropout(F.relu(self.linear1(x)))\n        x = self.linear2(x)\n        return x\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.ffn = FeedForward(d_model)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(0.1)\n        \n    def forward(self, x, mask=None):\n        # Self attention\n        attn_output = self.self_attn(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        \n        # Feed forward\n        ffn_output = self.ffn(x)\n        x = self.norm2(x + self.dropout(ffn_output))\n        return x\n\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n        self.ffn = FeedForward(d_model)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(0.1)\n        \n    def forward(self, x, enc_output, src_mask, tgt_mask):\n        # Self attention (маскированное)\n        attn_output = self.self_attn(x, x, x, tgt_mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        \n        # Cross attention (с выходом энкодера)\n        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n        x = self.norm2(x + self.dropout(attn_output))\n        \n        # Feed forward\n        ffn_output = self.ffn(x)\n        x = self.norm3(x + self.dropout(ffn_output))\n        return x\n\nclass Transformer(nn.Module):\n    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, num_heads=8, num_layers=6):\n        super().__init__()\n        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n        self.positional_encoding = PositionalEncoding(d_model, dropout=0.1)\n        \n        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads) for _ in range(num_layers)])\n        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads) for _ in range(num_layers)])\n        \n        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n            \n    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n        # Проверка размерностей входных данных\n        batch_size = src.size(0)\n        src_seq_len = src.size(1)\n        tgt_seq_len = tgt.size(1)\n        \n        # Энкодинг\n        src_emb = self.positional_encoding(self.encoder_embedding(src))\n        enc_output = src_emb\n        \n        for layer in self.encoder_layers:\n            enc_output = layer(enc_output, src_mask)\n        \n        # Декодинг\n        tgt_emb = self.positional_encoding(self.decoder_embedding(tgt))\n        dec_output = tgt_emb\n        \n        for layer in self.decoder_layers:\n            dec_output = layer(dec_output, enc_output, src_mask, tgt_mask)\n        \n        # Финальный слой\n        output = self.fc_out(dec_output)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T21:13:57.621027Z","iopub.execute_input":"2025-02-16T21:13:57.621412Z","iopub.status.idle":"2025-02-16T21:13:57.640942Z","shell.execute_reply.started":"2025-02-16T21:13:57.621376Z","shell.execute_reply":"2025-02-16T21:13:57.639978Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def test_transformer():\n    # Проверяем доступность CUDA и инициализируем устройство\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    # Установка seed для воспроизводимости\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(42)\n    torch.manual_seed(42)\n    \n    # Конфигурация\n    batch_size = 2\n    seq_len = 10\n    d_model = 512\n    num_heads = 8\n    src_vocab_size = 100\n    tgt_vocab_size = 100\n    num_layers = 2\n\n    # Генерация синтетических данных\n    src = torch.randint(0, src_vocab_size, (batch_size, seq_len)).to(device)\n    tgt = torch.randint(0, tgt_vocab_size, (batch_size, seq_len)).to(device)\n    \n    # Генерация масок\n    src_mask = torch.ones(batch_size, 1, 1, seq_len).to(device)\n    tgt_mask = torch.tril(torch.ones(seq_len, seq_len)).expand(batch_size, 1, seq_len, seq_len).to(device)\n\n    # Инициализация модели\n    transformer = Transformer(\n        src_vocab_size=src_vocab_size,\n        tgt_vocab_size=tgt_vocab_size,\n        d_model=d_model,\n        num_heads=num_heads,\n        num_layers=num_layers\n    ).to(device)\n\n    print(\"=\"*50)\n    print(\"1. Тест Positional Encoding\")\n    pe = PositionalEncoding(d_model, dropout=0.1).to(device)\n    x = torch.randn(1, seq_len, d_model).to(device)\n    print(f\"До PE: mean={x.mean().item():.4f}, std={x.std().item():.4f}\")\n    x_pe = pe(x)\n    print(f\"После PE: mean={x_pe.mean().item():.4f}, std={x_pe.std().item():.4f}\")\n    print(f\"Форма PE: {x_pe.shape} (должна быть [1, {seq_len}, {d_model}])\")\n    \n    print(\"\\n2. Тест Multi-Head Attention\")\n    mha = MultiHeadAttention(d_model, num_heads).to(device)\n    q = k = v = torch.randn(batch_size, seq_len, d_model).to(device)\n    attn_output = mha(q, k, v)\n    print(f\"Форма выхода внимания: {attn_output.shape} (должна быть {q.shape})\")\n    print(f\"Максимальное значение: {attn_output.max().item():.4f}\")\n    print(f\"Минимальное значение: {attn_output.min().item():.4f}\")\n\n    print(\"\\n3. Тест Encoder Layer\")\n    encoder_layer = EncoderLayer(d_model, num_heads).to(device)\n    enc_input = torch.randn(batch_size, seq_len, d_model).to(device)\n    enc_output = encoder_layer(enc_input)\n    print(f\"Форма выхода энкодера: {enc_output.shape} (должна быть {enc_input.shape})\")\n    print(f\"Изменение данных: {torch.allclose(enc_input, enc_output, atol=1e-4)} (должно быть False)\")\n\n    print(\"\\n4. Тест Decoder Layer\")\n    decoder_layer = DecoderLayer(d_model, num_heads).to(device)\n    dec_input = torch.randn(batch_size, seq_len, d_model).to(device)\n    dec_output = decoder_layer(dec_input, enc_output, src_mask, tgt_mask)\n    print(f\"Форма выхода декодера: {dec_output.shape} (должна быть {dec_input.shape})\")\n    print(f\"Норма выходных данных: {dec_output.norm().item():.4f}\")\n\n    print(\"\\n5. Полный тест Transformer\")\n    print(\"Входные данные:\")\n    print(f\"src: {src.shape} (max={src.max().item()}, min={src.min().item()})\")\n    print(f\"tgt: {tgt.shape} (max={tgt.max().item()}, min={tgt.min().item()})\")\n    \n    output = transformer(src, tgt, src_mask, tgt_mask)\n    print(\"\\nПроверка формы выхода:\")\n    print(f\"Ожидаемая форма: ({batch_size}, {seq_len}, {tgt_vocab_size})\")\n    print(f\"Реальная форма:   {output.shape}\")\n    \n    print(\"\\nПроверка градиентов:\")\n    dummy_loss = output.sum()\n    dummy_loss.backward()\n    has_gradients = any(p.grad is not None for p in transformer.parameters())\n    print(f\"Градиенты вычислены: {has_gradients} (должно быть True)\")\n\n    print(\"\\n6. Проверка параметров модели:\")\n    total_params = sum(p.numel() for p in transformer.parameters())\n    print(f\"Всего параметров: {total_params}\")\n    print(f\"Параметры энкодера: {sum(p.numel() for p in transformer.encoder_embedding.parameters())}\")\n    print(f\"Параметры декодера: {sum(p.numel() for p in transformer.decoder_embedding.parameters())}\")\n\n    print(\"\\nТест завершен!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T21:13:57.641752Z","iopub.execute_input":"2025-02-16T21:13:57.641980Z","iopub.status.idle":"2025-02-16T21:13:57.657062Z","shell.execute_reply.started":"2025-02-16T21:13:57.641960Z","shell.execute_reply":"2025-02-16T21:13:57.656343Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"test_transformer()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T21:13:57.658084Z","iopub.execute_input":"2025-02-16T21:13:57.658345Z","iopub.status.idle":"2025-02-16T21:13:58.558946Z","shell.execute_reply.started":"2025-02-16T21:13:57.658323Z","shell.execute_reply":"2025-02-16T21:13:58.558123Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n==================================================\n1. Тест Positional Encoding\nДо PE: mean=-0.0316, std=1.0065\nПосле PE: mean=0.4401, std=1.2013\nФорма PE: torch.Size([1, 10, 512]) (должна быть [1, 10, 512])\n\n2. Тест Multi-Head Attention\nФорма выхода внимания: torch.Size([2, 10, 512]) (должна быть torch.Size([2, 10, 512]))\nМаксимальное значение: 0.3949\nМинимальное значение: -0.4440\n\n3. Тест Encoder Layer\nФорма выхода энкодера: torch.Size([2, 10, 512]) (должна быть torch.Size([2, 10, 512]))\nИзменение данных: False (должно быть False)\n\n4. Тест Decoder Layer\nФорма выхода декодера: torch.Size([2, 10, 512]) (должна быть torch.Size([2, 10, 512]))\nНорма выходных данных: 101.1924\n\n5. Полный тест Transformer\nВходные данные:\nsrc: torch.Size([2, 10]) (max=95, min=6)\ntgt: torch.Size([2, 10]) (max=99, min=10)\n\nПроверка формы выхода:\nОжидаемая форма: (2, 10, 100)\nРеальная форма:   torch.Size([2, 10, 100])\n\nПроверка градиентов:\nГрадиенты вычислены: True (должно быть True)\n\n6. Проверка параметров модели:\nВсего параметров: 14866532\nПараметры энкодера: 51200\nПараметры декодера: 51200\n\nТест завершен!\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## Обучение","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import Adam\nfrom torch.nn.utils.rnn import pad_sequence\nimport numpy as np\nfrom tqdm.auto import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T21:13:58.560027Z","iopub.execute_input":"2025-02-16T21:13:58.560374Z","iopub.status.idle":"2025-02-16T21:13:58.564740Z","shell.execute_reply.started":"2025-02-16T21:13:58.560336Z","shell.execute_reply":"2025-02-16T21:13:58.563918Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class TranslationDataset(Dataset):\n    def __init__(self, processed_data):\n        self.input_ids = processed_data['input_ids']\n        self.attention_mask = processed_data['attention_mask']\n        self.labels = processed_data['labels']\n        self.decoder_attention_mask = processed_data['decoder_attention_mask']\n        \n    def __len__(self):\n        return len(self.input_ids)\n    \n    def __getitem__(self, idx):\n        return {\n            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n            'attention_mask': torch.tensor(self.attention_mask[idx], dtype=torch.long),\n            'labels': torch.tensor(self.labels[idx], dtype=torch.long),\n            'decoder_attention_mask': torch.tensor(self.decoder_attention_mask[idx], dtype=torch.long)\n        }\n\ndef create_masks(src_mask, tgt_mask):\n    src_mask = src_mask.unsqueeze(1).unsqueeze(2)\n    \n    device = tgt_mask.device\n    seq_len = tgt_mask.shape[1]\n    causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=device), diagonal=1).bool()\n    causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)  # [1, 1, seq_len, seq_len]\n    \n    tgt_mask = tgt_mask.unsqueeze(1).unsqueeze(2) & ~causal_mask\n    \n    return src_mask, tgt_mask\n\ndef train_epoch(model, dataloader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0\n    \n    progress_bar = tqdm(dataloader, desc='Training')\n    for batch_idx, batch in enumerate(progress_bar):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        decoder_attention_mask = batch['decoder_attention_mask'].to(device)\n        \n        # Вывод отладочной информации для первого батча\n        if batch_idx == 0:\n            print(\"\\nДиагностика первого батча:\")\n            print(f\"input_ids shape: {input_ids.shape}\")\n            print(f\"labels shape: {labels.shape}\")\n            print(f\"input_ids range: [{input_ids.min()}, {input_ids.max()}]\")\n            print(f\"labels range: [{labels.min()}, {labels.max()}]\")\n            print(f\"Vocab size: {model.fc_out.out_features}\")\n        \n        # Создаем маски\n        src_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n        tgt_mask = decoder_attention_mask[:, :-1].unsqueeze(1).unsqueeze(2)\n        \n        # Forward pass с проверкой размерностей\n        outputs = model(\n            input_ids,\n            labels[:, :-1],\n            src_mask,\n            tgt_mask\n        )\n        \n        if batch_idx == 0:\n            print(f\"\\nПосле forward pass:\")\n            print(f\"outputs shape before view: {outputs.shape}\")\n        \n        # Подготовка выходов и меток\n        outputs = outputs.contiguous().view(-1, outputs.size(-1))\n        target = labels[:, 1:].contiguous().view(-1)\n        \n        if batch_idx == 0:\n            print(f\"\\nПеред loss:\")\n            print(f\"outputs shape after view: {outputs.shape}\")\n            print(f\"target shape: {target.shape}\")\n            print(f\"target range: [{target.min()}, {target.max()}]\")\n            \n            # Проверка на некорректные индексы\n            invalid_indices = (target >= outputs.size(-1)).sum()\n            print(f\"Number of invalid indices in target: {invalid_indices}\")\n        \n        # Исключаем padding tokens\n        valid_targets = target != tokenizer.pad_token_id\n        outputs = outputs[valid_targets]\n        target = target[valid_targets]\n        \n        # Вычисляем loss\n        try:\n            loss = criterion(outputs, target)\n        except RuntimeError as e:\n            print(f\"\\nОшибка при вычислении loss:\")\n            print(f\"outputs shape: {outputs.shape}\")\n            print(f\"target shape: {target.shape}\")\n            print(f\"target unique values: {torch.unique(target)}\")\n            raise e\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n    \n    return total_loss / len(dataloader)\n\n\ndef translate_sentence(model, tokenizer, sentence, device, max_length=128):\n    model.eval()\n    \n    # Токенизация с учетом всех специальных токенов\n    inputs = tokenizer(sentence, \n                      return_tensors=\"pt\", \n                      max_length=max_length,\n                      padding='max_length',\n                      truncation=True)\n    \n    src = inputs['input_ids'].to(device)\n    src_mask = inputs['attention_mask'].to(device)\n    \n    # Инициализация декодера\n    decoder_input = torch.tensor([[tokenizer.bos_token_id]], dtype=torch.long, device=device)\n    \n    with torch.no_grad():\n        for i in range(max_length):\n            try:\n                # Создаем маски\n                src_mask_4d = src_mask.unsqueeze(1).unsqueeze(2)  # [1, 1, 1, src_len]\n                \n                # Каузальная маска для декодера\n                tgt_seq_len = decoder_input.size(1)\n                tgt_mask = torch.triu(torch.ones(tgt_seq_len, tgt_seq_len, device=device), diagonal=1).bool()\n                tgt_mask = tgt_mask.unsqueeze(0).unsqueeze(0)  # [1, 1, tgt_len, tgt_len]\n                \n                # Проверка размерностей перед forward pass\n                print(f\"Step {i}:\")\n                print(f\"src shape: {src.shape}\")  # Должно быть [1, seq_len]\n                print(f\"decoder_input shape: {decoder_input.shape}\")\n                print(f\"src_mask shape: {src_mask_4d.shape}\")\n                print(f\"tgt_mask shape: {tgt_mask.shape}\")\n                \n                # Forward pass\n                output = model(src, decoder_input, src_mask_4d, tgt_mask)\n                \n                # Получаем следующий токен\n                next_token = output[:, -1].argmax(-1).unsqueeze(1)\n                decoder_input = torch.cat([decoder_input, next_token], dim=1)\n                \n                if next_token.item() == tokenizer.eos_token_id:\n                    break\n                    \n            except Exception as e:\n                print(f\"Error at step {i}:\")\n                print(f\"Shapes - src: {src.shape}, decoder: {decoder_input.shape}\")\n                print(f\"Masks - src: {src_mask_4d.shape}, tgt: {tgt_mask.shape}\")\n                raise e\n                \n    return tokenizer.decode(decoder_input[0].cpu().numpy(), skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T21:30:40.666705Z","iopub.execute_input":"2025-02-16T21:30:40.667061Z","iopub.status.idle":"2025-02-16T21:30:40.684547Z","shell.execute_reply.started":"2025-02-16T21:30:40.667034Z","shell.execute_reply":"2025-02-16T21:30:40.683685Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"translation = translate_sentence(model, \n                                tokenizer, \n                                \"Hello, how are you?\", \n                                device='cuda')\nprint(translation)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T21:30:43.118408Z","iopub.execute_input":"2025-02-16T21:30:43.118758Z","iopub.status.idle":"2025-02-16T21:30:43.182750Z","shell.execute_reply.started":"2025-02-16T21:30:43.118733Z","shell.execute_reply":"2025-02-16T21:30:43.181446Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-0b957cf63f7e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m translation = translate_sentence(model, \n\u001b[0m\u001b[1;32m      2\u001b[0m                                 \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                 \u001b[0;34m\"Hello, how are you?\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                 device='cuda')\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-21-30667ff4b13b>\u001b[0m in \u001b[0;36mtranslate_sentence\u001b[0;34m(model, tokenizer, sentence, device, max_length)\u001b[0m\n\u001b[1;32m    114\u001b[0m                       truncation=True)\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0msrc_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"],"ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","output_type":"error"}],"execution_count":22},{"cell_type":"code","source":"BATCH_SIZE = 32\nEPOCHS = 5\nLEARNING_RATE = 0.0001\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nprocessed_data, tokenizer = prepare_data_with_hf(dataset)\ntrain_dataset = TranslationDataset(processed_data)\n\nprint(f\"BOS token: {tokenizer.bos_token} (id: {tokenizer.bos_token_id})\")\nprint(f\"EOS token: {tokenizer.eos_token} (id: {tokenizer.eos_token_id}\")\nprint(f\"PAD token: {tokenizer.pad_token} (id: {tokenizer.pad_token_id})\")\n\nsample = tokenizer.decode(train_dataset[0]['input_ids'])\nprint(\"\\nSample input:\", sample)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T21:13:58.751735Z","iopub.execute_input":"2025-02-16T21:13:58.752063Z","iopub.status.idle":"2025-02-16T21:16:05.446915Z","shell.execute_reply.started":"2025-02-16T21:13:58.752028Z","shell.execute_reply":"2025-02-16T21:16:05.446082Z"}},"outputs":[{"name":"stdout","text":"BOS token: <s> (id: 62518)\nEOS token: </s> (id: 0\nPAD token: <pad> (id: 62517)\n\nSample input: For once in my life I'm doing a good deed... And it is useless.</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"## Full dataset\n\"\"\"\ntrain_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\nmodel = Transformer(\n    src_vocab_size=tokenizer.vocab_size,\n    tgt_vocab_size=tokenizer.vocab_size,\n    d_model=512,\n    num_heads=8,\n    num_layers=6\n).to(DEVICE)\n\noptimizer = Adam(model.parameters(), lr=LEARNING_RATE)\ncriterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n\ntest_sentence = \"Hello, how are you today?\"\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T21:16:05.447786Z","iopub.execute_input":"2025-02-16T21:16:05.448123Z","iopub.status.idle":"2025-02-16T21:16:05.453286Z","shell.execute_reply.started":"2025-02-16T21:16:05.448087Z","shell.execute_reply":"2025-02-16T21:16:05.452537Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"'\\ntrain_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\\n\\nmodel = Transformer(\\n    src_vocab_size=tokenizer.vocab_size,\\n    tgt_vocab_size=tokenizer.vocab_size,\\n    d_model=512,\\n    num_heads=8,\\n    num_layers=6\\n).to(DEVICE)\\n\\noptimizer = Adam(model.parameters(), lr=LEARNING_RATE)\\ncriterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\\n\\ntest_sentence = \"Hello, how are you today?\"\\n'"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"## Subset\n\nSUBSET_SIZE = 100\n\ntrain_subset = torch.utils.data.Subset(train_dataset, indices=range(SUBSET_SIZE))\n\ntrain_dataloader = DataLoader(\n    train_subset, \n    batch_size=BATCH_SIZE, \n    shuffle=True,\n    num_workers=2,\n    pin_memory=True \n)\n\nmodel = Transformer(\n    src_vocab_size=tokenizer.vocab_size,\n    tgt_vocab_size=tokenizer.vocab_size,\n    d_model=512,\n    num_heads=8,\n    num_layers=6\n).to(DEVICE)\n\noptimizer = Adam(model.parameters(), lr=LEARNING_RATE)\ncriterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id, reduction='mean')\n\ntest_sentence = \"Hello, how are you today?\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T21:16:05.454169Z","iopub.execute_input":"2025-02-16T21:16:05.454436Z","iopub.status.idle":"2025-02-16T21:16:06.876790Z","shell.execute_reply.started":"2025-02-16T21:16:05.454400Z","shell.execute_reply":"2025-02-16T21:16:06.876026Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"for epoch in range(EPOCHS):\n    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n    \n    train_loss = train_epoch(model, train_dataloader, optimizer, criterion, DEVICE)\n    print(f\"Training loss: {train_loss:.4f}\")\n\n    test_sentence = \"Hello, how are you today?\"\n    translation = translate_sentence(model, tokenizer, test_sentence, DEVICE)\n\n    print(f\"\\nTest translation:\")\n    print(f\"Input: {test_sentence}\")\n    print(f\"Output: {translation}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T21:16:06.877594Z","iopub.execute_input":"2025-02-16T21:16:06.877821Z","iopub.status.idle":"2025-02-16T21:16:09.731759Z","shell.execute_reply.started":"2025-02-16T21:16:06.877802Z","shell.execute_reply":"2025-02-16T21:16:09.730313Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1/5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"537f11de17bf41e6b5b8fcf9113dec85"}},"metadata":{}},{"name":"stdout","text":"\nДиагностика первого батча:\ninput_ids shape: torch.Size([32, 128])\nlabels shape: torch.Size([32, 128])\ninput_ids range: [0, 62517]\nlabels range: [0, 62517]\nVocab size: 62518\n\nПосле forward pass:\noutputs shape before view: torch.Size([32, 127, 62518])\n\nПеред loss:\noutputs shape after view: torch.Size([4064, 62518])\ntarget shape: torch.Size([4064])\ntarget range: [0, 62517]\nNumber of invalid indices in target: 0\nTraining loss: 9.5407\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-cbe9b58d53ff>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtest_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Hello, how are you today?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtranslation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nTest translation:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-20a19e877800>\u001b[0m in \u001b[0;36mtranslate_sentence\u001b[0;34m(model, tokenizer, sentence, device, max_length)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0msrc_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0mtgt_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_seq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_seq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0mtgt_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_seq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_seq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiagonal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mtgt_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'tgt_seq_len' is not defined"],"ename":"NameError","evalue":"name 'tgt_seq_len' is not defined","output_type":"error"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}