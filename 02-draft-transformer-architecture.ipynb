{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Собираем трансформер","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T22:14:15.624429Z","iopub.execute_input":"2025-01-30T22:14:15.624831Z","iopub.status.idle":"2025-01-30T22:14:20.085362Z","shell.execute_reply.started":"2025-01-30T22:14:15.624805Z","shell.execute_reply":"2025-01-30T22:14:20.084260Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## Positional Encoding\n\nДо появления архитектуры Transformer, модели для обработки последовательностей(такие как RNN и LSTM), обрабатывали данные последовательно, автоматически учитывая порядок элементов. Однако их вычислительная неэффективность из-за пошаговой обработки и сложности параллелизации привела к поиску альтернатив. \n\nTransformer, появившийся в 2017 году, устранил эти ограничения за счёт полностью параллельного подхода, но возникла новая проблема: модель не могла учитывать порядок элементов в последовательности, так как все токены обрабатывались одновременно. \n\nДля решения этой проблемы был введен **Positional Encoding** — механизм, кодирующий информацию о позиции каждого элемента.\n\nPositional Encoding добавляет к векторным представлениям токенов (эмбеддингам) специальные сигналы, зависящие от их позиции в последовательности. Это позволяет модели различать слова \"кошка\" в позиции 1 и \"кошка\" в позиции 5, даже если их семантические эмбеддинги идентичны. Формула кодирования использует комбинацию синусов и косинусов с разными частотами:  \n$$  \nPE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right), \\quad  \nPE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right),  \n$$  \nгде $pos$ — позиция элемента, $d_{\\text{model}}$ — размерность эмбеддингов, $i$ — индекс измерения в векторе.  \n\nОсновная идея в том, что синусоидальные функции позволяют модели обращать внимание на **относительные позиции**. \n\n### Почему эта странная формула кодирует относительные позиции токенов?\n\nПредставьте, что каждая позиция в последовательности — это точка на числовой прямой. Если для позиции $pos$ мы генерируем сигналы с помощью синуса и косинуса, то для позиции $pos + k$ эти сигналы можно выразить через комбинацию исходных значений. Например, по формуле сложения углов:  \n$$  \n\\sin(pos + k) = \\sin(pos)\\cos(k) + \\cos(pos)\\sin(k),  \n$$  \n\nЭто означает, что смещение на $k$ позиций выражается через взвешенную сумму исходных синуса и косинуса. Это позволяет модели автоматически улавливать, что «слово через три позиции» связано с исходным словом, даже если она никогда не видела такую длинную последовательность при обучении.  \n\nЛогарифмическое убывание частот в формуле $10000^{2i/d_{\\text{model}}}$ обеспечивает, что разные измерения вектора позиционного кодирования отвечают за разные уровни детализации позиции. При малых значениях $i$ (например, первые компоненты вектора) знаменатель $10000^{2i/d_{\\text{model}}}$ становится большим, что замедляет рост аргумента синуса и косинуса при увеличении $pos$. Это создаёт низкочастотные колебания, которые позволяют различать позиции на больших масштабах: например, начало текста (позиции 1-100) от середины (позиции 101-200). При больших $i$ знаменатель уменьшается, аргумент функций растёт быстрее, и возникают высокочастотные колебания, которые кодируют тонкие различия между соседними позициями (например, 101 и 102).  \n\nЧередование синусов и косинусов для чётных и нечётных индексов решает проблему уникальности позиционных кодировок. Если бы использовалась только синусоида, разные позиции могли бы случайно совпадать из-за периодичности функции (например, $\\sin(pos)$ и $\\sin(pos + 2\\pi)$). Добавление косинуса для соседних компонент вектора устраняет эту симметрию: комбинация $\\sin(f(pos))$ и $\\cos(f(pos))$ для разных частот $f$ гарантирует, что каждая позиция $pos$ будет иметь уникальный вектор. Ортогональность синусов и косинусов (их скалярное произведение близко к нулю) минимизирует перекрытие с эмбеддингами слов, что позволяет модели раздельно обрабатывать семантику и позицию.  \n\nСуммирование $\\text{Embedding} + PE$ возможно, потому что эмбеддинги слов и позиционные кодировки имеют одинаковую размерность $d_{\\text{model}}$. Это сложение не требует обучаемых параметров: модель получает объединённый сигнал, где семантика слова модифицируется в соответствии с его позицией. Градиенты распространяются через эту операцию без искажений, так как производная суммы равна сумме производных. В результате, во время обучения модель автоматически учится корректировать и семантические эмбеддинги, и использование позиционной информации (через механизм внимания), не сталкиваясь с конфликтом сигналов.  \n\nИсследовались так же и альтернативные подходы, такие как обучаемые позиционные эмбеддинги. Но синусоидальная схема оказалась предпочтительнее из-за способности обобщаться на последовательности длиннее тех, что встречались при обучении. Таким образом, Positional Encoding стал компромиссом между выразительностью, вычислительной эффективностью и отсутствием дополнительных обучаемых параметров, что идеально соответствовало задаче параллельной обработки в Transformer.  ","metadata":{}},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        \n        pe[:, 0::2] = torch.sin(position * div_term)  # Четные индексы\n        pe[:, 1::2] = torch.cos(position * div_term)  # Нечетные индексы\n        pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        # x: [batch_size, seq_len, d_model]\n        x = x + self.pe[:, :x.size(1), :]\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T22:14:25.689410Z","iopub.execute_input":"2025-01-30T22:14:25.690019Z","iopub.status.idle":"2025-01-30T22:14:25.697351Z","shell.execute_reply.started":"2025-01-30T22:14:25.689986Z","shell.execute_reply":"2025-01-30T22:14:25.696001Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## MultiHeadAttention\n\n**MultiHeadAttention** в архитектуре Transformer возник как ответ на ограничения механизмов внимания, которые до этого использовались в моделях seq2seq. Изначально самовнимание (self-attention) позволяло каждому элементу последовательности взаимодействовать с другими, вычисляя взвешенные суммы их признаков. Однако проблема заключалась в том, что одно \"головное\" внимание (single head) могло фокусироваться только на одном типе зависимостей — например, на синтаксических связях или семантической близости. Для сложных задач, таких как перевод, требовалось одновременно учитывать **разнородные взаимодействия**: связи между подлежащим и сказуемым, анафоры, контекстные синонимы и т.д.  \n\n**Решение**: вместо одного механизма внимания использовать несколько параллельных \"голов\" (heads), каждая из которых учится выделять свой тип зависимостей. Формально, для входных векторов (эмбеддингов) $X \\in \\mathbb{R}^{n \\times d_{\\text{model}}}$, где $n$ — длина последовательности, а $d_{\\text{model}}$ — размерность эмбеддингов, каждая голова $h$ проецирует $X$ в три пространства — запросов ($Q_h$), ключей ($K_h$) и значений ($V_h$) — через обучаемые матрицы весов:  \n$$  \nQ_h = X W_h^Q, \\quad K_h = X W_h^K, \\quad V_h = X W_h^V,  \n$$  \nгде $W_h^Q, W_h^K \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W_h^V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_v}$, а $d_k$ и $d_v$ — размерности подпространств для ключей/запросов и значений. Выбор трёх матриц ($Q, K, V$) обусловлен аналогией с информационным поиском:  \n- **Запросы** ($Q$) — что ищем,  \n- **Ключи** ($K$) — по чему ищем,  \n- **Значения** ($V$) — что возвращаем.  \n\nЕсли бы использовались только $Q$ и $K$, модель не смогла бы преобразовать найденные зависимости в новые признаки. Матрица $V$ добавляет гибкость, позволяя перевзвешивать значения в соответствии с контекстом.  \n\nДля каждой головы вычисляется **масштабированное скалярное произведение** (scaled dot-product attention):  \n$$  \n\\text{Attention}(Q_h, K_h, V_h) = \\text{softmax}\\left(\\frac{Q_h K_h^T}{\\sqrt{d_k}}\\right) V_h.  \n$$  \n**Почему softmax?** Функция softmax преобразует неограниченные оценки сходства (логиты) в вероятностное распределение, где сумма весов внимания равна 1. Это гарантирует, что выходные значения остаются в разумном диапазоне, а модель фокусируется на наиболее релевантных элементах.  \n\n**Масштабирование на $\\sqrt{d_k}$** введено для контроля дисперсии. Если компоненты $Q_h$ и $K_h$ независимы и имеют дисперсию 1, то дисперсия их скалярного произведения $Q_h K_h^T$ равна $d_k$. Без масштабирования при больших $d_k$ аргументы softmax становятся экстремальными, градиенты насыщаются, и обучение замедляется.  \n\n**Объединение голов**: выходы всех голов конкатенируются и проецируются обратно в пространство размерности $d_{\\text{model}}$:  \n$$  \n\\text{MultiHead}(X) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_H) W^O,  \n$$  \nгде $W^O \\in \\mathbb{R}^{H d_v \\times d_{\\text{model}}}$ — обучаемая матрица. Размерности $d_k$ и $d_v$ обычно выбирают равными $d_{\\text{model}} / H$, чтобы сохранить общую вычислительную сложность. Например, при $d_{\\text{model}}=512$ и $H=8$, $d_k = d_v = 64$.  \n\n**Почему именно такая размерность?**  \n- Если бы $d_k$ и $d_v$ не уменьшались с ростом $H$, вычислительная сложность MultiHeadAttention росла бы квадратично: $O(H n^2 d_k)$. Сокращение $d_k$ и $d_v$ до $d_{\\text{model}} / H$ сохраняет сложность на уровне $O(n^2 d_{\\text{model}})$, как у одноголового внимания.  \n- Проекция $W^O$ компенсирует уменьшение размерности в головах, возвращая выход в исходное пространство $d_{\\text{model}}$, что необходимо для совместимости с другими слоями Transformer.  \n\n**Почему именно эта формула?**  \n1. **Разделение подпространств**: Каждая голова работает в своём $d_k$-мерном пространстве, что позволяет моделировать **независимые типы взаимодействий**. Например, одна голова может отслеживать согласование существительного с прилагательным, другая — ссылки на предыдущие предложения. Линейные проекции $W_h^Q, W_h^K, W_h^V$ \"разделяют\" исходные эмбеддинги на компоненты, которые легче интерпретировать в рамках конкретной задачи.  \n2. **Параллелизм**: Независимость вычислений между головами позволяет эффективно распределять их на GPU.  \n3. **Интерпретируемость**: Анализ весов внимания в разных головах (после обучения) позволяет выявить, какие типы паттернов выучила модель.  \n\n**Пример вычисления для одной головы**:  \nПусть $X$ — матрица эмбеддингов размерности $n \\times d_{\\text{model}}$. Для головы $h$:  \n- $Q_h = X W_h^Q$ ($n \\times d_k$),  \n- $K_h = X W_h^K$ ($n \\times d_k$),  \n- $V_h = X W_h^V$ ($n \\times d_v$).  \n\nМатрица весов внимания $A_h = \\text{softmax}\\left(\\frac{Q_h K_h^T}{\\sqrt{d_k}}\\right)$ ($n \\times n$) умножается на $V_h$, давая выход $A_h V_h$ ($n \\times d_v$). Конкатенация выходов всех голов создаёт матрицу $n \\times (H d_v)$, которая проецируется обратно в $n \\times d_{\\text{model}}$ через $W^O$.  \n\nКритически важным стало **сохранение размерности**: выход MultiHeadAttention имеет ту же размерность $d_{\\text{model}}$, что и вход, что позволяет стыковать его с остальными компонентами Transformer (нормализацией, feed-forward слоями) без дополнительных преобразований. Это также обеспечивает стабильность градиентов при глубоких архитектурах.  \n\nИсторически, MultiHeadAttention стал ключевым отличием Transformer от предыдущих моделей с вниманием, таких как Pointer Networks. Его способность декомпозировать сложные зависимости в параллельные подзадачи сделала возможным обучение на больших корпусах текстов с сохранением интерпретируемости и вычислительной эффективности.  ","metadata":{}},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n        \n        assert self.head_dim * num_heads == d_model, \"d_model must be divisible by num_heads\"\n        \n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n        \n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        # Q: [batch_size, num_heads, seq_len, head_dim]\n        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n        \n        if mask is not None:\n            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n        \n        attn_probs = F.softmax(attn_scores, dim=-1)\n        output = torch.matmul(attn_probs, V)\n        return output\n        \n    def forward(self, Q, K, V, mask=None):\n        batch_size = Q.size(0)\n        \n        # Линейные преобразования\n        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        \n        # Вычисление внимания\n        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n        \n        # Объединение голов\n        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n        \n        # Финальное линейное преобразование\n        output = self.W_o(attn_output)\n        return output","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## FeedForward \n\nПозиционно-зависимый Feed Forward слой в Transformer появился как ответ на необходимость **нелинейного преобразования признаков** после этапа внимания. MultiHeadAttention эффективно вычисляет глобальные зависимости между токенами, но для сложных задач (например, перевода) этого недостаточно: модель должна уметь комбинировать извлечённые паттерны и трансформировать их в новые семантические представления.  \n\nКаждый токен последовательности независимо проходит через два линейных слоя. Первый слой расширяет пространство с $d_{\\text{model}}$ (например, 512) до $d_{\\text{ff}}$ (обычно 2048), применяя функцию активации ReLU:  \n$$  \n\\text{hidden} = \\text{ReLU}(x W_1 + b_1),  \n$$  \nгде $W_1 \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{\\text{ff}}}$. \n\nРасширение размерности в 4 раза ($d_{\\text{ff}} = 4d_{\\text{model}}$) даёт модели достаточно параметров для обучения неочевидных комбинаций признаков. Второй слой возвращает представление в исходную размерность $d_{\\text{model}}$:  \n$$  \n\\text{output} = \\text{hidden} W_2 + b_2,  \n$$  \nгде $W_2 \\in \\mathbb{R}^{d_{\\text{ff}} \\times d_{\\text{model}}}$. Между слоями добавляется dropout (например, с вероятностью 0.1) для регуляризации.  \n\n**Почему именно так?**  \n- **Нелинейность**: ReLU разрывает линейность, позволяя моделировать сложные функции. Без неё комбинация линейных слоёв сводилась бы к одному матричному умножению.  \n- **Расширение-сжатие**: Увеличение размерности создаёт «бутылочное горлышко», вынуждая модель фильтровать шумовые признаки. Это похоже на работу autoencoder, но без потери информации, так как выход сохраняет исходную размерность.  \n- **Позиционная независимость**: Обработка каждого токена отдельно компенсирует потенциальные потери локальных зависимостей после глобального внимания. Например, для фразы «синий шар» внимание может связать прилагательное и существительное, а FFN преобразует их совместное представление в вектор, кодирующий цвет и форму.  \n\nВход и выход слоя имеют одинаковую размерность $d_{\\text{model}}$, что позволяет повторять блоки Encoder/Decoder. Dropout и остаточные соединения (реализованные вне этого слоя) стабилизируют обучение глубоких сетей. Исторически, эта архитектура заменила свёрточные слои из ранних моделей seq2seq, обеспечив более гибкое преобразование признаков без ограничений локальных рецептивных полей.  ","metadata":{}},{"cell_type":"code","source":"class FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff=2048):\n        super().__init__()\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.linear2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(0.1)\n        \n    def forward(self, x):\n        x = self.dropout(F.relu(self.linear1(x)))\n        x = self.linear2(x)\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## EncoderLayer\n\nДо появления **EncoderLayer** в Transformer исследователи сталкивались с дилеммой: как совместить глобальное понимание контекста с локальными преобразованиями признаков, не теряя стабильности обучения в глубоких сетях. Ранние подходы, такие как RNN, страдали от исчезающих градиентов, а свёрточные сети требовали множества слоёв для захвата длинных зависимостей. Self-attention в Transformer решил проблему глобального контекста, но сам по себе не мог обеспечить сложную иерархическую обработку данных. Возник вопрос: как организовать последовательные преобразования, чтобы модель могла сначала выявить связи между токенами, а затем «переосмыслить» их, сохраняя устойчивость к глубине?  \n\n**EncoderLayer** стал ответом — модулем, который объединяет два принципиальных этапа. Сначала входные эмбеддинги $x$, обогащённые позиционной информацией (Positional Encoding), проходят через **MultiHeadAttention**. Здесь каждый токен «спрашивает» остальные:  \n$$  \n\\text{attn\\_output} = \\text{MultiHeadAttention}(x, x, x, mask),  \n$$  \nгде $mask$ скрывает будущие токены (в декодере) или padding. Этот шаг позволяет, например, связать местоимение «он» с соответствующим существительным, даже если они разделены десятками слов. Но самовнимание — операция линейная в пространстве признаков. Чтобы добавить нелинейность и глубину, следом идёт **Feed Forward Network (FFN)** — два линейных слоя с расширением размерности:  \n$$  \n\\text{ffn\\_output} = \\text{FFN}(x) = \\text{ReLU}(x W_1 + b_1) W_2 + b_2.  \n$$  \nFFN действует как «мыслительный процесс»: преобразует глобальные зависимости, найденные вниманием, в новые семантические представления. Например, если внимание связало «яблоко» и «зелёное», FFN может закодировать это в вектор «фрукт + цвет».  \n\nНо простое соединение этих шагов приводило к проблемам. Глубокие сети «забывали» исходные данные — градиенты исчезали, а признаки искажались. Здесь на помощь пришли **остаточные соединения** и **слойная нормализация**. После каждого подшага (самовнимание или FFN) к выходу добавляется исходный вход $x$, а результат нормализуется:  \n$$  \nx = \\text{LayerNorm}(x + \\text{Dropout}(\\text{sublayer}(x))).  \n$$  \nОстатки работают как мосты, через которые градиенты и исходная информация свободно протекают даже через десятки слоёв. LayerNorm стабилизирует распределение активаций, вычисляя среднее и дисперсию по $d_{\\text{model}}$-измерениям, что предотвращает «взрыв» или «затухание» значений.  \n\n**Почему именно этот порядок?** Если бы FFN шёл до внимания, нелинейности ReLU могли бы «сломать» позиционную информацию, критичную для self-attention. А пост-нормализация (после остатка) вместо пре-нормализации (до подшага) выбрана не случайно: в оригинальном Transformer это заставляло градиенты проходить как через преобразованный, так и через исходный путь, балансируя обновления параметров.  \n\n**Пример**: Эмбеддинг слова «bank» после самовнимания может получить признаки, связывающие его с «river» (банк как берег) или «money» (банк как учреждение). FFN преобразует эти связи в контекстно-зависимое представление, а остатки и нормализация сохраняют стабильность сигнала. Повторяясь через несколько EncoderLayer, модель итеративно уточняет смысл, как будто перечитывая текст, каждый раз замечая новые нюансы.  \n\nИсторически, EncoderLayer стал шаблоном для масштабируемости. Его можно было повторять N раз (например, 6 или 12 слоёв), создавая глубокие модели без краха градиентов. Комбинация самовнимания и FFN оказалась настолько эффективной, что даже современные LLM, как GPT-4, сохраняют эту базовую структуру, дополняя её новыми механизмами.  ","metadata":{}},{"cell_type":"code","source":"class EncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.ffn = FeedForward(d_model)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(0.1)\n        \n    def forward(self, x, mask=None):\n        # Self attention\n        attn_output = self.self_attn(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        \n        # Feed forward\n        ffn_output = self.ffn(x)\n        x = self.norm2(x + self.dropout(ffn_output))\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## DecoderLayer\n\nЕсли EncoderLayer в Transformer научился понимать входной текст, собирая контекст в плотные векторы, то **DecoderLayer** возник из необходимости **генерировать выход** — слово за словом, учитывая и прошлые предсказания, и информацию от энкодера. Ранние подходы, как seq2seq с вниманием, уже связывали энкодер и декодер, но их рекуррентная природа ограничивала параллелизацию и способность захватывать сложные зависимости. В Transformer декодер должен был стать авторегрессионным, но без потери параллелизма — и здесь ключевым стал механизм **маскированного самовнимания** в сочетании с **кросс-вниманием**.  \n\n**DecoderLayer** начинает работу с уже частично сгенерированной последовательности (например, переведённого предложения до текущего слова). Чтобы гарантировать, что модель не «подсматривает» будущие токены, применяется **маскированное самовнимание**:  \n$$  \n\\text{attn\\_output} = \\text{MultiHeadAttention}(x, x, x, tgt\\_mask),  \n$$  \nгде $tgt\\_mask$ — верхнетреугольная матрица с $-\\\\infty$ на позициях будущих токенов. При вычислении softmax это превращается в ноль, обнуляя их влияние. Например, при генерации третьего слова маска скрывает все токены после третьего, заставляя модель опираться только на уже созданный контекст.  \n\nНо самовнимания недостаточно — декодер должен **соотносить выход с входом**. Для этого вводится **кросс-внимание**, где запросы ($Q$) берутся из декодера, а ключи ($K$) и значения ($V$) — из выхода энкодера:  \n$$  \n\\text{cross\\_attn\\_output} = \\text{MultiHeadAttention}(x, enc\\_output, enc\\_output, src\\_mask).  \n$$  \nЗдесь $src\\_mask$ скрывает padding-токены исходной последовательности. Этот шаг работает как «опрос» энкодера: декодер «спрашивает», какие части входного текста релевантны текущему шагу генерации. Например, при переводе слова «apple» декодер через кросс-внимание связывает его с энкодеровскими «яблоко» или «компания», в зависимости от контекста.  \n\nПосле кросс-внимания, как и в энкодере, следует **Feed Forward Network**, добавляющий нелинейность:  \n$$  \n\\text{ffn\\_output} = \\text{FFN}(x).  \n$$  \nКаждый шаг сопровождается **остаточными соединениями** и **слойной нормализацией**:  \n$$  \nx = \\text{LayerNorm}(x + \\text{Dropout}(\\text{sublayer}(x))),  \n$$  \nчто сохраняет стабильность градиентов даже в глубоких сетях.  \n\n**Почему именно три этапа?**  \n1. **Маскированное самовнимание** изолирует уже сгенерированную часть последовательности, имитируя авторегрессию RNN.  \n2. **Кросс-внимание** синхронизирует энкодер и декодер, позволяя последнему «заглядывать» в исходные данные — аналогично выравниванию в статистическом машинном переводе.  \n3. **FFN** переосмысляет объединённую информацию, как финальный этап «принятия решения» о следующем токене.  \n\n**Пример**: При переводе «I hit the bank» на русский, декодер:  \n1. Через маскированное самовнимание связывает «я ударил» с «по», игнорируя будущие слова.  \n2. Кросс-внимание находит в энкодере связь «bank» → «берег» (если контекст о реке) или «банк» (если о финансах).  \n3. FFN преобразует это в «по берегу» или «в банк», сохраняя грамматику.  \n\n**Исторически**, DecoderLayer стал мостом между пониманием (энкодер) и генерацией. Его способность параллельно обрабатывать последовательность, но авторегрессионно предсказывать токены, сделала Transformer универсальным каркасом для задач, требующих преобразования структур — от перевода до генерации кода. Каждый слой декодера — это шаг в «диалоге» между исходными данными и растущим выходом, где кросс-внимание выступает переводчиком, а FFN — редактором.  ","metadata":{}},{"cell_type":"code","source":"class DecoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n        self.ffn = FeedForward(d_model)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(0.1)\n        \n    def forward(self, x, enc_output, src_mask, tgt_mask):\n        # Self attention (маскированное)\n        attn_output = self.self_attn(x, x, x, tgt_mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        \n        # Cross attention (с выходом энкодера)\n        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n        x = self.norm2(x + self.dropout(attn_output))\n        \n        # Feed forward\n        ffn_output = self.ffn(x)\n        x = self.norm3(x + self.dropout(ffn_output))\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Transformer\n\nКогда все компоненты Transformer — энкодер, декодер, механизмы внимания и позиционные кодировки — были разработаны, оставалась задача объединить их в единую модель, способную обучаться на парах последовательностей (например, исходный текст и перевод). Ранние подходы, такие как Seq2Seq, уже использовали разделение на энкодер и декодер, но их рекуррентная природа ограничивала параллелизацию и глубину. Архитектура Transformer, представленная в коде, стала итогом поиска баланса между выразительностью и вычислительной эффективностью.  \n\n**Сборка модели** начинается с преобразования токенов в векторы. Эмбеддинги (`encoder_embedding` и `decoder_embedding`) отображают слова в пространство размерности $d_{\\text{model}}$, а `positional_encoding` добавляет информацию о позициях:  \n$$  \nX_{\\text{enc}} = \\text{Embedding}(src) + \\text{PositionalEncoding}(src),  \n$$  \n$$  \nX_{\\text{dec}} = \\text{Embedding}(tgt) + \\text{PositionalEncoding}(tgt).  \n$$  \nБез позиционного кодирования модель не смогла бы отличить перестановки слов, так как self-attention инвариантен к порядку.  \n\nДалее энкодер и декодер собираются как **стек слоёв** (`num_layers`). Каждый слой в энкодере (`EncoderLayer`) последовательно уточняет представления входных данных: самовнимание находит глобальные зависимости, FFN добавляет нелинейность, а остатки и нормализация сохраняют устойчивость. Аналогично, декодер (`DecoderLayer`) поочерёдно применяет маскированное самовнимание, кросс-внимание к энкодеру и FFN. Многократное повторение этих слоёв позволяет модели итеративно улучшать представления, как бы «перечитывая» данные на разных уровнях абстракции.  \n\n**Финальный слой** (`fc_out`) выполняет проекцию из $d_{\\text{model}}$ в размер словаря целевого языка. Это преобразование интерпретирует векторы декодера как логиты — оценки вероятности каждого токена в словаре:  \n$$  \n\\text{output} = W_{\\text{out}} \\cdot \\text{dec\\_output} + b_{\\text{out}}.  \n$$  \nSoftmax на выходе (не явно указанный в коде, но подразумеваемый в функции потерь) превращает логиты в распределение вероятностей, из которого выбирается следующее слово.  \n\n**Почему именно так?**  \n- **Глубина (`num_layers`)**: Каждый слой захватывает разные аспекты данных. Ранние слои энкодера могут выделять синтаксис, поздние — семантику. В декодере нижние слои отвечают за связь с энкодером, верхние — за грамматику вывода.  \n- **Разделение эмбеддингов**: Разные матрицы для исходного и целевого языков позволяют модели работать с multilingual данными.  \n- **Совместимость размерностей**: Все компоненты сохраняют размерность $d_{\\text{model}}$, что упрощает обучение — градиенты свободно текут через остатки, а параметры обновляются согласованно. ","metadata":{}},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, num_heads=8, num_layers=6):\n        super().__init__()\n        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n        self.positional_encoding = PositionalEncoding(d_model)\n        \n        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads) for _ in range(num_layers)])\n        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads) for _ in range(num_layers)])\n        \n        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n        \n    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n        # Энкодинг\n        src_emb = self.positional_encoding(self.encoder_embedding(src))\n        enc_output = src_emb\n        for layer in self.encoder_layers:\n            enc_output = layer(enc_output, src_mask)\n        \n        # Декодинг\n        tgt_emb = self.positional_encoding(self.decoder_embedding(tgt))\n        dec_output = tgt_emb\n        for layer in self.decoder_layers:\n            dec_output = layer(dec_output, enc_output, src_mask, tgt_mask)\n        \n        # Финальный слой\n        output = self.fc_out(dec_output)\n        return output","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}