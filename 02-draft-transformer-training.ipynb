{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1375271,"sourceType":"datasetVersion","datasetId":802062}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Архитетура модели\n\nЯ буду использовать модель, описанную в этом ноутбке","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T22:37:04.055052Z","iopub.execute_input":"2025-02-03T22:37:04.055322Z","iopub.status.idle":"2025-02-03T22:37:07.046961Z","shell.execute_reply.started":"2025-02-03T22:37:04.055291Z","shell.execute_reply":"2025-02-03T22:37:07.046101Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_causal_mask(size):\n    return torch.triu(torch.ones(size, size), diagonal=1).bool()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T22:37:13.277312Z","iopub.execute_input":"2025-02-03T22:37:13.277596Z","iopub.status.idle":"2025-02-03T22:37:13.281498Z","shell.execute_reply.started":"2025-02-03T22:37:13.277576Z","shell.execute_reply":"2025-02-03T22:37:13.280426Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n        self.d_model = d_model\n        \n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        \n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x * math.sqrt(self.d_model)  # Теперь self.d_model доступен\n        x = x + self.pe[:, :x.size(1), :]\n        return x\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n        \n        assert self.head_dim * num_heads == d_model, \"d_model must be divisible by num_heads\"\n        \n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n        \n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n        \n        if mask is not None:\n            attn_scores = attn_scores.masked_fill(mask, -1e9)  # Изменено == 0 на mask\n        \n        attn_probs = F.softmax(attn_scores, dim=-1)\n        return torch.matmul(attn_probs, V)\n        \n    def forward(self, Q, K, V, mask=None):\n        batch_size = Q.size(0)\n        \n        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        \n        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n        \n        output = self.W_o(attn_output)\n        return output\n\nclass FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff=2048):\n        super().__init__()\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.linear2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(0.1)\n        \n    def forward(self, x):\n        x = self.dropout(F.relu(self.linear1(x)))\n        x = self.linear2(x)\n        return x\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.ffn = FeedForward(d_model)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(0.1)\n        \n    def forward(self, x, mask=None):\n        # Self attention\n        attn_output = self.self_attn(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        \n        # Feed forward\n        ffn_output = self.ffn(x)\n        x = self.norm2(x + self.dropout(ffn_output))\n        return x\n\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n        self.ffn = FeedForward(d_model)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(0.1)\n        \n    def forward(self, x, enc_output, src_mask, tgt_mask):\n        # Self attention\n        attn_output = self.self_attn(x, x, x, tgt_mask)  # Убрано создание маски\n        x = self.norm1(x + self.dropout(attn_output))\n        \n        # Cross attention\n        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n        x = self.norm2(x + self.dropout(attn_output))\n        \n        # Feed forward\n        ffn_output = self.ffn(x)\n        x = self.norm3(x + self.dropout(ffn_output))\n        return x\n\nclass Transformer(nn.Module):\n    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, num_heads=8, num_layers=6):\n        super().__init__()\n        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n        self.positional_encoding = PositionalEncoding(d_model)\n        \n        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads) for _ in range(num_layers)])\n        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads) for _ in range(num_layers)])\n        \n        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n        \n    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n        # Encoding\n        src_emb = self.positional_encoding(self.encoder_embedding(src))\n        enc_output = src_emb\n        for layer in self.encoder_layers:\n            enc_output = layer(enc_output, src_mask)\n        \n        # Decoding\n        tgt_emb = self.positional_encoding(self.decoder_embedding(tgt))\n        dec_output = tgt_emb\n        for layer in self.decoder_layers:\n            dec_output = layer(dec_output, enc_output, src_mask, tgt_mask)\n        \n        # Output layer\n        output = self.fc_out(dec_output)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T22:37:13.795319Z","iopub.execute_input":"2025-02-03T22:37:13.795578Z","iopub.status.idle":"2025-02-03T22:37:13.813637Z","shell.execute_reply.started":"2025-02-03T22:37:13.795556Z","shell.execute_reply":"2025-02-03T22:37:13.812876Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport pandas as pd\nimport numpy as np\n\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom nltk.translate.bleu_score import corpus_bleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T22:37:17.232842Z","iopub.execute_input":"2025-02-03T22:37:17.233173Z","iopub.status.idle":"2025-02-03T22:37:20.020458Z","shell.execute_reply.started":"2025-02-03T22:37:17.233148Z","shell.execute_reply":"2025-02-03T22:37:20.019802Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/frenchenglish/fra.txt', sep=\"\\t\", header=None)\ndf = df.drop(columns=2)\ndf = df.rename(columns={0: 'en', 1: 'fr'})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T22:37:20.021403Z","iopub.execute_input":"2025-02-03T22:37:20.021741Z","iopub.status.idle":"2025-02-03T22:37:21.020384Z","shell.execute_reply.started":"2025-02-03T22:37:20.021707Z","shell.execute_reply":"2025-02-03T22:37:21.019679Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T22:37:21.021723Z","iopub.execute_input":"2025-02-03T22:37:21.022076Z","iopub.status.idle":"2025-02-03T22:37:21.037958Z","shell.execute_reply.started":"2025-02-03T22:37:21.022045Z","shell.execute_reply":"2025-02-03T22:37:21.037208Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Подготовка данных\nclass TranslationDataset(Dataset):\n    def __init__(self, file_path, src_lang='en', tgt_lang='fr', max_samples=50000):\n        self.data = pd.read_csv(file_path, sep='\\t', header=None).sample(max_samples)\n\n        self.data = self.data.drop(columns=2)\n        self.data = self.data.rename(columns={0: 'en', 1: 'fr'})\n        self.data = self.data.dropna()\n        self.data = self.data.drop_duplicates()\n\n        # Добавление специальных токенов\n        self.special_tokens = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n        \n        # Создание словарей\n        self.src_vocab = self.build_vocab(self.data[src_lang])\n        self.tgt_vocab = self.build_vocab(self.data[tgt_lang])\n        \n        \n    def build_vocab(self, sentences, min_freq=2):\n        vocab = {}\n        word_counts = {}\n        \n        # Токенизация простым split\n        for sentence in sentences:\n            for word in re.findall(r\"\\w+|[^\\w\\s]\", sentence.lower()):\n                word_counts[word] = word_counts.get(word, 0) + 1\n                \n        # Фильтрация редких слов\n        vocab = {word:i+len(self.special_tokens) for i, (word, count) in \n                enumerate([(k,v) for k,v in word_counts.items() if v >= min_freq])}\n        \n        # Добавление специальных токенов\n        vocab.update(self.special_tokens)\n        return vocab\n    \n    def sentence_to_ids(self, sentence, vocab):\n        tokens = re.findall(r\"\\w+|[^\\w\\s]\", sentence.lower())\n        return [vocab.get(token, vocab['<unk>']) for token in tokens]\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        src_sentence = self.data.iloc[idx]['en']\n        tgt_sentence = self.data.iloc[idx]['fr']\n        \n        # Конвертация в индексы\n        src_ids = [self.src_vocab['<sos>']] + self.sentence_to_ids(src_sentence, self.src_vocab) + [self.src_vocab['<eos>']]\n        tgt_ids = [self.tgt_vocab['<sos>']] + self.sentence_to_ids(tgt_sentence, self.tgt_vocab) + [self.tgt_vocab['<eos>']]\n        \n        return {\n            'src': torch.LongTensor(src_ids),\n            'tgt': torch.LongTensor(tgt_ids)\n        }\n\n# 2. Функция для паддинга\ndef collate_fn(batch):\n    src_batch = [item['src'] for item in batch]\n    tgt_batch = [item['tgt'] for item in batch]\n    \n    # Паддинг\n    src_batch = nn.utils.rnn.pad_sequence(src_batch, padding_value=0)\n    tgt_batch = nn.utils.rnn.pad_sequence(tgt_batch, padding_value=0)\n    \n    return {\n        'src': src_batch.transpose(0, 1),  # [batch, seq_len]\n        'tgt': tgt_batch.transpose(0, 1)\n    }\n\n# 3. Инициализация данных\ndataset = TranslationDataset('/kaggle/input/frenchenglish/fra.txt')\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\n\ntrain_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, collate_fn=collate_fn, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, collate_fn=collate_fn)\n\n# 4. Инициализация модели\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel = Transformer(\n    src_vocab_size=len(dataset.src_vocab),\n    tgt_vocab_size=len(dataset.tgt_vocab),\n    d_model=256,\n    num_heads=4,\n    num_layers=2\n).to(device)\n\n# 5. Обучение\noptimizer = optim.Adam(model.parameters(), lr=3e-4)\ncriterion = nn.CrossEntropyLoss(ignore_index=0)\n\ndef init_weights(m):\n    if isinstance(m, nn.Linear):\n        nn.init.xavier_normal_(m.weight)\n        if m.bias is not None:\n            nn.init.zeros_(m.bias)\n    elif isinstance(m, nn.Embedding):\n        nn.init.normal_(m.weight, mean=0, std=0.01)\n        \nmodel.apply(init_weights)\n\ndef train_epoch(model, loader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0\n    \n    for batch in loader:\n        src = batch['src'].to(device)\n        tgt = batch['tgt'].to(device)\n        \n        # Подготовка данных для декодера\n        tgt_input = tgt[:, :-1]  # Исключаем последний токен\n        tgt_output = tgt[:, 1:]   # Исключаем первый токен (SOS)\n        \n        # Создание масок\n        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)  # [batch, 1, 1, src_len]\n        \n        # Маска для декодера: padding + causal\n        tgt_padding_mask = (tgt_input != 0).unsqueeze(1).unsqueeze(2)  # [batch, 1, 1, tgt_len-1]\n        causal_mask = generate_causal_mask(tgt_input.size(1)).to(device)\n        causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)  # [1, 1, tgt_len-1, tgt_len-1]\n        tgt_mask = (tgt_padding_mask.to(torch.bool) | causal_mask.to(torch.bool))\n        \n        optimizer.zero_grad()\n        \n        output = model(src, tgt_input, src_mask, tgt_mask)\n        \n        # Расчет потерь\n        loss = criterion(\n            output.reshape(-1, output.size(-1)),\n            tgt_output.reshape(-1)\n        )\n        \n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) \n        optimizer.step()\n        total_loss += loss.item()\n    \n    return total_loss / len(loader)\n\n# 6. Валидация\ndef evaluate(model, loader, criterion, device, src_vocab, tgt_vocab):\n    model.eval()\n    total_loss = 0\n    references = []\n    hypotheses = []\n    \n    idx_to_word = {v: k for k, v in tgt_vocab.items()}\n    \n    with torch.no_grad():\n        for batch in loader:\n            # Исправляем передачу device\n            src = batch['src'].to(device)\n            tgt = batch['tgt'].to(device)\n            \n            # Подготовка данных для декодера\n            tgt_input = tgt[:, :-1]\n            tgt_output = tgt[:, 1:]\n            \n            # Создание масок\n            src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n            tgt_padding_mask = (tgt_input != 0).unsqueeze(1).unsqueeze(2)\n            causal_mask = generate_causal_mask(tgt_input.size(1)).to(device)\n            causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)\n            tgt_mask = tgt_padding_mask | causal_mask\n            \n            # Перенос масок на устройство\n            src_mask = src_mask.to(device)\n            tgt_mask = tgt_mask.to(device)\n            \n            # Forward pass\n            output = model(src, tgt_input, src_mask, tgt_mask)\n            \n            # Расчет потерь\n            loss = criterion(\n                output.reshape(-1, output.size(-1)),\n                tgt_output.reshape(-1)\n            )\n            total_loss += loss.item()\n            \n            # Генерация переводов для BLEU\n            preds = output.argmax(dim=-1)\n            for i in range(preds.size(0)):\n                ref = [idx_to_word.get(idx.item(), '<unk>') for idx in tgt[i, 1:]]\n                hyp = [idx_to_word.get(idx.item(), '<unk>') for idx in preds[i]]\n                \n                references.append([ref])\n                hypotheses.append(hyp)\n    \n    bleu = corpus_bleu(references, hypotheses)\n    return bleu\n\n# 7. Основной цикл обучения\nnum_epochs = 20\n\nfor epoch in range(num_epochs):\n    train_loss = train_epoch(model, train_loader, optimizer, criterion, device=\"cuda\")\n    val_bleu = evaluate(\n        model, \n        val_loader, \n        criterion,  # Добавляем criterion для вычисления потерь\n        device=\"cuda\", \n        src_vocab=dataset.src_vocab, \n        tgt_vocab=dataset.tgt_vocab\n    )\n    \n    print(f'Epoch {epoch+1}/{num_epochs}')\n    print(f'Train Loss: {train_loss:.4f} | Val BLEU: {val_bleu:.4f}')\n    print('-'*50)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-03T22:37:44.812773Z","iopub.execute_input":"2025-02-03T22:37:44.813102Z","iopub.status.idle":"2025-02-03T22:38:35.250025Z","shell.execute_reply.started":"2025-02-03T22:37:44.813078Z","shell.execute_reply":"2025-02-03T22:38:35.248799Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def translate(sentence, model, src_vocab, tgt_vocab, device, max_len=50, temperature=0.7):\n    model.eval()\n    \n    # Токенизация с обработкой неизвестных слов\n    tokens = re.findall(r\"\\w+|[^\\w\\s]\", sentence.lower())\n    src_ids = [src_vocab.get('<sos>', 1)] \n    src_ids += [src_vocab.get(tok, src_vocab['<unk>']) for tok in tokens]\n    src_ids.append(src_vocab.get('<eos>', 2))\n    \n    src = torch.LongTensor(src_ids).unsqueeze(0).to(device)\n    \n    # Кодирование\n    with torch.no_grad():\n        src_emb = model.encoder_embedding(src)\n        src_emb = model.positional_encoding(src_emb)\n        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n        enc_output = src_emb\n        for layer in model.encoder_layers:\n            enc_output = layer(enc_output, src_mask.to(device))\n    \n    # Генерация с температурой\n    tgt_ids = [tgt_vocab['<sos>']]\n    for _ in range(max_len):\n        tgt = torch.LongTensor(tgt_ids).unsqueeze(0).to(device)\n        \n        tgt_padding_mask = (tgt != 0).unsqueeze(1).unsqueeze(2).to(device)\n        causal_mask = generate_causal_mask(tgt.size(1)).to(device)\n        tgt_mask = (tgt_padding_mask | causal_mask.unsqueeze(0)).bool()\n        \n        with torch.no_grad():\n            tgt_emb = model.decoder_embedding(tgt)\n            tgt_emb = model.positional_encoding(tgt_emb)\n            dec_output = tgt_emb\n            for layer in model.decoder_layers:\n                dec_output = layer(dec_output, enc_output, src_mask.to(device), tgt_mask)\n            \n            logits = model.fc_out(dec_output[:, -1, :]) / temperature\n            probs = F.softmax(logits, dim=-1)\n            \n        next_token = torch.multinomial(probs, 1).item()\n        \n        if next_token == tgt_vocab.get('<eos>', 2):\n            break\n            \n        tgt_ids.append(next_token)\n    \n    # Конвертация в текст\n    idx_to_word = {v:k for k,v in tgt_vocab.items()}\n    return ' '.join([idx_to_word.get(idx, '<unk>') for idx in tgt_ids[1:]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T22:31:59.077682Z","iopub.execute_input":"2025-02-03T22:31:59.078106Z","iopub.status.idle":"2025-02-03T22:31:59.087336Z","shell.execute_reply.started":"2025-02-03T22:31:59.078068Z","shell.execute_reply":"2025-02-03T22:31:59.086281Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Пример использования\ntest_sentence = \"Hello, how are you?\"\nprint(\"Input:\", test_sentence)\nprint(\"Translation:\", translate(test_sentence, model, dataset.src_vocab, dataset.tgt_vocab, device))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T22:26:31.526722Z","iopub.execute_input":"2025-02-03T22:26:31.527120Z","iopub.status.idle":"2025-02-03T22:26:31.836773Z","shell.execute_reply.started":"2025-02-03T22:26:31.527088Z","shell.execute_reply":"2025-02-03T22:26:31.835973Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_tokens = dataset.sentence_to_ids(\"Hello, how are you?\", dataset.src_vocab)\nprint(\"Tokenized test sentence:\", test_tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T22:28:52.353455Z","iopub.execute_input":"2025-02-03T22:28:52.353816Z","iopub.status.idle":"2025-02-03T22:28:52.358408Z","shell.execute_reply.started":"2025-02-03T22:28:52.353787Z","shell.execute_reply":"2025-02-03T22:28:52.357708Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Пример использования\ntest_sentence = \"Top-down economics never works, said Obama.\"\nprint(\"Input:\", test_sentence)\nprint(\"Translation:\", translate(test_sentence, model, dataset.src_vocab, dataset.tgt_vocab, device))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T22:29:43.010109Z","iopub.execute_input":"2025-02-03T22:29:43.010429Z","iopub.status.idle":"2025-02-03T22:29:43.155612Z","shell.execute_reply.started":"2025-02-03T22:29:43.010406Z","shell.execute_reply":"2025-02-03T22:29:43.154843Z"}},"outputs":[],"execution_count":null}]}