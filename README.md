# Transformer Playground üß†‚ö°

To deeply understand transformer architectures through practice‚Äîfrom implementing one from scratch to adapting pre-trained models (BERT, GPT). Why? Because transformers power nearly everything in modern ML, and the best way to learn is to get your hands dirty.  

---

## Progress Checklist üîç

### Basic Components  
- [x] **Tokenization**: BPE, HuggingFace Tokenizers‚Äîlearn how text becomes numbers.  
- [x] **Transformer Architecture**: A clean-code implementation (no `import transformers` allowed).  
- [x] **Transformer Training**: Train a model on a simple translation task
- [x] **Experiment Tracking**: Integrate WandB/MLFlow‚Äîlog metrics, weights, visualizations.  

### Enhancements & Optimization üõ†Ô∏è  
- [ ] **Architecture Enhancing**: Add FlashAttention, ALiBi, or other tweaks.  
- [ ] **Optimization**: Mixed Precision, Gradient Checkpointing, etc 
- [ ] **Adapters**: Implement and use adapters for fine-tuning.  
- [ ] **Quantization**: Compress models without (major) quality loss.  

### Working with Pretrained Models ü§ñ  
- [ ] **BERT**: Use Hugging Face's pretrained BERT for classification/NER.  
- [ ] **GPT**: Text generation with customization (prompt engineering, fine-tuning).  
- [ ] **Visual Transformers**: Image classification with ViT.  
- [ ] **CLIP**: Experiment with multimodal magic.  

---

## Resources & References üìö  
*List in progress.*  

---

**Note**: This is a living project‚Äîcode and structure may evolve.