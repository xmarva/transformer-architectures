# Transformer Playground

To understand Transformer Architectures through practiceâ€”from implementing one from scratch to adapting pre-trained models (BERT, GPT). 

| Notebook | Concepts | Links |
|--------|----------|-------|
| 1. Tokenization and Encoding | BPE, HuggingFace Tokenizers, Collator | **[Prototype]**<br>**[Going Modular]** |
| 2. Transformer Architecture | Positional Encoding, Attention | **[Prototype]**<br>**[Going Modular]** |
| 3. Functions, Metrics, Tools | Translation, BLEU, ROGUE, METEOR, WandB | **[Prototype]**<br>**[Going Modular]** |
| 4. Transformer Training | Training, Evaluation, Scheduler, Xavier, LabelSmoothing | **[Prototype]**<br>**[Going Modular]** |

---

## Progress

### Basic Components  
- [x] **Tokenization**
- [x] **Transformer Architecture**  
- [x] **Functions and Tools**
- [x] **Transformer Training**
- [ ] **Improvements and Experiments**

---

## Resources & References ðŸ“š  
*List is coming soon.*  

---

**Note**: This is a living projectâ€”code and structure may evolve.