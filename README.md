# Transformer Playground

To understand Transformer Architectures through practice‚Äîfrom implementing one from scratch to adapting pre-trained models (BERT, GPT). 

| Notebook | Concepts | Links |
|--------|----------|-------|
| 1. Tokenization and Encoding | BPE, HuggingFace Tokenizers, Collator | [Prototype](https://www.kaggle.com/code/qmarva/bpe-tokenization) <br> [Going Modular] |
| 2. Transformer Architecture | Positional Encoding, Attention | [Prototype](https://www.kaggle.com/code/qmarva/building-transformer) <br> [Going Modular] |
| 3. Functions, Metrics, Tools | Translation, BLEU, ROGUE, METEOR, WandB | [Prototype] <br> [Going Modular] |
| 4. Transformer Training | Training, Evaluation, Scheduler, Xavier, LabelSmoothing | [Prototype] <br> [Going Modular] |

---


## Progress

### Basic Components  
- [x] **Tokenization**
- [x] **Transformer Architecture**  
- [x] **Functions and Tools**
- [x] **Transformer Training**
- [ ] **Improvements and Experiments**

---

## Resources & References üìö  
| **Paper**                                          | **Architecture**                | **Link**                                                   |
|--------------------------------------------------------------|-----------------------------------------|--------------------------------------------------------------|
| Attention is All You Need                                    | Transformer                              | [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)          |
| BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding | BERT (Bidirectional Encoder Representations from Transformers) | [arXiv:1810.04805](https://arxiv.org/abs/1810.04805)          |
| GPT-3: Language Models are Few-Shot Learners                  | GPT-3 (Generative Pre-trained Transformer 3) | [arXiv:2005.14165](https://arxiv.org/abs/2005.14165)          |
| T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer | T5 (Text-to-Text Transfer Transformer)   | [arXiv:1910.10683](https://arxiv.org/abs/1910.10683)          |
| Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context | Transformer-XL                          | [arXiv:1901.02860](https://arxiv.org/abs/1901.02860)          |
| RoBERTa: A Robustly Optimized BERT Pretraining Approach       | RoBERTa                                 | [arXiv:1907.11692](https://arxiv.org/abs/1907.11692)          |
| ALBERT: A Lite BERT for Self-supervised Learning of Language Representations | ALBERT (A Lite BERT)                   | [arXiv:1909.11942](https://arxiv.org/abs/1909.11942)          |
| XLNet: Generalized Autoregressive Pretraining for Language Understanding | XLNet                                  | [arXiv:1906.08237](https://arxiv.org/abs/1906.08237)          |
| Vision Transformer (ViT): An Image is Worth 16x16 Words       | Vision Transformer (ViT)                | [arXiv:2010.11929](https://arxiv.org/abs/2010.11929)          |
| DEBERTA: Decoding-enhanced BERT with Disentangled Attention   | DeBERTa                                 | [arXiv:2006.03654](https://arxiv.org/abs/2006.03654)          |
| Efficient Transformers: A Survey                            | –û–±—â–∏–π –æ–±–∑–æ—Ä —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –∏ –∏—Ö —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ | [arXiv:2009.06732](https://arxiv.org/abs/2009.06732)          |
| Reformer: The Efficient Transformer                          | Reformer                                | [arXiv:2001.04451](https://arxiv.org/abs/2001.04451)          |
| Linformer: Self-Attention with Linear Complexity              | Linformer                               | [arXiv:2006.04768](https://arxiv.org/abs/2006.04768)          |
| Longformer: The Long-Document Transformer                    | Longformer                              | [arXiv:2004.05150](https://arxiv.org/abs/2004.05150)          |


---

**Note**: This is a living project‚Äîcode and structure may evolve.